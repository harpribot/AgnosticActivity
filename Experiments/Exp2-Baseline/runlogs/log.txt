I1027 19:35:28.195552 51542 caffe.cpp:217] Using GPUs 0
I1027 19:35:28.201683 51542 caffe.cpp:222] GPU 0: Tesla K40m
I1027 19:35:28.454144 51542 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 500
base_lr: 0.01
display: 40
max_iter: 20000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "models/activity_alexnet/snapshots/caffe_alexnet_train"
solver_mode: GPU
device_id: 0
net: "models/activity_alexnet/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1027 19:35:28.456441 51542 solver.cpp:91] Creating training net from net file: models/activity_alexnet/train_val.prototxt
I1027 19:35:28.459103 51542 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1027 19:35:28.459167 51542 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer precision@1
I1027 19:35:28.459172 51542 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer precision@5
I1027 19:35:28.459789 51542 net.cpp:58] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "examples/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/work/03713/harshal1/maverick/vision_proj/data/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-new"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 504
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-new"
  bottom: "label"
  top: "loss"
}
I1027 19:35:28.460057 51542 layer_factory.hpp:77] Creating layer data
I1027 19:35:28.461014 51542 net.cpp:100] Creating Layer data
I1027 19:35:28.461088 51542 net.cpp:408] data -> data
I1027 19:35:28.461196 51542 net.cpp:408] data -> label
I1027 19:35:28.461251 51542 data_transformer.cpp:25] Loading mean file from: examples/imagenet/imagenet_mean.binaryproto
I1027 19:35:28.471346 51653 db_lmdb.cpp:35] Opened lmdb /work/03713/harshal1/maverick/vision_proj/data/train_lmdb
I1027 19:35:28.483790 51542 data_layer.cpp:41] output data size: 256,3,227,227
I1027 19:35:28.779815 51542 net.cpp:150] Setting up data
I1027 19:35:28.779970 51542 net.cpp:157] Top shape: 256 3 227 227 (39574272)
I1027 19:35:28.779980 51542 net.cpp:157] Top shape: 256 (256)
I1027 19:35:28.779984 51542 net.cpp:165] Memory required for data: 158298112
I1027 19:35:28.780000 51542 layer_factory.hpp:77] Creating layer conv1
I1027 19:35:28.780067 51542 net.cpp:100] Creating Layer conv1
I1027 19:35:28.780102 51542 net.cpp:434] conv1 <- data
I1027 19:35:28.780131 51542 net.cpp:408] conv1 -> conv1
I1027 19:35:29.036427 51542 net.cpp:150] Setting up conv1
I1027 19:35:29.036480 51542 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I1027 19:35:29.036485 51542 net.cpp:165] Memory required for data: 455667712
I1027 19:35:29.036545 51542 layer_factory.hpp:77] Creating layer relu1
I1027 19:35:29.036607 51542 net.cpp:100] Creating Layer relu1
I1027 19:35:29.036614 51542 net.cpp:434] relu1 <- conv1
I1027 19:35:29.036623 51542 net.cpp:395] relu1 -> conv1 (in-place)
I1027 19:35:29.036907 51542 net.cpp:150] Setting up relu1
I1027 19:35:29.036918 51542 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I1027 19:35:29.036921 51542 net.cpp:165] Memory required for data: 753037312
I1027 19:35:29.036926 51542 layer_factory.hpp:77] Creating layer norm1
I1027 19:35:29.036968 51542 net.cpp:100] Creating Layer norm1
I1027 19:35:29.036989 51542 net.cpp:434] norm1 <- conv1
I1027 19:35:29.036996 51542 net.cpp:408] norm1 -> norm1
I1027 19:35:29.037242 51542 net.cpp:150] Setting up norm1
I1027 19:35:29.037253 51542 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I1027 19:35:29.037300 51542 net.cpp:165] Memory required for data: 1050406912
I1027 19:35:29.037304 51542 layer_factory.hpp:77] Creating layer pool1
I1027 19:35:29.037314 51542 net.cpp:100] Creating Layer pool1
I1027 19:35:29.037318 51542 net.cpp:434] pool1 <- norm1
I1027 19:35:29.037324 51542 net.cpp:408] pool1 -> pool1
I1027 19:35:29.037436 51542 net.cpp:150] Setting up pool1
I1027 19:35:29.037446 51542 net.cpp:157] Top shape: 256 96 27 27 (17915904)
I1027 19:35:29.037448 51542 net.cpp:165] Memory required for data: 1122070528
I1027 19:35:29.037452 51542 layer_factory.hpp:77] Creating layer conv2
I1027 19:35:29.037469 51542 net.cpp:100] Creating Layer conv2
I1027 19:35:29.037473 51542 net.cpp:434] conv2 <- pool1
I1027 19:35:29.037479 51542 net.cpp:408] conv2 -> conv2
I1027 19:35:29.044201 51542 net.cpp:150] Setting up conv2
I1027 19:35:29.044214 51542 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I1027 19:35:29.044230 51542 net.cpp:165] Memory required for data: 1313173504
I1027 19:35:29.044240 51542 layer_factory.hpp:77] Creating layer relu2
I1027 19:35:29.044256 51542 net.cpp:100] Creating Layer relu2
I1027 19:35:29.044260 51542 net.cpp:434] relu2 <- conv2
I1027 19:35:29.044268 51542 net.cpp:395] relu2 -> conv2 (in-place)
I1027 19:35:29.044420 51542 net.cpp:150] Setting up relu2
I1027 19:35:29.044430 51542 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I1027 19:35:29.044433 51542 net.cpp:165] Memory required for data: 1504276480
I1027 19:35:29.044437 51542 layer_factory.hpp:77] Creating layer norm2
I1027 19:35:29.044445 51542 net.cpp:100] Creating Layer norm2
I1027 19:35:29.044447 51542 net.cpp:434] norm2 <- conv2
I1027 19:35:29.044455 51542 net.cpp:408] norm2 -> norm2
I1027 19:35:29.044733 51542 net.cpp:150] Setting up norm2
I1027 19:35:29.044742 51542 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I1027 19:35:29.044746 51542 net.cpp:165] Memory required for data: 1695379456
I1027 19:35:29.044749 51542 layer_factory.hpp:77] Creating layer pool2
I1027 19:35:29.044760 51542 net.cpp:100] Creating Layer pool2
I1027 19:35:29.044764 51542 net.cpp:434] pool2 <- norm2
I1027 19:35:29.044770 51542 net.cpp:408] pool2 -> pool2
I1027 19:35:29.044806 51542 net.cpp:150] Setting up pool2
I1027 19:35:29.044812 51542 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I1027 19:35:29.044816 51542 net.cpp:165] Memory required for data: 1739681792
I1027 19:35:29.044818 51542 layer_factory.hpp:77] Creating layer conv3
I1027 19:35:29.044831 51542 net.cpp:100] Creating Layer conv3
I1027 19:35:29.044834 51542 net.cpp:434] conv3 <- pool2
I1027 19:35:29.044842 51542 net.cpp:408] conv3 -> conv3
I1027 19:35:29.058789 51542 net.cpp:150] Setting up conv3
I1027 19:35:29.058826 51542 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1027 19:35:29.058831 51542 net.cpp:165] Memory required for data: 1806135296
I1027 19:35:29.058845 51542 layer_factory.hpp:77] Creating layer relu3
I1027 19:35:29.058856 51542 net.cpp:100] Creating Layer relu3
I1027 19:35:29.058861 51542 net.cpp:434] relu3 <- conv3
I1027 19:35:29.058871 51542 net.cpp:395] relu3 -> conv3 (in-place)
I1027 19:35:29.059044 51542 net.cpp:150] Setting up relu3
I1027 19:35:29.059057 51542 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1027 19:35:29.059061 51542 net.cpp:165] Memory required for data: 1872588800
I1027 19:35:29.059064 51542 layer_factory.hpp:77] Creating layer conv4
I1027 19:35:29.059080 51542 net.cpp:100] Creating Layer conv4
I1027 19:35:29.059084 51542 net.cpp:434] conv4 <- conv3
I1027 19:35:29.059092 51542 net.cpp:408] conv4 -> conv4
I1027 19:35:29.070459 51542 net.cpp:150] Setting up conv4
I1027 19:35:29.070478 51542 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1027 19:35:29.070493 51542 net.cpp:165] Memory required for data: 1939042304
I1027 19:35:29.070502 51542 layer_factory.hpp:77] Creating layer relu4
I1027 19:35:29.070514 51542 net.cpp:100] Creating Layer relu4
I1027 19:35:29.070518 51542 net.cpp:434] relu4 <- conv4
I1027 19:35:29.070525 51542 net.cpp:395] relu4 -> conv4 (in-place)
I1027 19:35:29.070677 51542 net.cpp:150] Setting up relu4
I1027 19:35:29.070685 51542 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1027 19:35:29.070718 51542 net.cpp:165] Memory required for data: 2005495808
I1027 19:35:29.070722 51542 layer_factory.hpp:77] Creating layer conv5
I1027 19:35:29.070737 51542 net.cpp:100] Creating Layer conv5
I1027 19:35:29.070741 51542 net.cpp:434] conv5 <- conv4
I1027 19:35:29.070749 51542 net.cpp:408] conv5 -> conv5
I1027 19:35:29.079095 51542 net.cpp:150] Setting up conv5
I1027 19:35:29.079109 51542 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I1027 19:35:29.079125 51542 net.cpp:165] Memory required for data: 2049798144
I1027 19:35:29.079138 51542 layer_factory.hpp:77] Creating layer relu5
I1027 19:35:29.079146 51542 net.cpp:100] Creating Layer relu5
I1027 19:35:29.079150 51542 net.cpp:434] relu5 <- conv5
I1027 19:35:29.079157 51542 net.cpp:395] relu5 -> conv5 (in-place)
I1027 19:35:29.079306 51542 net.cpp:150] Setting up relu5
I1027 19:35:29.079316 51542 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I1027 19:35:29.079319 51542 net.cpp:165] Memory required for data: 2094100480
I1027 19:35:29.079322 51542 layer_factory.hpp:77] Creating layer pool5
I1027 19:35:29.079331 51542 net.cpp:100] Creating Layer pool5
I1027 19:35:29.079334 51542 net.cpp:434] pool5 <- conv5
I1027 19:35:29.079344 51542 net.cpp:408] pool5 -> pool5
I1027 19:35:29.079385 51542 net.cpp:150] Setting up pool5
I1027 19:35:29.079392 51542 net.cpp:157] Top shape: 256 256 6 6 (2359296)
I1027 19:35:29.079396 51542 net.cpp:165] Memory required for data: 2103537664
I1027 19:35:29.079398 51542 layer_factory.hpp:77] Creating layer fc6
I1027 19:35:29.079452 51542 net.cpp:100] Creating Layer fc6
I1027 19:35:29.079459 51542 net.cpp:434] fc6 <- pool5
I1027 19:35:29.079468 51542 net.cpp:408] fc6 -> fc6
I1027 19:35:29.614233 51542 net.cpp:150] Setting up fc6
I1027 19:35:29.614279 51542 net.cpp:157] Top shape: 256 4096 (1048576)
I1027 19:35:29.614284 51542 net.cpp:165] Memory required for data: 2107731968
I1027 19:35:29.614295 51542 layer_factory.hpp:77] Creating layer relu6
I1027 19:35:29.614312 51542 net.cpp:100] Creating Layer relu6
I1027 19:35:29.614317 51542 net.cpp:434] relu6 <- fc6
I1027 19:35:29.614326 51542 net.cpp:395] relu6 -> fc6 (in-place)
I1027 19:35:29.614760 51542 net.cpp:150] Setting up relu6
I1027 19:35:29.614769 51542 net.cpp:157] Top shape: 256 4096 (1048576)
I1027 19:35:29.614773 51542 net.cpp:165] Memory required for data: 2111926272
I1027 19:35:29.614789 51542 layer_factory.hpp:77] Creating layer drop6
I1027 19:35:29.614850 51542 net.cpp:100] Creating Layer drop6
I1027 19:35:29.614857 51542 net.cpp:434] drop6 <- fc6
I1027 19:35:29.614863 51542 net.cpp:395] drop6 -> fc6 (in-place)
I1027 19:35:29.614900 51542 net.cpp:150] Setting up drop6
I1027 19:35:29.614907 51542 net.cpp:157] Top shape: 256 4096 (1048576)
I1027 19:35:29.614910 51542 net.cpp:165] Memory required for data: 2116120576
I1027 19:35:29.614913 51542 layer_factory.hpp:77] Creating layer fc7
I1027 19:35:29.614923 51542 net.cpp:100] Creating Layer fc7
I1027 19:35:29.614928 51542 net.cpp:434] fc7 <- fc6
I1027 19:35:29.614934 51542 net.cpp:408] fc7 -> fc7
I1027 19:35:29.848763 51542 net.cpp:150] Setting up fc7
I1027 19:35:29.848811 51542 net.cpp:157] Top shape: 256 4096 (1048576)
I1027 19:35:29.848815 51542 net.cpp:165] Memory required for data: 2120314880
I1027 19:35:29.848829 51542 layer_factory.hpp:77] Creating layer relu7
I1027 19:35:29.848847 51542 net.cpp:100] Creating Layer relu7
I1027 19:35:29.848852 51542 net.cpp:434] relu7 <- fc7
I1027 19:35:29.848865 51542 net.cpp:395] relu7 -> fc7 (in-place)
I1027 19:35:29.849117 51542 net.cpp:150] Setting up relu7
I1027 19:35:29.849128 51542 net.cpp:157] Top shape: 256 4096 (1048576)
I1027 19:35:29.849130 51542 net.cpp:165] Memory required for data: 2124509184
I1027 19:35:29.849134 51542 layer_factory.hpp:77] Creating layer drop7
I1027 19:35:29.849146 51542 net.cpp:100] Creating Layer drop7
I1027 19:35:29.849150 51542 net.cpp:434] drop7 <- fc7
I1027 19:35:29.849155 51542 net.cpp:395] drop7 -> fc7 (in-place)
I1027 19:35:29.849177 51542 net.cpp:150] Setting up drop7
I1027 19:35:29.849186 51542 net.cpp:157] Top shape: 256 4096 (1048576)
I1027 19:35:29.849230 51542 net.cpp:165] Memory required for data: 2128703488
I1027 19:35:29.849234 51542 layer_factory.hpp:77] Creating layer fc8-new
I1027 19:35:29.849246 51542 net.cpp:100] Creating Layer fc8-new
I1027 19:35:29.849251 51542 net.cpp:434] fc8-new <- fc7
I1027 19:35:29.849259 51542 net.cpp:408] fc8-new -> fc8-new
I1027 19:35:29.878196 51542 net.cpp:150] Setting up fc8-new
I1027 19:35:29.878218 51542 net.cpp:157] Top shape: 256 504 (129024)
I1027 19:35:29.878221 51542 net.cpp:165] Memory required for data: 2129219584
I1027 19:35:29.878227 51542 layer_factory.hpp:77] Creating layer loss
I1027 19:35:29.878288 51542 net.cpp:100] Creating Layer loss
I1027 19:35:29.878295 51542 net.cpp:434] loss <- fc8-new
I1027 19:35:29.878299 51542 net.cpp:434] loss <- label
I1027 19:35:29.878314 51542 net.cpp:408] loss -> loss
I1027 19:35:29.878373 51542 layer_factory.hpp:77] Creating layer loss
I1027 19:35:29.879465 51542 net.cpp:150] Setting up loss
I1027 19:35:29.879475 51542 net.cpp:157] Top shape: (1)
I1027 19:35:29.879478 51542 net.cpp:160]     with loss weight 1
I1027 19:35:29.879580 51542 net.cpp:165] Memory required for data: 2129219588
I1027 19:35:29.879583 51542 net.cpp:226] loss needs backward computation.
I1027 19:35:29.879588 51542 net.cpp:226] fc8-new needs backward computation.
I1027 19:35:29.879591 51542 net.cpp:226] drop7 needs backward computation.
I1027 19:35:29.879595 51542 net.cpp:226] relu7 needs backward computation.
I1027 19:35:29.879596 51542 net.cpp:226] fc7 needs backward computation.
I1027 19:35:29.879601 51542 net.cpp:226] drop6 needs backward computation.
I1027 19:35:29.879602 51542 net.cpp:226] relu6 needs backward computation.
I1027 19:35:29.879606 51542 net.cpp:226] fc6 needs backward computation.
I1027 19:35:29.879608 51542 net.cpp:226] pool5 needs backward computation.
I1027 19:35:29.879612 51542 net.cpp:226] relu5 needs backward computation.
I1027 19:35:29.879616 51542 net.cpp:226] conv5 needs backward computation.
I1027 19:35:29.879618 51542 net.cpp:226] relu4 needs backward computation.
I1027 19:35:29.879621 51542 net.cpp:226] conv4 needs backward computation.
I1027 19:35:29.879624 51542 net.cpp:226] relu3 needs backward computation.
I1027 19:35:29.879626 51542 net.cpp:226] conv3 needs backward computation.
I1027 19:35:29.879631 51542 net.cpp:226] pool2 needs backward computation.
I1027 19:35:29.879633 51542 net.cpp:226] norm2 needs backward computation.
I1027 19:35:29.879637 51542 net.cpp:226] relu2 needs backward computation.
I1027 19:35:29.879639 51542 net.cpp:226] conv2 needs backward computation.
I1027 19:35:29.879642 51542 net.cpp:226] pool1 needs backward computation.
I1027 19:35:29.879645 51542 net.cpp:226] norm1 needs backward computation.
I1027 19:35:29.879648 51542 net.cpp:226] relu1 needs backward computation.
I1027 19:35:29.879652 51542 net.cpp:226] conv1 needs backward computation.
I1027 19:35:29.879655 51542 net.cpp:228] data does not need backward computation.
I1027 19:35:29.879658 51542 net.cpp:270] This network produces output loss
I1027 19:35:29.879674 51542 net.cpp:283] Network initialization done.
I1027 19:35:29.882055 51542 solver.cpp:181] Creating test net (#0) specified by net file: models/activity_alexnet/train_val.prototxt
I1027 19:35:29.882115 51542 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1027 19:35:29.882740 51542 net.cpp:58] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "examples/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/work/03713/harshal1/maverick/vision_proj/data/val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-new"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 504
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "precision@1"
  type: "Accuracy"
  bottom: "fc8-new"
  bottom: "label"
  top: "precision@1"
  include {
    phase: TEST
  }
}
layer {
  name: "precision@5"
  type: "Accuracy"
  bottom: "fc8-new"
  bottom: "label"
  top: "precision@5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-new"
  bottom: "label"
  top: "loss"
}
I1027 19:35:29.882921 51542 layer_factory.hpp:77] Creating layer data
I1027 19:35:29.883050 51542 net.cpp:100] Creating Layer data
I1027 19:35:29.883080 51542 net.cpp:408] data -> data
I1027 19:35:29.883090 51542 net.cpp:408] data -> label
I1027 19:35:29.883100 51542 data_transformer.cpp:25] Loading mean file from: examples/imagenet/imagenet_mean.binaryproto
I1027 19:35:29.894004 51655 db_lmdb.cpp:35] Opened lmdb /work/03713/harshal1/maverick/vision_proj/data/val_lmdb
I1027 19:35:29.895189 51542 data_layer.cpp:41] output data size: 50,3,227,227
I1027 19:35:29.953909 51542 net.cpp:150] Setting up data
I1027 19:35:29.953959 51542 net.cpp:157] Top shape: 50 3 227 227 (7729350)
I1027 19:35:29.953966 51542 net.cpp:157] Top shape: 50 (50)
I1027 19:35:29.953969 51542 net.cpp:165] Memory required for data: 30917600
I1027 19:35:29.953979 51542 layer_factory.hpp:77] Creating layer label_data_1_split
I1027 19:35:29.954083 51542 net.cpp:100] Creating Layer label_data_1_split
I1027 19:35:29.954092 51542 net.cpp:434] label_data_1_split <- label
I1027 19:35:29.954102 51542 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1027 19:35:29.954115 51542 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1027 19:35:29.954123 51542 net.cpp:408] label_data_1_split -> label_data_1_split_2
I1027 19:35:29.954213 51542 net.cpp:150] Setting up label_data_1_split
I1027 19:35:29.954224 51542 net.cpp:157] Top shape: 50 (50)
I1027 19:35:29.954229 51542 net.cpp:157] Top shape: 50 (50)
I1027 19:35:29.954233 51542 net.cpp:157] Top shape: 50 (50)
I1027 19:35:29.954236 51542 net.cpp:165] Memory required for data: 30918200
I1027 19:35:29.954241 51542 layer_factory.hpp:77] Creating layer conv1
I1027 19:35:29.954259 51542 net.cpp:100] Creating Layer conv1
I1027 19:35:29.954264 51542 net.cpp:434] conv1 <- data
I1027 19:35:29.954272 51542 net.cpp:408] conv1 -> conv1
I1027 19:35:29.958690 51542 net.cpp:150] Setting up conv1
I1027 19:35:29.958703 51542 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I1027 19:35:29.958719 51542 net.cpp:165] Memory required for data: 88998200
I1027 19:35:29.958731 51542 layer_factory.hpp:77] Creating layer relu1
I1027 19:35:29.958740 51542 net.cpp:100] Creating Layer relu1
I1027 19:35:29.958745 51542 net.cpp:434] relu1 <- conv1
I1027 19:35:29.958750 51542 net.cpp:395] relu1 -> conv1 (in-place)
I1027 19:35:29.958904 51542 net.cpp:150] Setting up relu1
I1027 19:35:29.958915 51542 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I1027 19:35:29.958919 51542 net.cpp:165] Memory required for data: 147078200
I1027 19:35:29.958922 51542 layer_factory.hpp:77] Creating layer norm1
I1027 19:35:29.958932 51542 net.cpp:100] Creating Layer norm1
I1027 19:35:29.958936 51542 net.cpp:434] norm1 <- conv1
I1027 19:35:29.958943 51542 net.cpp:408] norm1 -> norm1
I1027 19:35:29.959254 51542 net.cpp:150] Setting up norm1
I1027 19:35:29.959265 51542 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I1027 19:35:29.959270 51542 net.cpp:165] Memory required for data: 205158200
I1027 19:35:29.959273 51542 layer_factory.hpp:77] Creating layer pool1
I1027 19:35:29.959282 51542 net.cpp:100] Creating Layer pool1
I1027 19:35:29.959286 51542 net.cpp:434] pool1 <- norm1
I1027 19:35:29.959293 51542 net.cpp:408] pool1 -> pool1
I1027 19:35:29.959336 51542 net.cpp:150] Setting up pool1
I1027 19:35:29.959343 51542 net.cpp:157] Top shape: 50 96 27 27 (3499200)
I1027 19:35:29.959347 51542 net.cpp:165] Memory required for data: 219155000
I1027 19:35:29.959350 51542 layer_factory.hpp:77] Creating layer conv2
I1027 19:35:29.959395 51542 net.cpp:100] Creating Layer conv2
I1027 19:35:29.959399 51542 net.cpp:434] conv2 <- pool1
I1027 19:35:29.959408 51542 net.cpp:408] conv2 -> conv2
I1027 19:35:29.965623 51542 net.cpp:150] Setting up conv2
I1027 19:35:29.965649 51542 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I1027 19:35:29.965653 51542 net.cpp:165] Memory required for data: 256479800
I1027 19:35:29.965662 51542 layer_factory.hpp:77] Creating layer relu2
I1027 19:35:29.965670 51542 net.cpp:100] Creating Layer relu2
I1027 19:35:29.965674 51542 net.cpp:434] relu2 <- conv2
I1027 19:35:29.965680 51542 net.cpp:395] relu2 -> conv2 (in-place)
I1027 19:35:29.965960 51542 net.cpp:150] Setting up relu2
I1027 19:35:29.965975 51542 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I1027 19:35:29.965978 51542 net.cpp:165] Memory required for data: 293804600
I1027 19:35:29.965982 51542 layer_factory.hpp:77] Creating layer norm2
I1027 19:35:29.965992 51542 net.cpp:100] Creating Layer norm2
I1027 19:35:29.965996 51542 net.cpp:434] norm2 <- conv2
I1027 19:35:29.966003 51542 net.cpp:408] norm2 -> norm2
I1027 19:35:29.966290 51542 net.cpp:150] Setting up norm2
I1027 19:35:29.966302 51542 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I1027 19:35:29.966306 51542 net.cpp:165] Memory required for data: 331129400
I1027 19:35:29.966310 51542 layer_factory.hpp:77] Creating layer pool2
I1027 19:35:29.966318 51542 net.cpp:100] Creating Layer pool2
I1027 19:35:29.966322 51542 net.cpp:434] pool2 <- norm2
I1027 19:35:29.966330 51542 net.cpp:408] pool2 -> pool2
I1027 19:35:29.966368 51542 net.cpp:150] Setting up pool2
I1027 19:35:29.966375 51542 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I1027 19:35:29.966378 51542 net.cpp:165] Memory required for data: 339782200
I1027 19:35:29.966382 51542 layer_factory.hpp:77] Creating layer conv3
I1027 19:35:29.966406 51542 net.cpp:100] Creating Layer conv3
I1027 19:35:29.966410 51542 net.cpp:434] conv3 <- pool2
I1027 19:35:29.966418 51542 net.cpp:408] conv3 -> conv3
I1027 19:35:29.981052 51542 net.cpp:150] Setting up conv3
I1027 19:35:29.981094 51542 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1027 19:35:29.981098 51542 net.cpp:165] Memory required for data: 352761400
I1027 19:35:29.981114 51542 layer_factory.hpp:77] Creating layer relu3
I1027 19:35:29.981127 51542 net.cpp:100] Creating Layer relu3
I1027 19:35:29.981132 51542 net.cpp:434] relu3 <- conv3
I1027 19:35:29.981142 51542 net.cpp:395] relu3 -> conv3 (in-place)
I1027 19:35:29.981407 51542 net.cpp:150] Setting up relu3
I1027 19:35:29.981420 51542 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1027 19:35:29.981422 51542 net.cpp:165] Memory required for data: 365740600
I1027 19:35:29.981426 51542 layer_factory.hpp:77] Creating layer conv4
I1027 19:35:29.981441 51542 net.cpp:100] Creating Layer conv4
I1027 19:35:29.981446 51542 net.cpp:434] conv4 <- conv3
I1027 19:35:29.981453 51542 net.cpp:408] conv4 -> conv4
I1027 19:35:29.993212 51542 net.cpp:150] Setting up conv4
I1027 19:35:29.993227 51542 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1027 19:35:29.993242 51542 net.cpp:165] Memory required for data: 378719800
I1027 19:35:29.993249 51542 layer_factory.hpp:77] Creating layer relu4
I1027 19:35:29.993258 51542 net.cpp:100] Creating Layer relu4
I1027 19:35:29.993263 51542 net.cpp:434] relu4 <- conv4
I1027 19:35:29.993268 51542 net.cpp:395] relu4 -> conv4 (in-place)
I1027 19:35:29.993532 51542 net.cpp:150] Setting up relu4
I1027 19:35:29.993543 51542 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1027 19:35:29.993546 51542 net.cpp:165] Memory required for data: 391699000
I1027 19:35:29.993551 51542 layer_factory.hpp:77] Creating layer conv5
I1027 19:35:29.993562 51542 net.cpp:100] Creating Layer conv5
I1027 19:35:29.993566 51542 net.cpp:434] conv5 <- conv4
I1027 19:35:29.993574 51542 net.cpp:408] conv5 -> conv5
I1027 19:35:30.001987 51542 net.cpp:150] Setting up conv5
I1027 19:35:30.002017 51542 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I1027 19:35:30.002020 51542 net.cpp:165] Memory required for data: 400351800
I1027 19:35:30.002032 51542 layer_factory.hpp:77] Creating layer relu5
I1027 19:35:30.002074 51542 net.cpp:100] Creating Layer relu5
I1027 19:35:30.002079 51542 net.cpp:434] relu5 <- conv5
I1027 19:35:30.002085 51542 net.cpp:395] relu5 -> conv5 (in-place)
I1027 19:35:30.002239 51542 net.cpp:150] Setting up relu5
I1027 19:35:30.002249 51542 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I1027 19:35:30.002254 51542 net.cpp:165] Memory required for data: 409004600
I1027 19:35:30.002256 51542 layer_factory.hpp:77] Creating layer pool5
I1027 19:35:30.002269 51542 net.cpp:100] Creating Layer pool5
I1027 19:35:30.002272 51542 net.cpp:434] pool5 <- conv5
I1027 19:35:30.002279 51542 net.cpp:408] pool5 -> pool5
I1027 19:35:30.002326 51542 net.cpp:150] Setting up pool5
I1027 19:35:30.002336 51542 net.cpp:157] Top shape: 50 256 6 6 (460800)
I1027 19:35:30.002341 51542 net.cpp:165] Memory required for data: 410847800
I1027 19:35:30.002343 51542 layer_factory.hpp:77] Creating layer fc6
I1027 19:35:30.002354 51542 net.cpp:100] Creating Layer fc6
I1027 19:35:30.002358 51542 net.cpp:434] fc6 <- pool5
I1027 19:35:30.002365 51542 net.cpp:408] fc6 -> fc6
I1027 19:35:30.557708 51542 net.cpp:150] Setting up fc6
I1027 19:35:30.557760 51542 net.cpp:157] Top shape: 50 4096 (204800)
I1027 19:35:30.557765 51542 net.cpp:165] Memory required for data: 411667000
I1027 19:35:30.557776 51542 layer_factory.hpp:77] Creating layer relu6
I1027 19:35:30.557795 51542 net.cpp:100] Creating Layer relu6
I1027 19:35:30.557801 51542 net.cpp:434] relu6 <- fc6
I1027 19:35:30.557811 51542 net.cpp:395] relu6 -> fc6 (in-place)
I1027 19:35:30.558347 51542 net.cpp:150] Setting up relu6
I1027 19:35:30.558358 51542 net.cpp:157] Top shape: 50 4096 (204800)
I1027 19:35:30.558374 51542 net.cpp:165] Memory required for data: 412486200
I1027 19:35:30.558377 51542 layer_factory.hpp:77] Creating layer drop6
I1027 19:35:30.558388 51542 net.cpp:100] Creating Layer drop6
I1027 19:35:30.558393 51542 net.cpp:434] drop6 <- fc6
I1027 19:35:30.558398 51542 net.cpp:395] drop6 -> fc6 (in-place)
I1027 19:35:30.558431 51542 net.cpp:150] Setting up drop6
I1027 19:35:30.558437 51542 net.cpp:157] Top shape: 50 4096 (204800)
I1027 19:35:30.558440 51542 net.cpp:165] Memory required for data: 413305400
I1027 19:35:30.558455 51542 layer_factory.hpp:77] Creating layer fc7
I1027 19:35:30.558468 51542 net.cpp:100] Creating Layer fc7
I1027 19:35:30.558471 51542 net.cpp:434] fc7 <- fc6
I1027 19:35:30.558478 51542 net.cpp:408] fc7 -> fc7
I1027 19:35:30.793973 51542 net.cpp:150] Setting up fc7
I1027 19:35:30.794026 51542 net.cpp:157] Top shape: 50 4096 (204800)
I1027 19:35:30.794031 51542 net.cpp:165] Memory required for data: 414124600
I1027 19:35:30.794044 51542 layer_factory.hpp:77] Creating layer relu7
I1027 19:35:30.794061 51542 net.cpp:100] Creating Layer relu7
I1027 19:35:30.794067 51542 net.cpp:434] relu7 <- fc7
I1027 19:35:30.794078 51542 net.cpp:395] relu7 -> fc7 (in-place)
I1027 19:35:30.794307 51542 net.cpp:150] Setting up relu7
I1027 19:35:30.794317 51542 net.cpp:157] Top shape: 50 4096 (204800)
I1027 19:35:30.794320 51542 net.cpp:165] Memory required for data: 414943800
I1027 19:35:30.794323 51542 layer_factory.hpp:77] Creating layer drop7
I1027 19:35:30.794333 51542 net.cpp:100] Creating Layer drop7
I1027 19:35:30.794337 51542 net.cpp:434] drop7 <- fc7
I1027 19:35:30.794342 51542 net.cpp:395] drop7 -> fc7 (in-place)
I1027 19:35:30.794373 51542 net.cpp:150] Setting up drop7
I1027 19:35:30.794380 51542 net.cpp:157] Top shape: 50 4096 (204800)
I1027 19:35:30.794384 51542 net.cpp:165] Memory required for data: 415763000
I1027 19:35:30.794386 51542 layer_factory.hpp:77] Creating layer fc8-new
I1027 19:35:30.794399 51542 net.cpp:100] Creating Layer fc8-new
I1027 19:35:30.794402 51542 net.cpp:434] fc8-new <- fc7
I1027 19:35:30.794409 51542 net.cpp:408] fc8-new -> fc8-new
I1027 19:35:30.823354 51542 net.cpp:150] Setting up fc8-new
I1027 19:35:30.823364 51542 net.cpp:157] Top shape: 50 504 (25200)
I1027 19:35:30.823380 51542 net.cpp:165] Memory required for data: 415863800
I1027 19:35:30.823386 51542 layer_factory.hpp:77] Creating layer fc8-new_fc8-new_0_split
I1027 19:35:30.823422 51542 net.cpp:100] Creating Layer fc8-new_fc8-new_0_split
I1027 19:35:30.823426 51542 net.cpp:434] fc8-new_fc8-new_0_split <- fc8-new
I1027 19:35:30.823433 51542 net.cpp:408] fc8-new_fc8-new_0_split -> fc8-new_fc8-new_0_split_0
I1027 19:35:30.823441 51542 net.cpp:408] fc8-new_fc8-new_0_split -> fc8-new_fc8-new_0_split_1
I1027 19:35:30.823446 51542 net.cpp:408] fc8-new_fc8-new_0_split -> fc8-new_fc8-new_0_split_2
I1027 19:35:30.823494 51542 net.cpp:150] Setting up fc8-new_fc8-new_0_split
I1027 19:35:30.823501 51542 net.cpp:157] Top shape: 50 504 (25200)
I1027 19:35:30.823505 51542 net.cpp:157] Top shape: 50 504 (25200)
I1027 19:35:30.823508 51542 net.cpp:157] Top shape: 50 504 (25200)
I1027 19:35:30.823511 51542 net.cpp:165] Memory required for data: 416166200
I1027 19:35:30.823514 51542 layer_factory.hpp:77] Creating layer precision@1
I1027 19:35:30.823576 51542 net.cpp:100] Creating Layer precision@1
I1027 19:35:30.823585 51542 net.cpp:434] precision@1 <- fc8-new_fc8-new_0_split_0
I1027 19:35:30.823590 51542 net.cpp:434] precision@1 <- label_data_1_split_0
I1027 19:35:30.823596 51542 net.cpp:408] precision@1 -> precision@1
I1027 19:35:30.823639 51542 net.cpp:150] Setting up precision@1
I1027 19:35:30.823647 51542 net.cpp:157] Top shape: (1)
I1027 19:35:30.823650 51542 net.cpp:165] Memory required for data: 416166204
I1027 19:35:30.823653 51542 layer_factory.hpp:77] Creating layer precision@5
I1027 19:35:30.823662 51542 net.cpp:100] Creating Layer precision@5
I1027 19:35:30.823665 51542 net.cpp:434] precision@5 <- fc8-new_fc8-new_0_split_1
I1027 19:35:30.823670 51542 net.cpp:434] precision@5 <- label_data_1_split_1
I1027 19:35:30.823676 51542 net.cpp:408] precision@5 -> precision@5
I1027 19:35:30.823685 51542 net.cpp:150] Setting up precision@5
I1027 19:35:30.823689 51542 net.cpp:157] Top shape: (1)
I1027 19:35:30.823693 51542 net.cpp:165] Memory required for data: 416166208
I1027 19:35:30.823695 51542 layer_factory.hpp:77] Creating layer loss
I1027 19:35:30.823701 51542 net.cpp:100] Creating Layer loss
I1027 19:35:30.823704 51542 net.cpp:434] loss <- fc8-new_fc8-new_0_split_2
I1027 19:35:30.823709 51542 net.cpp:434] loss <- label_data_1_split_2
I1027 19:35:30.823714 51542 net.cpp:408] loss -> loss
I1027 19:35:30.823722 51542 layer_factory.hpp:77] Creating layer loss
I1027 19:35:30.824168 51542 net.cpp:150] Setting up loss
I1027 19:35:30.824179 51542 net.cpp:157] Top shape: (1)
I1027 19:35:30.824183 51542 net.cpp:160]     with loss weight 1
I1027 19:35:30.824196 51542 net.cpp:165] Memory required for data: 416166212
I1027 19:35:30.824200 51542 net.cpp:226] loss needs backward computation.
I1027 19:35:30.824204 51542 net.cpp:228] precision@5 does not need backward computation.
I1027 19:35:30.824208 51542 net.cpp:228] precision@1 does not need backward computation.
I1027 19:35:30.824213 51542 net.cpp:226] fc8-new_fc8-new_0_split needs backward computation.
I1027 19:35:30.824215 51542 net.cpp:226] fc8-new needs backward computation.
I1027 19:35:30.824218 51542 net.cpp:226] drop7 needs backward computation.
I1027 19:35:30.824221 51542 net.cpp:226] relu7 needs backward computation.
I1027 19:35:30.824224 51542 net.cpp:226] fc7 needs backward computation.
I1027 19:35:30.824228 51542 net.cpp:226] drop6 needs backward computation.
I1027 19:35:30.824231 51542 net.cpp:226] relu6 needs backward computation.
I1027 19:35:30.824234 51542 net.cpp:226] fc6 needs backward computation.
I1027 19:35:30.824237 51542 net.cpp:226] pool5 needs backward computation.
I1027 19:35:30.824241 51542 net.cpp:226] relu5 needs backward computation.
I1027 19:35:30.824244 51542 net.cpp:226] conv5 needs backward computation.
I1027 19:35:30.824249 51542 net.cpp:226] relu4 needs backward computation.
I1027 19:35:30.824264 51542 net.cpp:226] conv4 needs backward computation.
I1027 19:35:30.824267 51542 net.cpp:226] relu3 needs backward computation.
I1027 19:35:30.824270 51542 net.cpp:226] conv3 needs backward computation.
I1027 19:35:30.824273 51542 net.cpp:226] pool2 needs backward computation.
I1027 19:35:30.824277 51542 net.cpp:226] norm2 needs backward computation.
I1027 19:35:30.824293 51542 net.cpp:226] relu2 needs backward computation.
I1027 19:35:30.824297 51542 net.cpp:226] conv2 needs backward computation.
I1027 19:35:30.824301 51542 net.cpp:226] pool1 needs backward computation.
I1027 19:35:30.824304 51542 net.cpp:226] norm1 needs backward computation.
I1027 19:35:30.824308 51542 net.cpp:226] relu1 needs backward computation.
I1027 19:35:30.824312 51542 net.cpp:226] conv1 needs backward computation.
I1027 19:35:30.824316 51542 net.cpp:228] label_data_1_split does not need backward computation.
I1027 19:35:30.824321 51542 net.cpp:228] data does not need backward computation.
I1027 19:35:30.824323 51542 net.cpp:270] This network produces output loss
I1027 19:35:30.824327 51542 net.cpp:270] This network produces output precision@1
I1027 19:35:30.824331 51542 net.cpp:270] This network produces output precision@5
I1027 19:35:30.824347 51542 net.cpp:283] Network initialization done.
I1027 19:35:30.824442 51542 solver.cpp:60] Solver scaffolding done.
I1027 19:35:30.825073 51542 caffe.cpp:155] Finetuning from models/activity_alexnet/pretrained_model/bvlc_alexnet.caffemodel
I1027 19:35:31.328811 51542 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: models/activity_alexnet/pretrained_model/bvlc_alexnet.caffemodel
I1027 19:35:31.328855 51542 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1027 19:35:31.328863 51542 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1027 19:35:31.329035 51542 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: models/activity_alexnet/pretrained_model/bvlc_alexnet.caffemodel
I1027 19:35:31.719873 51542 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1027 19:35:31.761982 51542 net.cpp:761] Ignoring source layer fc8
I1027 19:35:32.261931 51542 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: models/activity_alexnet/pretrained_model/bvlc_alexnet.caffemodel
I1027 19:35:32.261977 51542 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1027 19:35:32.261981 51542 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1027 19:35:32.262006 51542 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: models/activity_alexnet/pretrained_model/bvlc_alexnet.caffemodel
I1027 19:35:32.651310 51542 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1027 19:35:32.692863 51542 net.cpp:761] Ignoring source layer fc8
I1027 19:35:32.695494 51542 caffe.cpp:251] Starting Optimization
I1027 19:35:32.695569 51542 solver.cpp:279] Solving AlexNet
I1027 19:35:32.695574 51542 solver.cpp:280] Learning Rate Policy: step
I1027 19:35:32.697298 51542 solver.cpp:337] Iteration 0, Testing net (#0)
I1027 19:36:30.096269 51542 solver.cpp:404]     Test net output #0: loss = 6.48472 (* 1 = 6.48472 loss)
I1027 19:36:30.096408 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.0017
I1027 19:36:30.096417 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.00876001
I1027 19:36:30.374959 51542 solver.cpp:228] Iteration 0, loss = 6.87631
I1027 19:36:30.375026 51542 solver.cpp:244]     Train net output #0: loss = 6.87631 (* 1 = 6.87631 loss)
I1027 19:36:30.375083 51542 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1027 19:37:08.022090 51542 solver.cpp:228] Iteration 40, loss = 6.08987
I1027 19:37:08.022326 51542 solver.cpp:244]     Train net output #0: loss = 6.08987 (* 1 = 6.08987 loss)
I1027 19:37:08.022338 51542 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I1027 19:37:45.681851 51542 solver.cpp:228] Iteration 80, loss = 5.69931
I1027 19:37:45.682157 51542 solver.cpp:244]     Train net output #0: loss = 5.69931 (* 1 = 5.69931 loss)
I1027 19:37:45.682169 51542 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I1027 19:38:23.385514 51542 solver.cpp:228] Iteration 120, loss = 5.39938
I1027 19:38:23.385795 51542 solver.cpp:244]     Train net output #0: loss = 5.39938 (* 1 = 5.39938 loss)
I1027 19:38:23.385807 51542 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I1027 19:39:01.071668 51542 solver.cpp:228] Iteration 160, loss = 5.00002
I1027 19:39:01.071950 51542 solver.cpp:244]     Train net output #0: loss = 5.00002 (* 1 = 5.00002 loss)
I1027 19:39:01.071962 51542 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I1027 19:39:38.759945 51542 solver.cpp:228] Iteration 200, loss = 4.92389
I1027 19:39:38.760215 51542 solver.cpp:244]     Train net output #0: loss = 4.92389 (* 1 = 4.92389 loss)
I1027 19:39:38.760226 51542 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1027 19:40:16.263736 51542 solver.cpp:228] Iteration 240, loss = 4.71831
I1027 19:40:16.263952 51542 solver.cpp:244]     Train net output #0: loss = 4.71831 (* 1 = 4.71831 loss)
I1027 19:40:16.263960 51542 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I1027 19:40:53.567045 51542 solver.cpp:228] Iteration 280, loss = 4.71598
I1027 19:40:53.567263 51542 solver.cpp:244]     Train net output #0: loss = 4.71598 (* 1 = 4.71598 loss)
I1027 19:40:53.567271 51542 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I1027 19:41:30.902797 51542 solver.cpp:228] Iteration 320, loss = 4.53135
I1027 19:41:30.903002 51542 solver.cpp:244]     Train net output #0: loss = 4.53135 (* 1 = 4.53135 loss)
I1027 19:41:30.903010 51542 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I1027 19:42:08.264490 51542 solver.cpp:228] Iteration 360, loss = 4.7346
I1027 19:42:08.264662 51542 solver.cpp:244]     Train net output #0: loss = 4.7346 (* 1 = 4.7346 loss)
I1027 19:42:08.264670 51542 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I1027 19:42:45.607205 51542 solver.cpp:228] Iteration 400, loss = 4.43178
I1027 19:42:45.607360 51542 solver.cpp:244]     Train net output #0: loss = 4.43178 (* 1 = 4.43178 loss)
I1027 19:42:45.607368 51542 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1027 19:43:22.887415 51542 solver.cpp:228] Iteration 440, loss = 4.44294
I1027 19:43:22.887581 51542 solver.cpp:244]     Train net output #0: loss = 4.44294 (* 1 = 4.44294 loss)
I1027 19:43:22.887589 51542 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I1027 19:44:00.192873 51542 solver.cpp:228] Iteration 480, loss = 4.27577
I1027 19:44:00.192998 51542 solver.cpp:244]     Train net output #0: loss = 4.27577 (* 1 = 4.27577 loss)
I1027 19:44:00.193006 51542 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I1027 19:44:17.900033 51542 solver.cpp:454] Snapshotting to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_500.caffemodel
I1027 19:44:24.259101 51542 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_500.solverstate
I1027 19:44:25.655011 51542 solver.cpp:337] Iteration 500, Testing net (#0)
I1027 19:45:23.367214 51542 solver.cpp:404]     Test net output #0: loss = 4.39091 (* 1 = 4.39091 loss)
I1027 19:45:23.367408 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.1412
I1027 19:45:23.367419 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.32876
I1027 19:45:42.258231 51542 solver.cpp:228] Iteration 520, loss = 4.18409
I1027 19:45:42.258247 51542 solver.cpp:244]     Train net output #0: loss = 4.18409 (* 1 = 4.18409 loss)
I1027 19:45:42.258265 51542 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I1027 19:46:19.589628 51542 solver.cpp:228] Iteration 560, loss = 4.56367
I1027 19:46:19.589776 51542 solver.cpp:244]     Train net output #0: loss = 4.56367 (* 1 = 4.56367 loss)
I1027 19:46:19.589784 51542 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I1027 19:46:56.898270 51542 solver.cpp:228] Iteration 600, loss = 4.28319
I1027 19:46:56.898468 51542 solver.cpp:244]     Train net output #0: loss = 4.28319 (* 1 = 4.28319 loss)
I1027 19:46:56.898476 51542 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1027 19:47:34.171739 51542 solver.cpp:228] Iteration 640, loss = 4.13102
I1027 19:47:34.171913 51542 solver.cpp:244]     Train net output #0: loss = 4.13102 (* 1 = 4.13102 loss)
I1027 19:47:34.171921 51542 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I1027 19:48:11.489581 51542 solver.cpp:228] Iteration 680, loss = 4.18059
I1027 19:48:11.489748 51542 solver.cpp:244]     Train net output #0: loss = 4.18059 (* 1 = 4.18059 loss)
I1027 19:48:11.489756 51542 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I1027 19:48:48.755247 51542 solver.cpp:228] Iteration 720, loss = 3.83935
I1027 19:48:48.755419 51542 solver.cpp:244]     Train net output #0: loss = 3.83935 (* 1 = 3.83935 loss)
I1027 19:48:48.755426 51542 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I1027 19:49:26.041820 51542 solver.cpp:228] Iteration 760, loss = 3.92257
I1027 19:49:26.041990 51542 solver.cpp:244]     Train net output #0: loss = 3.92257 (* 1 = 3.92257 loss)
I1027 19:49:26.042001 51542 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I1027 19:50:03.296427 51542 solver.cpp:228] Iteration 800, loss = 4.05518
I1027 19:50:03.296599 51542 solver.cpp:244]     Train net output #0: loss = 4.05518 (* 1 = 4.05518 loss)
I1027 19:50:03.296607 51542 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1027 19:50:40.538534 51542 solver.cpp:228] Iteration 840, loss = 3.94727
I1027 19:50:40.538712 51542 solver.cpp:244]     Train net output #0: loss = 3.94727 (* 1 = 3.94727 loss)
I1027 19:50:40.538720 51542 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I1027 19:51:17.850296 51542 solver.cpp:228] Iteration 880, loss = 4.00212
I1027 19:51:17.850456 51542 solver.cpp:244]     Train net output #0: loss = 4.00212 (* 1 = 4.00212 loss)
I1027 19:51:17.850464 51542 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I1027 19:51:55.144649 51542 solver.cpp:228] Iteration 920, loss = 3.9236
I1027 19:51:55.144764 51542 solver.cpp:244]     Train net output #0: loss = 3.9236 (* 1 = 3.9236 loss)
I1027 19:51:55.144773 51542 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I1027 19:52:32.415190 51542 solver.cpp:228] Iteration 960, loss = 3.8537
I1027 19:52:32.415359 51542 solver.cpp:244]     Train net output #0: loss = 3.8537 (* 1 = 3.8537 loss)
I1027 19:52:32.415367 51542 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I1027 19:53:08.757839 51542 solver.cpp:454] Snapshotting to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_1000.caffemodel
I1027 19:53:11.774232 51542 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_1000.solverstate
I1027 19:53:13.852864 51542 solver.cpp:337] Iteration 1000, Testing net (#0)
I1027 19:54:11.536788 51542 solver.cpp:404]     Test net output #0: loss = 4.06513 (* 1 = 4.06513 loss)
I1027 19:54:11.536978 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.1804
I1027 19:54:11.536986 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.39736
I1027 19:54:11.803155 51542 solver.cpp:228] Iteration 1000, loss = 3.66559
I1027 19:54:11.803172 51542 solver.cpp:244]     Train net output #0: loss = 3.66559 (* 1 = 3.66559 loss)
I1027 19:54:11.803194 51542 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1027 19:54:49.184432 51542 solver.cpp:228] Iteration 1040, loss = 3.97893
I1027 19:54:49.184602 51542 solver.cpp:244]     Train net output #0: loss = 3.97893 (* 1 = 3.97893 loss)
I1027 19:54:49.184609 51542 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I1027 19:55:26.503191 51542 solver.cpp:228] Iteration 1080, loss = 3.86794
I1027 19:55:26.503356 51542 solver.cpp:244]     Train net output #0: loss = 3.86794 (* 1 = 3.86794 loss)
I1027 19:55:26.503365 51542 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I1027 19:56:03.757323 51542 solver.cpp:228] Iteration 1120, loss = 3.87134
I1027 19:56:03.757493 51542 solver.cpp:244]     Train net output #0: loss = 3.87134 (* 1 = 3.87134 loss)
I1027 19:56:03.757501 51542 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I1027 19:56:41.075475 51542 solver.cpp:228] Iteration 1160, loss = 3.71259
I1027 19:56:41.075659 51542 solver.cpp:244]     Train net output #0: loss = 3.71259 (* 1 = 3.71259 loss)
I1027 19:56:41.075667 51542 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I1027 19:57:18.384618 51542 solver.cpp:228] Iteration 1200, loss = 3.68649
I1027 19:57:18.384793 51542 solver.cpp:244]     Train net output #0: loss = 3.68649 (* 1 = 3.68649 loss)
I1027 19:57:18.384801 51542 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1027 19:57:55.626668 51542 solver.cpp:228] Iteration 1240, loss = 3.75361
I1027 19:57:55.626837 51542 solver.cpp:244]     Train net output #0: loss = 3.75361 (* 1 = 3.75361 loss)
I1027 19:57:55.626844 51542 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I1027 19:58:33.008195 51542 solver.cpp:228] Iteration 1280, loss = 3.77753
I1027 19:58:33.008365 51542 solver.cpp:244]     Train net output #0: loss = 3.77753 (* 1 = 3.77753 loss)
I1027 19:58:33.008373 51542 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I1027 19:59:10.296514 51542 solver.cpp:228] Iteration 1320, loss = 3.57723
I1027 19:59:10.296681 51542 solver.cpp:244]     Train net output #0: loss = 3.57723 (* 1 = 3.57723 loss)
I1027 19:59:10.296689 51542 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I1027 19:59:47.602999 51542 solver.cpp:228] Iteration 1360, loss = 3.62846
I1027 19:59:47.603164 51542 solver.cpp:244]     Train net output #0: loss = 3.62846 (* 1 = 3.62846 loss)
I1027 19:59:47.603173 51542 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I1027 20:00:24.965013 51542 solver.cpp:228] Iteration 1400, loss = 3.7123
I1027 20:00:24.965229 51542 solver.cpp:244]     Train net output #0: loss = 3.7123 (* 1 = 3.7123 loss)
I1027 20:00:24.965237 51542 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I1027 20:01:02.263593 51542 solver.cpp:228] Iteration 1440, loss = 3.51026
I1027 20:01:02.263761 51542 solver.cpp:244]     Train net output #0: loss = 3.51026 (* 1 = 3.51026 loss)
I1027 20:01:02.263768 51542 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I1027 20:01:39.552893 51542 solver.cpp:228] Iteration 1480, loss = 3.5292
I1027 20:01:39.553061 51542 solver.cpp:244]     Train net output #0: loss = 3.5292 (* 1 = 3.5292 loss)
I1027 20:01:39.553069 51542 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I1027 20:01:57.235162 51542 solver.cpp:454] Snapshotting to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_1500.caffemodel
I1027 20:02:00.194269 51542 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_1500.solverstate
I1027 20:02:01.594794 51542 solver.cpp:337] Iteration 1500, Testing net (#0)
I1027 20:02:59.299293 51542 solver.cpp:404]     Test net output #0: loss = 4.03108 (* 1 = 4.03108 loss)
I1027 20:02:59.299475 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.18822
I1027 20:02:59.299485 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.40488
I1027 20:03:18.211549 51542 solver.cpp:228] Iteration 1520, loss = 3.66236
I1027 20:03:18.211565 51542 solver.cpp:244]     Train net output #0: loss = 3.66236 (* 1 = 3.66236 loss)
I1027 20:03:18.211570 51542 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I1027 20:03:55.578168 51542 solver.cpp:228] Iteration 1560, loss = 3.47294
I1027 20:03:55.578336 51542 solver.cpp:244]     Train net output #0: loss = 3.47294 (* 1 = 3.47294 loss)
I1027 20:03:55.578344 51542 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I1027 20:04:32.887135 51542 solver.cpp:228] Iteration 1600, loss = 3.45437
I1027 20:04:32.887279 51542 solver.cpp:244]     Train net output #0: loss = 3.45437 (* 1 = 3.45437 loss)
I1027 20:04:32.887289 51542 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1027 20:05:10.182422 51542 solver.cpp:228] Iteration 1640, loss = 3.45157
I1027 20:05:10.182590 51542 solver.cpp:244]     Train net output #0: loss = 3.45157 (* 1 = 3.45157 loss)
I1027 20:05:10.182598 51542 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I1027 20:05:47.441186 51542 solver.cpp:228] Iteration 1680, loss = 3.53661
I1027 20:05:47.441360 51542 solver.cpp:244]     Train net output #0: loss = 3.53661 (* 1 = 3.53661 loss)
I1027 20:05:47.441368 51542 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I1027 20:06:24.741982 51542 solver.cpp:228] Iteration 1720, loss = 3.64663
I1027 20:06:24.742159 51542 solver.cpp:244]     Train net output #0: loss = 3.64663 (* 1 = 3.64663 loss)
I1027 20:06:24.742167 51542 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I1027 20:07:01.974409 51542 solver.cpp:228] Iteration 1760, loss = 3.30217
I1027 20:07:01.974578 51542 solver.cpp:244]     Train net output #0: loss = 3.30217 (* 1 = 3.30217 loss)
I1027 20:07:01.974586 51542 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I1027 20:07:39.253211 51542 solver.cpp:228] Iteration 1800, loss = 3.51931
I1027 20:07:39.253381 51542 solver.cpp:244]     Train net output #0: loss = 3.51931 (* 1 = 3.51931 loss)
I1027 20:07:39.253388 51542 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I1027 20:08:16.529279 51542 solver.cpp:228] Iteration 1840, loss = 3.52134
I1027 20:08:16.529444 51542 solver.cpp:244]     Train net output #0: loss = 3.52134 (* 1 = 3.52134 loss)
I1027 20:08:16.529451 51542 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I1027 20:08:53.822780 51542 solver.cpp:228] Iteration 1880, loss = 3.41223
I1027 20:08:53.822955 51542 solver.cpp:244]     Train net output #0: loss = 3.41223 (* 1 = 3.41223 loss)
I1027 20:08:53.822963 51542 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I1027 20:09:31.061210 51542 solver.cpp:228] Iteration 1920, loss = 3.12378
I1027 20:09:31.061378 51542 solver.cpp:244]     Train net output #0: loss = 3.12378 (* 1 = 3.12378 loss)
I1027 20:09:31.061386 51542 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I1027 20:10:08.317041 51542 solver.cpp:228] Iteration 1960, loss = 3.37052
I1027 20:10:08.317209 51542 solver.cpp:244]     Train net output #0: loss = 3.37052 (* 1 = 3.37052 loss)
I1027 20:10:08.317216 51542 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I1027 20:10:44.634018 51542 solver.cpp:454] Snapshotting to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_2000.caffemodel
I1027 20:10:47.595842 51542 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_2000.solverstate
I1027 20:10:49.010566 51542 solver.cpp:337] Iteration 2000, Testing net (#0)
I1027 20:11:46.680599 51542 solver.cpp:404]     Test net output #0: loss = 4.00842 (* 1 = 4.00842 loss)
I1027 20:11:46.680785 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.19034
I1027 20:11:46.680795 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.40954
I1027 20:11:46.932492 51542 solver.cpp:228] Iteration 2000, loss = 3.36749
I1027 20:11:46.932508 51542 solver.cpp:244]     Train net output #0: loss = 3.36749 (* 1 = 3.36749 loss)
I1027 20:11:46.932528 51542 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1027 20:12:24.245860 51542 solver.cpp:228] Iteration 2040, loss = 3.21115
I1027 20:12:24.246021 51542 solver.cpp:244]     Train net output #0: loss = 3.21115 (* 1 = 3.21115 loss)
I1027 20:12:24.246029 51542 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I1027 20:13:01.549769 51542 solver.cpp:228] Iteration 2080, loss = 3.03313
I1027 20:13:01.549932 51542 solver.cpp:244]     Train net output #0: loss = 3.03313 (* 1 = 3.03313 loss)
I1027 20:13:01.549940 51542 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I1027 20:13:38.910578 51542 solver.cpp:228] Iteration 2120, loss = 3.43223
I1027 20:13:38.910744 51542 solver.cpp:244]     Train net output #0: loss = 3.43223 (* 1 = 3.43223 loss)
I1027 20:13:38.910753 51542 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I1027 20:14:16.197984 51542 solver.cpp:228] Iteration 2160, loss = 3.28563
I1027 20:14:16.198158 51542 solver.cpp:244]     Train net output #0: loss = 3.28563 (* 1 = 3.28563 loss)
I1027 20:14:16.198165 51542 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I1027 20:14:53.498699 51542 solver.cpp:228] Iteration 2200, loss = 3.37854
I1027 20:14:53.498865 51542 solver.cpp:244]     Train net output #0: loss = 3.37854 (* 1 = 3.37854 loss)
I1027 20:14:53.498873 51542 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1027 20:15:30.858875 51542 solver.cpp:228] Iteration 2240, loss = 3.36722
I1027 20:15:30.859112 51542 solver.cpp:244]     Train net output #0: loss = 3.36722 (* 1 = 3.36722 loss)
I1027 20:15:30.859120 51542 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I1027 20:16:08.200588 51542 solver.cpp:228] Iteration 2280, loss = 3.23099
I1027 20:16:08.200762 51542 solver.cpp:244]     Train net output #0: loss = 3.23099 (* 1 = 3.23099 loss)
I1027 20:16:08.200769 51542 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I1027 20:16:45.571905 51542 solver.cpp:228] Iteration 2320, loss = 3.32208
I1027 20:16:45.572067 51542 solver.cpp:244]     Train net output #0: loss = 3.32208 (* 1 = 3.32208 loss)
I1027 20:16:45.572075 51542 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I1027 20:17:22.885370 51542 solver.cpp:228] Iteration 2360, loss = 3.07536
I1027 20:17:22.885537 51542 solver.cpp:244]     Train net output #0: loss = 3.07536 (* 1 = 3.07536 loss)
I1027 20:17:22.885545 51542 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I1027 20:18:00.201144 51542 solver.cpp:228] Iteration 2400, loss = 3.13319
I1027 20:18:00.201311 51542 solver.cpp:244]     Train net output #0: loss = 3.13319 (* 1 = 3.13319 loss)
I1027 20:18:00.201319 51542 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I1027 20:18:37.471709 51542 solver.cpp:228] Iteration 2440, loss = 3.1675
I1027 20:18:37.471881 51542 solver.cpp:244]     Train net output #0: loss = 3.1675 (* 1 = 3.1675 loss)
I1027 20:18:37.471889 51542 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I1027 20:19:14.812258 51542 solver.cpp:228] Iteration 2480, loss = 3.23129
I1027 20:19:14.812422 51542 solver.cpp:244]     Train net output #0: loss = 3.23129 (* 1 = 3.23129 loss)
I1027 20:19:14.812430 51542 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I1027 20:19:32.536356 51542 solver.cpp:454] Snapshotting to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_2500.caffemodel
I1027 20:19:35.496997 51542 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_2500.solverstate
I1027 20:19:36.896589 51542 solver.cpp:337] Iteration 2500, Testing net (#0)
I1027 20:20:34.566896 51542 solver.cpp:404]     Test net output #0: loss = 4.08579 (* 1 = 4.08579 loss)
I1027 20:20:34.567050 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.19046
I1027 20:20:34.567059 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.40256
I1027 20:20:53.462903 51542 solver.cpp:228] Iteration 2520, loss = 3.03252
I1027 20:20:53.462919 51542 solver.cpp:244]     Train net output #0: loss = 3.03252 (* 1 = 3.03252 loss)
I1027 20:20:53.462937 51542 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I1027 20:21:30.808686 51542 solver.cpp:228] Iteration 2560, loss = 3.40939
I1027 20:21:30.808835 51542 solver.cpp:244]     Train net output #0: loss = 3.40939 (* 1 = 3.40939 loss)
I1027 20:21:30.808842 51542 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I1027 20:22:08.141425 51542 solver.cpp:228] Iteration 2600, loss = 3.07159
I1027 20:22:08.141590 51542 solver.cpp:244]     Train net output #0: loss = 3.07159 (* 1 = 3.07159 loss)
I1027 20:22:08.141598 51542 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1027 20:22:45.514564 51542 solver.cpp:228] Iteration 2640, loss = 3.47507
I1027 20:22:45.514730 51542 solver.cpp:244]     Train net output #0: loss = 3.47507 (* 1 = 3.47507 loss)
I1027 20:22:45.514739 51542 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I1027 20:23:22.851274 51542 solver.cpp:228] Iteration 2680, loss = 3.20686
I1027 20:23:22.852984 51542 solver.cpp:244]     Train net output #0: loss = 3.20686 (* 1 = 3.20686 loss)
I1027 20:23:22.853001 51542 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I1027 20:24:00.504670 51542 solver.cpp:228] Iteration 2720, loss = 3.13936
I1027 20:24:00.504894 51542 solver.cpp:244]     Train net output #0: loss = 3.13936 (* 1 = 3.13936 loss)
I1027 20:24:00.504909 51542 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I1027 20:24:38.177678 51542 solver.cpp:228] Iteration 2760, loss = 3.16106
I1027 20:24:38.177935 51542 solver.cpp:244]     Train net output #0: loss = 3.16106 (* 1 = 3.16106 loss)
I1027 20:24:38.177947 51542 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I1027 20:25:15.827337 51542 solver.cpp:228] Iteration 2800, loss = 3.03689
I1027 20:25:15.827576 51542 solver.cpp:244]     Train net output #0: loss = 3.03689 (* 1 = 3.03689 loss)
I1027 20:25:15.827587 51542 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I1027 20:25:53.432991 51542 solver.cpp:228] Iteration 2840, loss = 3.2372
I1027 20:25:53.433204 51542 solver.cpp:244]     Train net output #0: loss = 3.2372 (* 1 = 3.2372 loss)
I1027 20:25:53.433212 51542 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I1027 20:26:30.693675 51542 solver.cpp:228] Iteration 2880, loss = 3.12441
I1027 20:26:30.693888 51542 solver.cpp:244]     Train net output #0: loss = 3.12441 (* 1 = 3.12441 loss)
I1027 20:26:30.693897 51542 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I1027 20:27:07.947975 51542 solver.cpp:228] Iteration 2920, loss = 2.9915
I1027 20:27:07.948199 51542 solver.cpp:244]     Train net output #0: loss = 2.9915 (* 1 = 2.9915 loss)
I1027 20:27:07.948207 51542 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I1027 20:27:45.229822 51542 solver.cpp:228] Iteration 2960, loss = 3.09136
I1027 20:27:45.230002 51542 solver.cpp:244]     Train net output #0: loss = 3.09136 (* 1 = 3.09136 loss)
I1027 20:27:45.230010 51542 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I1027 20:28:21.562054 51542 solver.cpp:454] Snapshotting to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_3000.caffemodel
I1027 20:28:24.548916 51542 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/activity_alexnet/snapshots/caffe_alexnet_train_iter_3000.solverstate
I1027 20:28:25.970851 51542 solver.cpp:337] Iteration 3000, Testing net (#0)
I1027 20:29:23.707736 51542 solver.cpp:404]     Test net output #0: loss = 4.07034 (* 1 = 4.07034 loss)
I1027 20:29:23.707907 51542 solver.cpp:404]     Test net output #1: precision@1 = 0.19098
I1027 20:29:23.707917 51542 solver.cpp:404]     Test net output #2: precision@5 = 0.404
I1027 20:29:23.970526 51542 solver.cpp:228] Iteration 3000, loss = 2.93875
I1027 20:29:23.970553 51542 solver.cpp:244]     Train net output #0: loss = 2.93875 (* 1 = 2.93875 loss)
I1027 20:29:23.970562 51542 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1027 20:30:01.203055 51542 solver.cpp:228] Iteration 3040, loss = 3.07014
I1027 20:30:01.203224 51542 solver.cpp:244]     Train net output #0: loss = 3.07014 (* 1 = 3.07014 loss)
I1027 20:30:01.203233 51542 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I1027 20:30:38.451596 51542 solver.cpp:228] Iteration 3080, loss = 3.09183
I1027 20:30:38.451787 51542 solver.cpp:244]     Train net output #0: loss = 3.09183 (* 1 = 3.09183 loss)
I1027 20:30:38.451797 51542 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
