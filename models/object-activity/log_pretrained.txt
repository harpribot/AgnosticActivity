I1028 01:25:47.537124 62912 caffe.cpp:185] Using GPUs 0
I1028 01:25:47.543175 62912 caffe.cpp:190] GPU 0: Tesla K40m
I1028 01:25:47.795554 62912 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 500
base_lr: 0.01
display: 40
max_iter: 20000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 1000
snapshot_prefix: "models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train"
solver_mode: GPU
device_id: 0
net: "models/dissimilarity_siamese_net/train_val.prototxt"
I1028 01:25:47.797755 62912 solver.cpp:91] Creating training net from net file: models/dissimilarity_siamese_net/train_val.prototxt
I1028 01:25:47.801023 62912 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1028 01:25:47.801095 62912 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer precision@1
I1028 01:25:47.801100 62912 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer precision@5
I1028 01:25:47.802121 62912 net.cpp:49] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "examples/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/work/03713/harshal1/maverick/vision_proj/data/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "fc7_n"
  type: "L2Norm"
  bottom: "fc7"
  top: "fc7_n"
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data"
  top: "conv1_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "conv1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "norm1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "conv2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "norm2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "pool2_p"
  top: "conv3_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "pool5_p"
  type: "Pooling"
  bottom: "conv5_p"
  top: "pool5_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "pool5_p"
  top: "fc6_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7_p"
  type: "ReLU"
  bottom: "fc7_p"
  top: "fc7_p"
}
layer {
  name: "drop7_p"
  type: "Dropout"
  bottom: "fc7_p"
  top: "fc7_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 504
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc7_pn"
  type: "L2Norm"
  bottom: "fc7_p"
  top: "fc7_pn"
}
layer {
  name: "loss_p1"
  type: "EuclideanLoss"
  bottom: "fc7_n"
  bottom: "fc7_pn"
  top: "loss_p1"
  loss_weight: -0.35
}
layer {
  name: "loss_p2"
  type: "SoftmaxWithLoss"
  bottom: "fc8_p"
  bottom: "label"
  top: "loss_p2"
  loss_weight: 0.65
}
I1028 01:25:47.802513 62912 layer_factory.hpp:77] Creating layer data
I1028 01:25:47.803342 62912 net.cpp:91] Creating Layer data
I1028 01:25:47.803416 62912 net.cpp:399] data -> data
I1028 01:25:47.803524 62912 net.cpp:399] data -> label
I1028 01:25:47.803565 62912 data_transformer.cpp:25] Loading mean file from: examples/imagenet/imagenet_mean.binaryproto
I1028 01:25:47.813841 63023 db_lmdb.cpp:35] Opened lmdb /work/03713/harshal1/maverick/vision_proj/data/train_lmdb
I1028 01:25:47.825987 62912 data_layer.cpp:41] output data size: 256,3,227,227
I1028 01:25:48.131472 62912 net.cpp:141] Setting up data
I1028 01:25:48.131577 62912 net.cpp:148] Top shape: 256 3 227 227 (39574272)
I1028 01:25:48.131587 62912 net.cpp:148] Top shape: 256 (256)
I1028 01:25:48.131590 62912 net.cpp:156] Memory required for data: 158298112
I1028 01:25:48.131608 62912 layer_factory.hpp:77] Creating layer data_data_0_split
I1028 01:25:48.131654 62912 net.cpp:91] Creating Layer data_data_0_split
I1028 01:25:48.131690 62912 net.cpp:425] data_data_0_split <- data
I1028 01:25:48.131716 62912 net.cpp:399] data_data_0_split -> data_data_0_split_0
I1028 01:25:48.131732 62912 net.cpp:399] data_data_0_split -> data_data_0_split_1
I1028 01:25:48.131819 62912 net.cpp:141] Setting up data_data_0_split
I1028 01:25:48.131830 62912 net.cpp:148] Top shape: 256 3 227 227 (39574272)
I1028 01:25:48.131834 62912 net.cpp:148] Top shape: 256 3 227 227 (39574272)
I1028 01:25:48.131837 62912 net.cpp:156] Memory required for data: 474892288
I1028 01:25:48.131841 62912 layer_factory.hpp:77] Creating layer conv1
I1028 01:25:48.131901 62912 net.cpp:91] Creating Layer conv1
I1028 01:25:48.131922 62912 net.cpp:425] conv1 <- data_data_0_split_0
I1028 01:25:48.131929 62912 net.cpp:399] conv1 -> conv1
I1028 01:25:48.396598 62912 net.cpp:141] Setting up conv1
I1028 01:25:48.396646 62912 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 01:25:48.396651 62912 net.cpp:156] Memory required for data: 772261888
I1028 01:25:48.396773 62912 layer_factory.hpp:77] Creating layer relu1
I1028 01:25:48.396819 62912 net.cpp:91] Creating Layer relu1
I1028 01:25:48.396829 62912 net.cpp:425] relu1 <- conv1
I1028 01:25:48.396839 62912 net.cpp:386] relu1 -> conv1 (in-place)
I1028 01:25:48.397153 62912 net.cpp:141] Setting up relu1
I1028 01:25:48.397178 62912 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 01:25:48.397181 62912 net.cpp:156] Memory required for data: 1069631488
I1028 01:25:48.397186 62912 layer_factory.hpp:77] Creating layer norm1
I1028 01:25:48.397248 62912 net.cpp:91] Creating Layer norm1
I1028 01:25:48.397254 62912 net.cpp:425] norm1 <- conv1
I1028 01:25:48.397260 62912 net.cpp:399] norm1 -> norm1
I1028 01:25:48.397521 62912 net.cpp:141] Setting up norm1
I1028 01:25:48.397532 62912 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 01:25:48.397536 62912 net.cpp:156] Memory required for data: 1367001088
I1028 01:25:48.397541 62912 layer_factory.hpp:77] Creating layer pool1
I1028 01:25:48.397549 62912 net.cpp:91] Creating Layer pool1
I1028 01:25:48.397553 62912 net.cpp:425] pool1 <- norm1
I1028 01:25:48.397559 62912 net.cpp:399] pool1 -> pool1
I1028 01:25:48.397655 62912 net.cpp:141] Setting up pool1
I1028 01:25:48.397666 62912 net.cpp:148] Top shape: 256 96 27 27 (17915904)
I1028 01:25:48.397670 62912 net.cpp:156] Memory required for data: 1438664704
I1028 01:25:48.397673 62912 layer_factory.hpp:77] Creating layer conv2
I1028 01:25:48.397691 62912 net.cpp:91] Creating Layer conv2
I1028 01:25:48.397694 62912 net.cpp:425] conv2 <- pool1
I1028 01:25:48.397701 62912 net.cpp:399] conv2 -> conv2
I1028 01:25:48.404880 62912 net.cpp:141] Setting up conv2
I1028 01:25:48.404911 62912 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 01:25:48.404917 62912 net.cpp:156] Memory required for data: 1629767680
I1028 01:25:48.404927 62912 layer_factory.hpp:77] Creating layer relu2
I1028 01:25:48.404935 62912 net.cpp:91] Creating Layer relu2
I1028 01:25:48.404939 62912 net.cpp:425] relu2 <- conv2
I1028 01:25:48.404947 62912 net.cpp:386] relu2 -> conv2 (in-place)
I1028 01:25:48.405130 62912 net.cpp:141] Setting up relu2
I1028 01:25:48.405143 62912 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 01:25:48.405148 62912 net.cpp:156] Memory required for data: 1820870656
I1028 01:25:48.405151 62912 layer_factory.hpp:77] Creating layer norm2
I1028 01:25:48.405160 62912 net.cpp:91] Creating Layer norm2
I1028 01:25:48.405164 62912 net.cpp:425] norm2 <- conv2
I1028 01:25:48.405174 62912 net.cpp:399] norm2 -> norm2
I1028 01:25:48.405488 62912 net.cpp:141] Setting up norm2
I1028 01:25:48.405500 62912 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 01:25:48.405504 62912 net.cpp:156] Memory required for data: 2011973632
I1028 01:25:48.405508 62912 layer_factory.hpp:77] Creating layer pool2
I1028 01:25:48.405517 62912 net.cpp:91] Creating Layer pool2
I1028 01:25:48.405522 62912 net.cpp:425] pool2 <- norm2
I1028 01:25:48.405527 62912 net.cpp:399] pool2 -> pool2
I1028 01:25:48.405578 62912 net.cpp:141] Setting up pool2
I1028 01:25:48.405586 62912 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 01:25:48.405588 62912 net.cpp:156] Memory required for data: 2056275968
I1028 01:25:48.405592 62912 layer_factory.hpp:77] Creating layer conv3
I1028 01:25:48.405604 62912 net.cpp:91] Creating Layer conv3
I1028 01:25:48.405609 62912 net.cpp:425] conv3 <- pool2
I1028 01:25:48.405618 62912 net.cpp:399] conv3 -> conv3
I1028 01:25:48.420680 62912 net.cpp:141] Setting up conv3
I1028 01:25:48.420717 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:48.420722 62912 net.cpp:156] Memory required for data: 2122729472
I1028 01:25:48.420737 62912 layer_factory.hpp:77] Creating layer relu3
I1028 01:25:48.420750 62912 net.cpp:91] Creating Layer relu3
I1028 01:25:48.420755 62912 net.cpp:425] relu3 <- conv3
I1028 01:25:48.420763 62912 net.cpp:386] relu3 -> conv3 (in-place)
I1028 01:25:48.420931 62912 net.cpp:141] Setting up relu3
I1028 01:25:48.420944 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:48.420979 62912 net.cpp:156] Memory required for data: 2189182976
I1028 01:25:48.420984 62912 layer_factory.hpp:77] Creating layer conv4
I1028 01:25:48.421000 62912 net.cpp:91] Creating Layer conv4
I1028 01:25:48.421005 62912 net.cpp:425] conv4 <- conv3
I1028 01:25:48.421011 62912 net.cpp:399] conv4 -> conv4
I1028 01:25:48.433290 62912 net.cpp:141] Setting up conv4
I1028 01:25:48.433323 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:48.433327 62912 net.cpp:156] Memory required for data: 2255636480
I1028 01:25:48.433336 62912 layer_factory.hpp:77] Creating layer relu4
I1028 01:25:48.433346 62912 net.cpp:91] Creating Layer relu4
I1028 01:25:48.433351 62912 net.cpp:425] relu4 <- conv4
I1028 01:25:48.433357 62912 net.cpp:386] relu4 -> conv4 (in-place)
I1028 01:25:48.433535 62912 net.cpp:141] Setting up relu4
I1028 01:25:48.433547 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:48.433550 62912 net.cpp:156] Memory required for data: 2322089984
I1028 01:25:48.433554 62912 layer_factory.hpp:77] Creating layer conv5
I1028 01:25:48.433568 62912 net.cpp:91] Creating Layer conv5
I1028 01:25:48.433573 62912 net.cpp:425] conv5 <- conv4
I1028 01:25:48.433583 62912 net.cpp:399] conv5 -> conv5
I1028 01:25:48.442399 62912 net.cpp:141] Setting up conv5
I1028 01:25:48.442414 62912 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 01:25:48.442430 62912 net.cpp:156] Memory required for data: 2366392320
I1028 01:25:48.442442 62912 layer_factory.hpp:77] Creating layer relu5
I1028 01:25:48.442454 62912 net.cpp:91] Creating Layer relu5
I1028 01:25:48.442458 62912 net.cpp:425] relu5 <- conv5
I1028 01:25:48.442464 62912 net.cpp:386] relu5 -> conv5 (in-place)
I1028 01:25:48.442632 62912 net.cpp:141] Setting up relu5
I1028 01:25:48.442646 62912 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 01:25:48.442651 62912 net.cpp:156] Memory required for data: 2410694656
I1028 01:25:48.442654 62912 layer_factory.hpp:77] Creating layer pool5
I1028 01:25:48.442665 62912 net.cpp:91] Creating Layer pool5
I1028 01:25:48.442669 62912 net.cpp:425] pool5 <- conv5
I1028 01:25:48.442678 62912 net.cpp:399] pool5 -> pool5
I1028 01:25:48.442723 62912 net.cpp:141] Setting up pool5
I1028 01:25:48.442734 62912 net.cpp:148] Top shape: 256 256 6 6 (2359296)
I1028 01:25:48.442739 62912 net.cpp:156] Memory required for data: 2420131840
I1028 01:25:48.442741 62912 layer_factory.hpp:77] Creating layer fc6
I1028 01:25:48.442751 62912 net.cpp:91] Creating Layer fc6
I1028 01:25:48.442755 62912 net.cpp:425] fc6 <- pool5
I1028 01:25:48.442764 62912 net.cpp:399] fc6 -> fc6
I1028 01:25:48.978647 62912 net.cpp:141] Setting up fc6
I1028 01:25:48.978695 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:48.978699 62912 net.cpp:156] Memory required for data: 2424326144
I1028 01:25:48.978711 62912 layer_factory.hpp:77] Creating layer relu6
I1028 01:25:48.978726 62912 net.cpp:91] Creating Layer relu6
I1028 01:25:48.978732 62912 net.cpp:425] relu6 <- fc6
I1028 01:25:48.978741 62912 net.cpp:386] relu6 -> fc6 (in-place)
I1028 01:25:48.979171 62912 net.cpp:141] Setting up relu6
I1028 01:25:48.979195 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:48.979198 62912 net.cpp:156] Memory required for data: 2428520448
I1028 01:25:48.979202 62912 layer_factory.hpp:77] Creating layer drop6
I1028 01:25:48.979275 62912 net.cpp:91] Creating Layer drop6
I1028 01:25:48.979281 62912 net.cpp:425] drop6 <- fc6
I1028 01:25:48.979287 62912 net.cpp:386] drop6 -> fc6 (in-place)
I1028 01:25:48.979346 62912 net.cpp:141] Setting up drop6
I1028 01:25:48.979354 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:48.979358 62912 net.cpp:156] Memory required for data: 2432714752
I1028 01:25:48.979362 62912 layer_factory.hpp:77] Creating layer fc7
I1028 01:25:48.979372 62912 net.cpp:91] Creating Layer fc7
I1028 01:25:48.979377 62912 net.cpp:425] fc7 <- fc6
I1028 01:25:48.979384 62912 net.cpp:399] fc7 -> fc7
I1028 01:25:49.211482 62912 net.cpp:141] Setting up fc7
I1028 01:25:49.211532 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:49.211572 62912 net.cpp:156] Memory required for data: 2436909056
I1028 01:25:49.211585 62912 layer_factory.hpp:77] Creating layer fc7_n
I1028 01:25:49.211661 62912 net.cpp:91] Creating Layer fc7_n
I1028 01:25:49.211668 62912 net.cpp:425] fc7_n <- fc7
I1028 01:25:49.211683 62912 net.cpp:399] fc7_n -> fc7_n
I1028 01:25:49.211720 62912 net.cpp:141] Setting up fc7_n
I1028 01:25:49.211730 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:49.211731 62912 net.cpp:156] Memory required for data: 2441103360
I1028 01:25:49.211735 62912 layer_factory.hpp:77] Creating layer conv1_p
I1028 01:25:49.211755 62912 net.cpp:91] Creating Layer conv1_p
I1028 01:25:49.211758 62912 net.cpp:425] conv1_p <- data_data_0_split_1
I1028 01:25:49.211766 62912 net.cpp:399] conv1_p -> conv1_p
I1028 01:25:49.213323 62912 net.cpp:141] Setting up conv1_p
I1028 01:25:49.213348 62912 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 01:25:49.213351 62912 net.cpp:156] Memory required for data: 2738472960
I1028 01:25:49.213357 62912 layer_factory.hpp:77] Creating layer relu1_p
I1028 01:25:49.213366 62912 net.cpp:91] Creating Layer relu1_p
I1028 01:25:49.213369 62912 net.cpp:425] relu1_p <- conv1_p
I1028 01:25:49.213374 62912 net.cpp:386] relu1_p -> conv1_p (in-place)
I1028 01:25:49.213524 62912 net.cpp:141] Setting up relu1_p
I1028 01:25:49.213533 62912 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 01:25:49.213537 62912 net.cpp:156] Memory required for data: 3035842560
I1028 01:25:49.213541 62912 layer_factory.hpp:77] Creating layer norm1_p
I1028 01:25:49.213552 62912 net.cpp:91] Creating Layer norm1_p
I1028 01:25:49.213557 62912 net.cpp:425] norm1_p <- conv1_p
I1028 01:25:49.213562 62912 net.cpp:399] norm1_p -> norm1_p
I1028 01:25:49.213832 62912 net.cpp:141] Setting up norm1_p
I1028 01:25:49.213856 62912 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 01:25:49.213860 62912 net.cpp:156] Memory required for data: 3333212160
I1028 01:25:49.213863 62912 layer_factory.hpp:77] Creating layer pool1_p
I1028 01:25:49.213871 62912 net.cpp:91] Creating Layer pool1_p
I1028 01:25:49.213874 62912 net.cpp:425] pool1_p <- norm1_p
I1028 01:25:49.213879 62912 net.cpp:399] pool1_p -> pool1_p
I1028 01:25:49.213934 62912 net.cpp:141] Setting up pool1_p
I1028 01:25:49.213942 62912 net.cpp:148] Top shape: 256 96 27 27 (17915904)
I1028 01:25:49.213944 62912 net.cpp:156] Memory required for data: 3404875776
I1028 01:25:49.213948 62912 layer_factory.hpp:77] Creating layer conv2_p
I1028 01:25:49.213958 62912 net.cpp:91] Creating Layer conv2_p
I1028 01:25:49.213963 62912 net.cpp:425] conv2_p <- pool1_p
I1028 01:25:49.213981 62912 net.cpp:399] conv2_p -> conv2_p
I1028 01:25:49.219640 62912 net.cpp:141] Setting up conv2_p
I1028 01:25:49.219665 62912 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 01:25:49.219668 62912 net.cpp:156] Memory required for data: 3595978752
I1028 01:25:49.219679 62912 layer_factory.hpp:77] Creating layer relu2_p
I1028 01:25:49.219688 62912 net.cpp:91] Creating Layer relu2_p
I1028 01:25:49.219692 62912 net.cpp:425] relu2_p <- conv2_p
I1028 01:25:49.219697 62912 net.cpp:386] relu2_p -> conv2_p (in-place)
I1028 01:25:49.219965 62912 net.cpp:141] Setting up relu2_p
I1028 01:25:49.219979 62912 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 01:25:49.219982 62912 net.cpp:156] Memory required for data: 3787081728
I1028 01:25:49.219986 62912 layer_factory.hpp:77] Creating layer norm2_p
I1028 01:25:49.219993 62912 net.cpp:91] Creating Layer norm2_p
I1028 01:25:49.219997 62912 net.cpp:425] norm2_p <- conv2_p
I1028 01:25:49.220002 62912 net.cpp:399] norm2_p -> norm2_p
I1028 01:25:49.220180 62912 net.cpp:141] Setting up norm2_p
I1028 01:25:49.220190 62912 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 01:25:49.220192 62912 net.cpp:156] Memory required for data: 3978184704
I1028 01:25:49.220196 62912 layer_factory.hpp:77] Creating layer pool2_p
I1028 01:25:49.220204 62912 net.cpp:91] Creating Layer pool2_p
I1028 01:25:49.220208 62912 net.cpp:425] pool2_p <- norm2_p
I1028 01:25:49.220213 62912 net.cpp:399] pool2_p -> pool2_p
I1028 01:25:49.220265 62912 net.cpp:141] Setting up pool2_p
I1028 01:25:49.220274 62912 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 01:25:49.220278 62912 net.cpp:156] Memory required for data: 4022487040
I1028 01:25:49.220280 62912 layer_factory.hpp:77] Creating layer conv3_p
I1028 01:25:49.220291 62912 net.cpp:91] Creating Layer conv3_p
I1028 01:25:49.220295 62912 net.cpp:425] conv3_p <- pool2_p
I1028 01:25:49.220304 62912 net.cpp:399] conv3_p -> conv3_p
I1028 01:25:49.233554 62912 net.cpp:141] Setting up conv3_p
I1028 01:25:49.233566 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:49.233580 62912 net.cpp:156] Memory required for data: 4088940544
I1028 01:25:49.233587 62912 layer_factory.hpp:77] Creating layer relu3_p
I1028 01:25:49.233597 62912 net.cpp:91] Creating Layer relu3_p
I1028 01:25:49.233600 62912 net.cpp:425] relu3_p <- conv3_p
I1028 01:25:49.233605 62912 net.cpp:386] relu3_p -> conv3_p (in-place)
I1028 01:25:49.233850 62912 net.cpp:141] Setting up relu3_p
I1028 01:25:49.233860 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:49.233862 62912 net.cpp:156] Memory required for data: 4155394048
I1028 01:25:49.233866 62912 layer_factory.hpp:77] Creating layer conv4_p
I1028 01:25:49.233883 62912 net.cpp:91] Creating Layer conv4_p
I1028 01:25:49.233887 62912 net.cpp:425] conv4_p <- conv3_p
I1028 01:25:49.233898 62912 net.cpp:399] conv4_p -> conv4_p
I1028 01:25:49.244624 62912 net.cpp:141] Setting up conv4_p
I1028 01:25:49.244635 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:49.244650 62912 net.cpp:156] Memory required for data: 4221847552
I1028 01:25:49.244658 62912 layer_factory.hpp:77] Creating layer relu4_p
I1028 01:25:49.244666 62912 net.cpp:91] Creating Layer relu4_p
I1028 01:25:49.244669 62912 net.cpp:425] relu4_p <- conv4_p
I1028 01:25:49.244675 62912 net.cpp:386] relu4_p -> conv4_p (in-place)
I1028 01:25:49.244940 62912 net.cpp:141] Setting up relu4_p
I1028 01:25:49.244953 62912 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 01:25:49.244956 62912 net.cpp:156] Memory required for data: 4288301056
I1028 01:25:49.244961 62912 layer_factory.hpp:77] Creating layer conv5_p
I1028 01:25:49.244972 62912 net.cpp:91] Creating Layer conv5_p
I1028 01:25:49.244976 62912 net.cpp:425] conv5_p <- conv4_p
I1028 01:25:49.244983 62912 net.cpp:399] conv5_p -> conv5_p
I1028 01:25:49.252678 62912 net.cpp:141] Setting up conv5_p
I1028 01:25:49.252703 62912 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 01:25:49.252707 62912 net.cpp:156] Memory required for data: 4332603392
I1028 01:25:49.252712 62912 layer_factory.hpp:77] Creating layer relu5_p
I1028 01:25:49.252719 62912 net.cpp:91] Creating Layer relu5_p
I1028 01:25:49.252723 62912 net.cpp:425] relu5_p <- conv5_p
I1028 01:25:49.252728 62912 net.cpp:386] relu5_p -> conv5_p (in-place)
I1028 01:25:49.252995 62912 net.cpp:141] Setting up relu5_p
I1028 01:25:49.253008 62912 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 01:25:49.253011 62912 net.cpp:156] Memory required for data: 4376905728
I1028 01:25:49.253015 62912 layer_factory.hpp:77] Creating layer pool5_p
I1028 01:25:49.253021 62912 net.cpp:91] Creating Layer pool5_p
I1028 01:25:49.253026 62912 net.cpp:425] pool5_p <- conv5_p
I1028 01:25:49.253031 62912 net.cpp:399] pool5_p -> pool5_p
I1028 01:25:49.253073 62912 net.cpp:141] Setting up pool5_p
I1028 01:25:49.253082 62912 net.cpp:148] Top shape: 256 256 6 6 (2359296)
I1028 01:25:49.253085 62912 net.cpp:156] Memory required for data: 4386342912
I1028 01:25:49.253088 62912 layer_factory.hpp:77] Creating layer fc6_p
I1028 01:25:49.253099 62912 net.cpp:91] Creating Layer fc6_p
I1028 01:25:49.253103 62912 net.cpp:425] fc6_p <- pool5_p
I1028 01:25:49.253109 62912 net.cpp:399] fc6_p -> fc6_p
I1028 01:25:49.784627 62912 net.cpp:141] Setting up fc6_p
I1028 01:25:49.784677 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:49.784680 62912 net.cpp:156] Memory required for data: 4390537216
I1028 01:25:49.784693 62912 layer_factory.hpp:77] Creating layer relu6_p
I1028 01:25:49.784746 62912 net.cpp:91] Creating Layer relu6_p
I1028 01:25:49.784752 62912 net.cpp:425] relu6_p <- fc6_p
I1028 01:25:49.784765 62912 net.cpp:386] relu6_p -> fc6_p (in-place)
I1028 01:25:49.785233 62912 net.cpp:141] Setting up relu6_p
I1028 01:25:49.785254 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:49.785257 62912 net.cpp:156] Memory required for data: 4394731520
I1028 01:25:49.785261 62912 layer_factory.hpp:77] Creating layer drop6_p
I1028 01:25:49.785270 62912 net.cpp:91] Creating Layer drop6_p
I1028 01:25:49.785274 62912 net.cpp:425] drop6_p <- fc6_p
I1028 01:25:49.785281 62912 net.cpp:386] drop6_p -> fc6_p (in-place)
I1028 01:25:49.785305 62912 net.cpp:141] Setting up drop6_p
I1028 01:25:49.785311 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:49.785316 62912 net.cpp:156] Memory required for data: 4398925824
I1028 01:25:49.785320 62912 layer_factory.hpp:77] Creating layer fc7_p
I1028 01:25:49.785331 62912 net.cpp:91] Creating Layer fc7_p
I1028 01:25:49.785334 62912 net.cpp:425] fc7_p <- fc6_p
I1028 01:25:49.785339 62912 net.cpp:399] fc7_p -> fc7_p
I1028 01:25:50.020231 62912 net.cpp:141] Setting up fc7_p
I1028 01:25:50.020282 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:50.020287 62912 net.cpp:156] Memory required for data: 4403120128
I1028 01:25:50.020299 62912 layer_factory.hpp:77] Creating layer relu7_p
I1028 01:25:50.020319 62912 net.cpp:91] Creating Layer relu7_p
I1028 01:25:50.020326 62912 net.cpp:425] relu7_p <- fc7_p
I1028 01:25:50.020335 62912 net.cpp:386] relu7_p -> fc7_p (in-place)
I1028 01:25:50.020572 62912 net.cpp:141] Setting up relu7_p
I1028 01:25:50.020582 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:50.020586 62912 net.cpp:156] Memory required for data: 4407314432
I1028 01:25:50.020589 62912 layer_factory.hpp:77] Creating layer drop7_p
I1028 01:25:50.020598 62912 net.cpp:91] Creating Layer drop7_p
I1028 01:25:50.020601 62912 net.cpp:425] drop7_p <- fc7_p
I1028 01:25:50.020608 62912 net.cpp:386] drop7_p -> fc7_p (in-place)
I1028 01:25:50.020632 62912 net.cpp:141] Setting up drop7_p
I1028 01:25:50.020638 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:50.020642 62912 net.cpp:156] Memory required for data: 4411508736
I1028 01:25:50.020644 62912 layer_factory.hpp:77] Creating layer fc7_p_drop7_p_0_split
I1028 01:25:50.020653 62912 net.cpp:91] Creating Layer fc7_p_drop7_p_0_split
I1028 01:25:50.020658 62912 net.cpp:425] fc7_p_drop7_p_0_split <- fc7_p
I1028 01:25:50.020663 62912 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_0
I1028 01:25:50.020669 62912 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_1
I1028 01:25:50.020709 62912 net.cpp:141] Setting up fc7_p_drop7_p_0_split
I1028 01:25:50.020716 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:50.020720 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:50.020722 62912 net.cpp:156] Memory required for data: 4419897344
I1028 01:25:50.020725 62912 layer_factory.hpp:77] Creating layer fc8_p
I1028 01:25:50.020736 62912 net.cpp:91] Creating Layer fc8_p
I1028 01:25:50.020740 62912 net.cpp:425] fc8_p <- fc7_p_drop7_p_0_split_0
I1028 01:25:50.020747 62912 net.cpp:399] fc8_p -> fc8_p
I1028 01:25:50.049512 62912 net.cpp:141] Setting up fc8_p
I1028 01:25:50.049522 62912 net.cpp:148] Top shape: 256 504 (129024)
I1028 01:25:50.049538 62912 net.cpp:156] Memory required for data: 4420413440
I1028 01:25:50.049545 62912 layer_factory.hpp:77] Creating layer fc7_pn
I1028 01:25:50.049551 62912 net.cpp:91] Creating Layer fc7_pn
I1028 01:25:50.049556 62912 net.cpp:425] fc7_pn <- fc7_p_drop7_p_0_split_1
I1028 01:25:50.049563 62912 net.cpp:399] fc7_pn -> fc7_pn
I1028 01:25:50.049593 62912 net.cpp:141] Setting up fc7_pn
I1028 01:25:50.049600 62912 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 01:25:50.049603 62912 net.cpp:156] Memory required for data: 4424607744
I1028 01:25:50.049607 62912 layer_factory.hpp:77] Creating layer loss_p1
I1028 01:25:50.049666 62912 net.cpp:91] Creating Layer loss_p1
I1028 01:25:50.049674 62912 net.cpp:425] loss_p1 <- fc7_n
I1028 01:25:50.049710 62912 net.cpp:425] loss_p1 <- fc7_pn
I1028 01:25:50.049716 62912 net.cpp:399] loss_p1 -> loss_p1
I1028 01:25:50.049799 62912 net.cpp:141] Setting up loss_p1
I1028 01:25:50.049808 62912 net.cpp:148] Top shape: (1)
I1028 01:25:50.049811 62912 net.cpp:151]     with loss weight -0.35
I1028 01:25:50.049897 62912 net.cpp:156] Memory required for data: 4424607748
I1028 01:25:50.049902 62912 layer_factory.hpp:77] Creating layer loss_p2
I1028 01:25:50.049955 62912 net.cpp:91] Creating Layer loss_p2
I1028 01:25:50.049962 62912 net.cpp:425] loss_p2 <- fc8_p
I1028 01:25:50.049968 62912 net.cpp:425] loss_p2 <- label
I1028 01:25:50.049975 62912 net.cpp:399] loss_p2 -> loss_p2
I1028 01:25:50.050020 62912 layer_factory.hpp:77] Creating layer loss_p2
I1028 01:25:50.051079 62912 net.cpp:141] Setting up loss_p2
I1028 01:25:50.051091 62912 net.cpp:148] Top shape: (1)
I1028 01:25:50.051095 62912 net.cpp:151]     with loss weight 0.65
I1028 01:25:50.051101 62912 net.cpp:156] Memory required for data: 4424607752
I1028 01:25:50.051105 62912 net.cpp:217] loss_p2 needs backward computation.
I1028 01:25:50.051108 62912 net.cpp:217] loss_p1 needs backward computation.
I1028 01:25:50.051112 62912 net.cpp:217] fc7_pn needs backward computation.
I1028 01:25:50.051115 62912 net.cpp:217] fc8_p needs backward computation.
I1028 01:25:50.051118 62912 net.cpp:217] fc7_p_drop7_p_0_split needs backward computation.
I1028 01:25:50.051121 62912 net.cpp:217] drop7_p needs backward computation.
I1028 01:25:50.051125 62912 net.cpp:217] relu7_p needs backward computation.
I1028 01:25:50.051127 62912 net.cpp:217] fc7_p needs backward computation.
I1028 01:25:50.051131 62912 net.cpp:217] drop6_p needs backward computation.
I1028 01:25:50.051133 62912 net.cpp:217] relu6_p needs backward computation.
I1028 01:25:50.051136 62912 net.cpp:217] fc6_p needs backward computation.
I1028 01:25:50.051139 62912 net.cpp:217] pool5_p needs backward computation.
I1028 01:25:50.051142 62912 net.cpp:217] relu5_p needs backward computation.
I1028 01:25:50.051146 62912 net.cpp:217] conv5_p needs backward computation.
I1028 01:25:50.051149 62912 net.cpp:217] relu4_p needs backward computation.
I1028 01:25:50.051152 62912 net.cpp:217] conv4_p needs backward computation.
I1028 01:25:50.051156 62912 net.cpp:217] relu3_p needs backward computation.
I1028 01:25:50.051158 62912 net.cpp:217] conv3_p needs backward computation.
I1028 01:25:50.051162 62912 net.cpp:217] pool2_p needs backward computation.
I1028 01:25:50.051164 62912 net.cpp:217] norm2_p needs backward computation.
I1028 01:25:50.051167 62912 net.cpp:217] relu2_p needs backward computation.
I1028 01:25:50.051170 62912 net.cpp:217] conv2_p needs backward computation.
I1028 01:25:50.051173 62912 net.cpp:217] pool1_p needs backward computation.
I1028 01:25:50.051177 62912 net.cpp:217] norm1_p needs backward computation.
I1028 01:25:50.051179 62912 net.cpp:217] relu1_p needs backward computation.
I1028 01:25:50.051182 62912 net.cpp:217] conv1_p needs backward computation.
I1028 01:25:50.051187 62912 net.cpp:219] fc7_n does not need backward computation.
I1028 01:25:50.051189 62912 net.cpp:219] fc7 does not need backward computation.
I1028 01:25:50.051192 62912 net.cpp:219] drop6 does not need backward computation.
I1028 01:25:50.051197 62912 net.cpp:219] relu6 does not need backward computation.
I1028 01:25:50.051199 62912 net.cpp:219] fc6 does not need backward computation.
I1028 01:25:50.051203 62912 net.cpp:219] pool5 does not need backward computation.
I1028 01:25:50.051205 62912 net.cpp:219] relu5 does not need backward computation.
I1028 01:25:50.051208 62912 net.cpp:219] conv5 does not need backward computation.
I1028 01:25:50.051213 62912 net.cpp:219] relu4 does not need backward computation.
I1028 01:25:50.051215 62912 net.cpp:219] conv4 does not need backward computation.
I1028 01:25:50.051219 62912 net.cpp:219] relu3 does not need backward computation.
I1028 01:25:50.051223 62912 net.cpp:219] conv3 does not need backward computation.
I1028 01:25:50.051225 62912 net.cpp:219] pool2 does not need backward computation.
I1028 01:25:50.051241 62912 net.cpp:219] norm2 does not need backward computation.
I1028 01:25:50.051246 62912 net.cpp:219] relu2 does not need backward computation.
I1028 01:25:50.051249 62912 net.cpp:219] conv2 does not need backward computation.
I1028 01:25:50.051254 62912 net.cpp:219] pool1 does not need backward computation.
I1028 01:25:50.051256 62912 net.cpp:219] norm1 does not need backward computation.
I1028 01:25:50.051259 62912 net.cpp:219] relu1 does not need backward computation.
I1028 01:25:50.051264 62912 net.cpp:219] conv1 does not need backward computation.
I1028 01:25:50.051267 62912 net.cpp:219] data_data_0_split does not need backward computation.
I1028 01:25:50.051273 62912 net.cpp:219] data does not need backward computation.
I1028 01:25:50.051276 62912 net.cpp:261] This network produces output loss_p1
I1028 01:25:50.051280 62912 net.cpp:261] This network produces output loss_p2
I1028 01:25:50.051306 62912 net.cpp:274] Network initialization done.
I1028 01:25:50.054482 62912 solver.cpp:181] Creating test net (#0) specified by net file: models/dissimilarity_siamese_net/train_val.prototxt
I1028 01:25:50.054559 62912 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1028 01:25:50.055578 62912 net.cpp:49] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "examples/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/work/03713/harshal1/maverick/vision_proj/data/val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "fc7_n"
  type: "L2Norm"
  bottom: "fc7"
  top: "fc7_n"
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data"
  top: "conv1_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "conv1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "norm1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "conv2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "norm2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "pool2_p"
  top: "conv3_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "pool5_p"
  type: "Pooling"
  bottom: "conv5_p"
  top: "pool5_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "pool5_p"
  top: "fc6_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7_p"
  type: "ReLU"
  bottom: "fc7_p"
  top: "fc7_p"
}
layer {
  name: "drop7_p"
  type: "Dropout"
  bottom: "fc7_p"
  top: "fc7_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 504
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "precision@1"
  type: "Accuracy"
  bottom: "fc8_p"
  bottom: "label"
  top: "precision@1"
  include {
    phase: TEST
  }
}
layer {
  name: "precision@5"
  type: "Accuracy"
  bottom: "fc8_p"
  bottom: "label"
  top: "precision@5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "fc7_pn"
  type: "L2Norm"
  bottom: "fc7_p"
  top: "fc7_pn"
}
layer {
  name: "loss_p1"
  type: "EuclideanLoss"
  bottom: "fc7_n"
  bottom: "fc7_pn"
  top: "loss_p1"
  loss_weight: -0.35
}
layer {
  name: "loss_p2"
  type: "SoftmaxWithLoss"
  bottom: "fc8_p"
  bottom: "label"
  top: "loss_p2"
  loss_weight: 0.65
}
I1028 01:25:50.055783 62912 layer_factory.hpp:77] Creating layer data
I1028 01:25:50.055929 62912 net.cpp:91] Creating Layer data
I1028 01:25:50.055941 62912 net.cpp:399] data -> data
I1028 01:25:50.055950 62912 net.cpp:399] data -> label
I1028 01:25:50.055958 62912 data_transformer.cpp:25] Loading mean file from: examples/imagenet/imagenet_mean.binaryproto
I1028 01:25:50.067203 63025 db_lmdb.cpp:35] Opened lmdb /work/03713/harshal1/maverick/vision_proj/data/val_lmdb
I1028 01:25:50.068464 62912 data_layer.cpp:41] output data size: 50,3,227,227
I1028 01:25:50.127619 62912 net.cpp:141] Setting up data
I1028 01:25:50.127655 62912 net.cpp:148] Top shape: 50 3 227 227 (7729350)
I1028 01:25:50.127660 62912 net.cpp:148] Top shape: 50 (50)
I1028 01:25:50.127665 62912 net.cpp:156] Memory required for data: 30917600
I1028 01:25:50.127672 62912 layer_factory.hpp:77] Creating layer data_data_0_split
I1028 01:25:50.127693 62912 net.cpp:91] Creating Layer data_data_0_split
I1028 01:25:50.127698 62912 net.cpp:425] data_data_0_split <- data
I1028 01:25:50.127708 62912 net.cpp:399] data_data_0_split -> data_data_0_split_0
I1028 01:25:50.127725 62912 net.cpp:399] data_data_0_split -> data_data_0_split_1
I1028 01:25:50.127820 62912 net.cpp:141] Setting up data_data_0_split
I1028 01:25:50.127830 62912 net.cpp:148] Top shape: 50 3 227 227 (7729350)
I1028 01:25:50.127872 62912 net.cpp:148] Top shape: 50 3 227 227 (7729350)
I1028 01:25:50.127876 62912 net.cpp:156] Memory required for data: 92752400
I1028 01:25:50.127881 62912 layer_factory.hpp:77] Creating layer label_data_1_split
I1028 01:25:50.127888 62912 net.cpp:91] Creating Layer label_data_1_split
I1028 01:25:50.127897 62912 net.cpp:425] label_data_1_split <- label
I1028 01:25:50.127904 62912 net.cpp:399] label_data_1_split -> label_data_1_split_0
I1028 01:25:50.127912 62912 net.cpp:399] label_data_1_split -> label_data_1_split_1
I1028 01:25:50.127918 62912 net.cpp:399] label_data_1_split -> label_data_1_split_2
I1028 01:25:50.127979 62912 net.cpp:141] Setting up label_data_1_split
I1028 01:25:50.127990 62912 net.cpp:148] Top shape: 50 (50)
I1028 01:25:50.127993 62912 net.cpp:148] Top shape: 50 (50)
I1028 01:25:50.127996 62912 net.cpp:148] Top shape: 50 (50)
I1028 01:25:50.128000 62912 net.cpp:156] Memory required for data: 92753000
I1028 01:25:50.128003 62912 layer_factory.hpp:77] Creating layer conv1
I1028 01:25:50.128021 62912 net.cpp:91] Creating Layer conv1
I1028 01:25:50.128026 62912 net.cpp:425] conv1 <- data_data_0_split_0
I1028 01:25:50.128033 62912 net.cpp:399] conv1 -> conv1
I1028 01:25:50.132396 62912 net.cpp:141] Setting up conv1
I1028 01:25:50.132410 62912 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 01:25:50.132426 62912 net.cpp:156] Memory required for data: 150833000
I1028 01:25:50.132438 62912 layer_factory.hpp:77] Creating layer relu1
I1028 01:25:50.132447 62912 net.cpp:91] Creating Layer relu1
I1028 01:25:50.132452 62912 net.cpp:425] relu1 <- conv1
I1028 01:25:50.132457 62912 net.cpp:386] relu1 -> conv1 (in-place)
I1028 01:25:50.132733 62912 net.cpp:141] Setting up relu1
I1028 01:25:50.132745 62912 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 01:25:50.132750 62912 net.cpp:156] Memory required for data: 208913000
I1028 01:25:50.132753 62912 layer_factory.hpp:77] Creating layer norm1
I1028 01:25:50.132763 62912 net.cpp:91] Creating Layer norm1
I1028 01:25:50.132768 62912 net.cpp:425] norm1 <- conv1
I1028 01:25:50.132774 62912 net.cpp:399] norm1 -> norm1
I1028 01:25:50.133080 62912 net.cpp:141] Setting up norm1
I1028 01:25:50.133092 62912 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 01:25:50.133096 62912 net.cpp:156] Memory required for data: 266993000
I1028 01:25:50.133100 62912 layer_factory.hpp:77] Creating layer pool1
I1028 01:25:50.133110 62912 net.cpp:91] Creating Layer pool1
I1028 01:25:50.133113 62912 net.cpp:425] pool1 <- norm1
I1028 01:25:50.133119 62912 net.cpp:399] pool1 -> pool1
I1028 01:25:50.133164 62912 net.cpp:141] Setting up pool1
I1028 01:25:50.133172 62912 net.cpp:148] Top shape: 50 96 27 27 (3499200)
I1028 01:25:50.133177 62912 net.cpp:156] Memory required for data: 280989800
I1028 01:25:50.133179 62912 layer_factory.hpp:77] Creating layer conv2
I1028 01:25:50.133190 62912 net.cpp:91] Creating Layer conv2
I1028 01:25:50.133194 62912 net.cpp:425] conv2 <- pool1
I1028 01:25:50.133203 62912 net.cpp:399] conv2 -> conv2
I1028 01:25:50.139619 62912 net.cpp:141] Setting up conv2
I1028 01:25:50.139645 62912 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 01:25:50.139649 62912 net.cpp:156] Memory required for data: 318314600
I1028 01:25:50.139659 62912 layer_factory.hpp:77] Creating layer relu2
I1028 01:25:50.139668 62912 net.cpp:91] Creating Layer relu2
I1028 01:25:50.139673 62912 net.cpp:425] relu2 <- conv2
I1028 01:25:50.139679 62912 net.cpp:386] relu2 -> conv2 (in-place)
I1028 01:25:50.139972 62912 net.cpp:141] Setting up relu2
I1028 01:25:50.139986 62912 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 01:25:50.139988 62912 net.cpp:156] Memory required for data: 355639400
I1028 01:25:50.139992 62912 layer_factory.hpp:77] Creating layer norm2
I1028 01:25:50.140000 62912 net.cpp:91] Creating Layer norm2
I1028 01:25:50.140004 62912 net.cpp:425] norm2 <- conv2
I1028 01:25:50.140012 62912 net.cpp:399] norm2 -> norm2
I1028 01:25:50.140199 62912 net.cpp:141] Setting up norm2
I1028 01:25:50.140210 62912 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 01:25:50.140229 62912 net.cpp:156] Memory required for data: 392964200
I1028 01:25:50.140234 62912 layer_factory.hpp:77] Creating layer pool2
I1028 01:25:50.140242 62912 net.cpp:91] Creating Layer pool2
I1028 01:25:50.140246 62912 net.cpp:425] pool2 <- norm2
I1028 01:25:50.140252 62912 net.cpp:399] pool2 -> pool2
I1028 01:25:50.140296 62912 net.cpp:141] Setting up pool2
I1028 01:25:50.140305 62912 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 01:25:50.140308 62912 net.cpp:156] Memory required for data: 401617000
I1028 01:25:50.140312 62912 layer_factory.hpp:77] Creating layer conv3
I1028 01:25:50.140323 62912 net.cpp:91] Creating Layer conv3
I1028 01:25:50.140328 62912 net.cpp:425] conv3 <- pool2
I1028 01:25:50.140336 62912 net.cpp:399] conv3 -> conv3
I1028 01:25:50.155567 62912 net.cpp:141] Setting up conv3
I1028 01:25:50.155607 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:50.155611 62912 net.cpp:156] Memory required for data: 414596200
I1028 01:25:50.155628 62912 layer_factory.hpp:77] Creating layer relu3
I1028 01:25:50.155643 62912 net.cpp:91] Creating Layer relu3
I1028 01:25:50.155648 62912 net.cpp:425] relu3 <- conv3
I1028 01:25:50.155658 62912 net.cpp:386] relu3 -> conv3 (in-place)
I1028 01:25:50.155951 62912 net.cpp:141] Setting up relu3
I1028 01:25:50.155966 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:50.155969 62912 net.cpp:156] Memory required for data: 427575400
I1028 01:25:50.155973 62912 layer_factory.hpp:77] Creating layer conv4
I1028 01:25:50.155989 62912 net.cpp:91] Creating Layer conv4
I1028 01:25:50.155993 62912 net.cpp:425] conv4 <- conv3
I1028 01:25:50.156002 62912 net.cpp:399] conv4 -> conv4
I1028 01:25:50.168009 62912 net.cpp:141] Setting up conv4
I1028 01:25:50.168037 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:50.168042 62912 net.cpp:156] Memory required for data: 440554600
I1028 01:25:50.168050 62912 layer_factory.hpp:77] Creating layer relu4
I1028 01:25:50.168059 62912 net.cpp:91] Creating Layer relu4
I1028 01:25:50.168063 62912 net.cpp:425] relu4 <- conv4
I1028 01:25:50.168071 62912 net.cpp:386] relu4 -> conv4 (in-place)
I1028 01:25:50.168339 62912 net.cpp:141] Setting up relu4
I1028 01:25:50.168351 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:50.168355 62912 net.cpp:156] Memory required for data: 453533800
I1028 01:25:50.168359 62912 layer_factory.hpp:77] Creating layer conv5
I1028 01:25:50.168371 62912 net.cpp:91] Creating Layer conv5
I1028 01:25:50.168376 62912 net.cpp:425] conv5 <- conv4
I1028 01:25:50.168383 62912 net.cpp:399] conv5 -> conv5
I1028 01:25:50.176946 62912 net.cpp:141] Setting up conv5
I1028 01:25:50.176975 62912 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 01:25:50.176980 62912 net.cpp:156] Memory required for data: 462186600
I1028 01:25:50.176992 62912 layer_factory.hpp:77] Creating layer relu5
I1028 01:25:50.177004 62912 net.cpp:91] Creating Layer relu5
I1028 01:25:50.177008 62912 net.cpp:425] relu5 <- conv5
I1028 01:25:50.177016 62912 net.cpp:386] relu5 -> conv5 (in-place)
I1028 01:25:50.177289 62912 net.cpp:141] Setting up relu5
I1028 01:25:50.177299 62912 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 01:25:50.177304 62912 net.cpp:156] Memory required for data: 470839400
I1028 01:25:50.177307 62912 layer_factory.hpp:77] Creating layer pool5
I1028 01:25:50.177316 62912 net.cpp:91] Creating Layer pool5
I1028 01:25:50.177320 62912 net.cpp:425] pool5 <- conv5
I1028 01:25:50.177327 62912 net.cpp:399] pool5 -> pool5
I1028 01:25:50.177381 62912 net.cpp:141] Setting up pool5
I1028 01:25:50.177391 62912 net.cpp:148] Top shape: 50 256 6 6 (460800)
I1028 01:25:50.177393 62912 net.cpp:156] Memory required for data: 472682600
I1028 01:25:50.177397 62912 layer_factory.hpp:77] Creating layer fc6
I1028 01:25:50.177412 62912 net.cpp:91] Creating Layer fc6
I1028 01:25:50.177415 62912 net.cpp:425] fc6 <- pool5
I1028 01:25:50.177423 62912 net.cpp:399] fc6 -> fc6
I1028 01:25:50.730541 62912 net.cpp:141] Setting up fc6
I1028 01:25:50.730592 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:50.730633 62912 net.cpp:156] Memory required for data: 473501800
I1028 01:25:50.730648 62912 layer_factory.hpp:77] Creating layer relu6
I1028 01:25:50.730665 62912 net.cpp:91] Creating Layer relu6
I1028 01:25:50.730671 62912 net.cpp:425] relu6 <- fc6
I1028 01:25:50.730681 62912 net.cpp:386] relu6 -> fc6 (in-place)
I1028 01:25:50.730931 62912 net.cpp:141] Setting up relu6
I1028 01:25:50.730942 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:50.730944 62912 net.cpp:156] Memory required for data: 474321000
I1028 01:25:50.730948 62912 layer_factory.hpp:77] Creating layer drop6
I1028 01:25:50.730958 62912 net.cpp:91] Creating Layer drop6
I1028 01:25:50.730962 62912 net.cpp:425] drop6 <- fc6
I1028 01:25:50.730968 62912 net.cpp:386] drop6 -> fc6 (in-place)
I1028 01:25:50.731001 62912 net.cpp:141] Setting up drop6
I1028 01:25:50.731009 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:50.731012 62912 net.cpp:156] Memory required for data: 475140200
I1028 01:25:50.731015 62912 layer_factory.hpp:77] Creating layer fc7
I1028 01:25:50.731027 62912 net.cpp:91] Creating Layer fc7
I1028 01:25:50.731030 62912 net.cpp:425] fc7 <- fc6
I1028 01:25:50.731037 62912 net.cpp:399] fc7 -> fc7
I1028 01:25:50.967649 62912 net.cpp:141] Setting up fc7
I1028 01:25:50.967700 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:50.967706 62912 net.cpp:156] Memory required for data: 475959400
I1028 01:25:50.967718 62912 layer_factory.hpp:77] Creating layer fc7_n
I1028 01:25:50.967736 62912 net.cpp:91] Creating Layer fc7_n
I1028 01:25:50.967741 62912 net.cpp:425] fc7_n <- fc7
I1028 01:25:50.967753 62912 net.cpp:399] fc7_n -> fc7_n
I1028 01:25:50.967792 62912 net.cpp:141] Setting up fc7_n
I1028 01:25:50.967799 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:50.967803 62912 net.cpp:156] Memory required for data: 476778600
I1028 01:25:50.967805 62912 layer_factory.hpp:77] Creating layer conv1_p
I1028 01:25:50.967824 62912 net.cpp:91] Creating Layer conv1_p
I1028 01:25:50.967828 62912 net.cpp:425] conv1_p <- data_data_0_split_1
I1028 01:25:50.967835 62912 net.cpp:399] conv1_p -> conv1_p
I1028 01:25:50.969449 62912 net.cpp:141] Setting up conv1_p
I1028 01:25:50.969460 62912 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 01:25:50.969475 62912 net.cpp:156] Memory required for data: 534858600
I1028 01:25:50.969482 62912 layer_factory.hpp:77] Creating layer relu1_p
I1028 01:25:50.969491 62912 net.cpp:91] Creating Layer relu1_p
I1028 01:25:50.969494 62912 net.cpp:425] relu1_p <- conv1_p
I1028 01:25:50.969501 62912 net.cpp:386] relu1_p -> conv1_p (in-place)
I1028 01:25:50.969755 62912 net.cpp:141] Setting up relu1_p
I1028 01:25:50.969766 62912 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 01:25:50.969769 62912 net.cpp:156] Memory required for data: 592938600
I1028 01:25:50.969774 62912 layer_factory.hpp:77] Creating layer norm1_p
I1028 01:25:50.969784 62912 net.cpp:91] Creating Layer norm1_p
I1028 01:25:50.969787 62912 net.cpp:425] norm1_p <- conv1_p
I1028 01:25:50.969794 62912 net.cpp:399] norm1_p -> norm1_p
I1028 01:25:50.969990 62912 net.cpp:141] Setting up norm1_p
I1028 01:25:50.970001 62912 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 01:25:50.970005 62912 net.cpp:156] Memory required for data: 651018600
I1028 01:25:50.970008 62912 layer_factory.hpp:77] Creating layer pool1_p
I1028 01:25:50.970016 62912 net.cpp:91] Creating Layer pool1_p
I1028 01:25:50.970021 62912 net.cpp:425] pool1_p <- norm1_p
I1028 01:25:50.970026 62912 net.cpp:399] pool1_p -> pool1_p
I1028 01:25:50.970070 62912 net.cpp:141] Setting up pool1_p
I1028 01:25:50.970077 62912 net.cpp:148] Top shape: 50 96 27 27 (3499200)
I1028 01:25:50.970082 62912 net.cpp:156] Memory required for data: 665015400
I1028 01:25:50.970084 62912 layer_factory.hpp:77] Creating layer conv2_p
I1028 01:25:50.970095 62912 net.cpp:91] Creating Layer conv2_p
I1028 01:25:50.970099 62912 net.cpp:425] conv2_p <- pool1_p
I1028 01:25:50.970106 62912 net.cpp:399] conv2_p -> conv2_p
I1028 01:25:50.976050 62912 net.cpp:141] Setting up conv2_p
I1028 01:25:50.976091 62912 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 01:25:50.976107 62912 net.cpp:156] Memory required for data: 702340200
I1028 01:25:50.976121 62912 layer_factory.hpp:77] Creating layer relu2_p
I1028 01:25:50.976130 62912 net.cpp:91] Creating Layer relu2_p
I1028 01:25:50.976133 62912 net.cpp:425] relu2_p <- conv2_p
I1028 01:25:50.976140 62912 net.cpp:386] relu2_p -> conv2_p (in-place)
I1028 01:25:50.976294 62912 net.cpp:141] Setting up relu2_p
I1028 01:25:50.976305 62912 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 01:25:50.976307 62912 net.cpp:156] Memory required for data: 739665000
I1028 01:25:50.976310 62912 layer_factory.hpp:77] Creating layer norm2_p
I1028 01:25:50.976317 62912 net.cpp:91] Creating Layer norm2_p
I1028 01:25:50.976321 62912 net.cpp:425] norm2_p <- conv2_p
I1028 01:25:50.976327 62912 net.cpp:399] norm2_p -> norm2_p
I1028 01:25:50.976632 62912 net.cpp:141] Setting up norm2_p
I1028 01:25:50.976642 62912 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 01:25:50.976645 62912 net.cpp:156] Memory required for data: 776989800
I1028 01:25:50.976649 62912 layer_factory.hpp:77] Creating layer pool2_p
I1028 01:25:50.976655 62912 net.cpp:91] Creating Layer pool2_p
I1028 01:25:50.976658 62912 net.cpp:425] pool2_p <- norm2_p
I1028 01:25:50.976665 62912 net.cpp:399] pool2_p -> pool2_p
I1028 01:25:50.976719 62912 net.cpp:141] Setting up pool2_p
I1028 01:25:50.976725 62912 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 01:25:50.976728 62912 net.cpp:156] Memory required for data: 785642600
I1028 01:25:50.976732 62912 layer_factory.hpp:77] Creating layer conv3_p
I1028 01:25:50.976744 62912 net.cpp:91] Creating Layer conv3_p
I1028 01:25:50.976748 62912 net.cpp:425] conv3_p <- pool2_p
I1028 01:25:50.976754 62912 net.cpp:399] conv3_p -> conv3_p
I1028 01:25:50.990119 62912 net.cpp:141] Setting up conv3_p
I1028 01:25:50.990130 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:50.990147 62912 net.cpp:156] Memory required for data: 798621800
I1028 01:25:50.990154 62912 layer_factory.hpp:77] Creating layer relu3_p
I1028 01:25:50.990164 62912 net.cpp:91] Creating Layer relu3_p
I1028 01:25:50.990167 62912 net.cpp:425] relu3_p <- conv3_p
I1028 01:25:50.990175 62912 net.cpp:386] relu3_p -> conv3_p (in-place)
I1028 01:25:50.990326 62912 net.cpp:141] Setting up relu3_p
I1028 01:25:50.990335 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:50.990339 62912 net.cpp:156] Memory required for data: 811601000
I1028 01:25:50.990342 62912 layer_factory.hpp:77] Creating layer conv4_p
I1028 01:25:50.990353 62912 net.cpp:91] Creating Layer conv4_p
I1028 01:25:50.990357 62912 net.cpp:425] conv4_p <- conv3_p
I1028 01:25:50.990365 62912 net.cpp:399] conv4_p -> conv4_p
I1028 01:25:51.001267 62912 net.cpp:141] Setting up conv4_p
I1028 01:25:51.001291 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:51.001294 62912 net.cpp:156] Memory required for data: 824580200
I1028 01:25:51.001301 62912 layer_factory.hpp:77] Creating layer relu4_p
I1028 01:25:51.001309 62912 net.cpp:91] Creating Layer relu4_p
I1028 01:25:51.001313 62912 net.cpp:425] relu4_p <- conv4_p
I1028 01:25:51.001318 62912 net.cpp:386] relu4_p -> conv4_p (in-place)
I1028 01:25:51.001471 62912 net.cpp:141] Setting up relu4_p
I1028 01:25:51.001482 62912 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 01:25:51.001484 62912 net.cpp:156] Memory required for data: 837559400
I1028 01:25:51.001488 62912 layer_factory.hpp:77] Creating layer conv5_p
I1028 01:25:51.001499 62912 net.cpp:91] Creating Layer conv5_p
I1028 01:25:51.001503 62912 net.cpp:425] conv5_p <- conv4_p
I1028 01:25:51.001510 62912 net.cpp:399] conv5_p -> conv5_p
I1028 01:25:51.009323 62912 net.cpp:141] Setting up conv5_p
I1028 01:25:51.009335 62912 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 01:25:51.009349 62912 net.cpp:156] Memory required for data: 846212200
I1028 01:25:51.009356 62912 layer_factory.hpp:77] Creating layer relu5_p
I1028 01:25:51.009364 62912 net.cpp:91] Creating Layer relu5_p
I1028 01:25:51.009383 62912 net.cpp:425] relu5_p <- conv5_p
I1028 01:25:51.009390 62912 net.cpp:386] relu5_p -> conv5_p (in-place)
I1028 01:25:51.009543 62912 net.cpp:141] Setting up relu5_p
I1028 01:25:51.009554 62912 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 01:25:51.009557 62912 net.cpp:156] Memory required for data: 854865000
I1028 01:25:51.009562 62912 layer_factory.hpp:77] Creating layer pool5_p
I1028 01:25:51.009567 62912 net.cpp:91] Creating Layer pool5_p
I1028 01:25:51.009572 62912 net.cpp:425] pool5_p <- conv5_p
I1028 01:25:51.009578 62912 net.cpp:399] pool5_p -> pool5_p
I1028 01:25:51.009624 62912 net.cpp:141] Setting up pool5_p
I1028 01:25:51.009631 62912 net.cpp:148] Top shape: 50 256 6 6 (460800)
I1028 01:25:51.009634 62912 net.cpp:156] Memory required for data: 856708200
I1028 01:25:51.009637 62912 layer_factory.hpp:77] Creating layer fc6_p
I1028 01:25:51.009649 62912 net.cpp:91] Creating Layer fc6_p
I1028 01:25:51.009652 62912 net.cpp:425] fc6_p <- pool5_p
I1028 01:25:51.009659 62912 net.cpp:399] fc6_p -> fc6_p
I1028 01:25:51.540937 62912 net.cpp:141] Setting up fc6_p
I1028 01:25:51.540988 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.540993 62912 net.cpp:156] Memory required for data: 857527400
I1028 01:25:51.541004 62912 layer_factory.hpp:77] Creating layer relu6_p
I1028 01:25:51.541023 62912 net.cpp:91] Creating Layer relu6_p
I1028 01:25:51.541028 62912 net.cpp:425] relu6_p <- fc6_p
I1028 01:25:51.541040 62912 net.cpp:386] relu6_p -> fc6_p (in-place)
I1028 01:25:51.541527 62912 net.cpp:141] Setting up relu6_p
I1028 01:25:51.541535 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.541551 62912 net.cpp:156] Memory required for data: 858346600
I1028 01:25:51.541554 62912 layer_factory.hpp:77] Creating layer drop6_p
I1028 01:25:51.541566 62912 net.cpp:91] Creating Layer drop6_p
I1028 01:25:51.541570 62912 net.cpp:425] drop6_p <- fc6_p
I1028 01:25:51.541575 62912 net.cpp:386] drop6_p -> fc6_p (in-place)
I1028 01:25:51.541610 62912 net.cpp:141] Setting up drop6_p
I1028 01:25:51.541616 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.541620 62912 net.cpp:156] Memory required for data: 859165800
I1028 01:25:51.541623 62912 layer_factory.hpp:77] Creating layer fc7_p
I1028 01:25:51.541633 62912 net.cpp:91] Creating Layer fc7_p
I1028 01:25:51.541637 62912 net.cpp:425] fc7_p <- fc6_p
I1028 01:25:51.541646 62912 net.cpp:399] fc7_p -> fc7_p
I1028 01:25:51.777567 62912 net.cpp:141] Setting up fc7_p
I1028 01:25:51.777617 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.777621 62912 net.cpp:156] Memory required for data: 859985000
I1028 01:25:51.777634 62912 layer_factory.hpp:77] Creating layer relu7_p
I1028 01:25:51.777652 62912 net.cpp:91] Creating Layer relu7_p
I1028 01:25:51.777657 62912 net.cpp:425] relu7_p <- fc7_p
I1028 01:25:51.777669 62912 net.cpp:386] relu7_p -> fc7_p (in-place)
I1028 01:25:51.778141 62912 net.cpp:141] Setting up relu7_p
I1028 01:25:51.778151 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.778154 62912 net.cpp:156] Memory required for data: 860804200
I1028 01:25:51.778157 62912 layer_factory.hpp:77] Creating layer drop7_p
I1028 01:25:51.778167 62912 net.cpp:91] Creating Layer drop7_p
I1028 01:25:51.778170 62912 net.cpp:425] drop7_p <- fc7_p
I1028 01:25:51.778178 62912 net.cpp:386] drop7_p -> fc7_p (in-place)
I1028 01:25:51.778214 62912 net.cpp:141] Setting up drop7_p
I1028 01:25:51.778223 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.778225 62912 net.cpp:156] Memory required for data: 861623400
I1028 01:25:51.778228 62912 layer_factory.hpp:77] Creating layer fc7_p_drop7_p_0_split
I1028 01:25:51.778235 62912 net.cpp:91] Creating Layer fc7_p_drop7_p_0_split
I1028 01:25:51.778239 62912 net.cpp:425] fc7_p_drop7_p_0_split <- fc7_p
I1028 01:25:51.778244 62912 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_0
I1028 01:25:51.778262 62912 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_1
I1028 01:25:51.778306 62912 net.cpp:141] Setting up fc7_p_drop7_p_0_split
I1028 01:25:51.778316 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.778352 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.778357 62912 net.cpp:156] Memory required for data: 863261800
I1028 01:25:51.778359 62912 layer_factory.hpp:77] Creating layer fc8_p
I1028 01:25:51.778370 62912 net.cpp:91] Creating Layer fc8_p
I1028 01:25:51.778374 62912 net.cpp:425] fc8_p <- fc7_p_drop7_p_0_split_0
I1028 01:25:51.778383 62912 net.cpp:399] fc8_p -> fc8_p
I1028 01:25:51.807173 62912 net.cpp:141] Setting up fc8_p
I1028 01:25:51.807183 62912 net.cpp:148] Top shape: 50 504 (25200)
I1028 01:25:51.807199 62912 net.cpp:156] Memory required for data: 863362600
I1028 01:25:51.807205 62912 layer_factory.hpp:77] Creating layer fc8_p_fc8_p_0_split
I1028 01:25:51.807212 62912 net.cpp:91] Creating Layer fc8_p_fc8_p_0_split
I1028 01:25:51.807216 62912 net.cpp:425] fc8_p_fc8_p_0_split <- fc8_p
I1028 01:25:51.807224 62912 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_0
I1028 01:25:51.807230 62912 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_1
I1028 01:25:51.807236 62912 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_2
I1028 01:25:51.807288 62912 net.cpp:141] Setting up fc8_p_fc8_p_0_split
I1028 01:25:51.807296 62912 net.cpp:148] Top shape: 50 504 (25200)
I1028 01:25:51.807299 62912 net.cpp:148] Top shape: 50 504 (25200)
I1028 01:25:51.807303 62912 net.cpp:148] Top shape: 50 504 (25200)
I1028 01:25:51.807306 62912 net.cpp:156] Memory required for data: 863665000
I1028 01:25:51.807308 62912 layer_factory.hpp:77] Creating layer precision@1
I1028 01:25:51.807368 62912 net.cpp:91] Creating Layer precision@1
I1028 01:25:51.807374 62912 net.cpp:425] precision@1 <- fc8_p_fc8_p_0_split_0
I1028 01:25:51.807380 62912 net.cpp:425] precision@1 <- label_data_1_split_0
I1028 01:25:51.807385 62912 net.cpp:399] precision@1 -> precision@1
I1028 01:25:51.807441 62912 net.cpp:141] Setting up precision@1
I1028 01:25:51.807451 62912 net.cpp:148] Top shape: (1)
I1028 01:25:51.807453 62912 net.cpp:156] Memory required for data: 863665004
I1028 01:25:51.807456 62912 layer_factory.hpp:77] Creating layer precision@5
I1028 01:25:51.807464 62912 net.cpp:91] Creating Layer precision@5
I1028 01:25:51.807467 62912 net.cpp:425] precision@5 <- fc8_p_fc8_p_0_split_1
I1028 01:25:51.807472 62912 net.cpp:425] precision@5 <- label_data_1_split_1
I1028 01:25:51.807477 62912 net.cpp:399] precision@5 -> precision@5
I1028 01:25:51.807485 62912 net.cpp:141] Setting up precision@5
I1028 01:25:51.807489 62912 net.cpp:148] Top shape: (1)
I1028 01:25:51.807492 62912 net.cpp:156] Memory required for data: 863665008
I1028 01:25:51.807495 62912 layer_factory.hpp:77] Creating layer fc7_pn
I1028 01:25:51.807500 62912 net.cpp:91] Creating Layer fc7_pn
I1028 01:25:51.807503 62912 net.cpp:425] fc7_pn <- fc7_p_drop7_p_0_split_1
I1028 01:25:51.807508 62912 net.cpp:399] fc7_pn -> fc7_pn
I1028 01:25:51.807538 62912 net.cpp:141] Setting up fc7_pn
I1028 01:25:51.807546 62912 net.cpp:148] Top shape: 50 4096 (204800)
I1028 01:25:51.807548 62912 net.cpp:156] Memory required for data: 864484208
I1028 01:25:51.807551 62912 layer_factory.hpp:77] Creating layer loss_p1
I1028 01:25:51.807559 62912 net.cpp:91] Creating Layer loss_p1
I1028 01:25:51.807562 62912 net.cpp:425] loss_p1 <- fc7_n
I1028 01:25:51.807566 62912 net.cpp:425] loss_p1 <- fc7_pn
I1028 01:25:51.807570 62912 net.cpp:399] loss_p1 -> loss_p1
I1028 01:25:51.807616 62912 net.cpp:141] Setting up loss_p1
I1028 01:25:51.807624 62912 net.cpp:148] Top shape: (1)
I1028 01:25:51.807627 62912 net.cpp:151]     with loss weight -0.35
I1028 01:25:51.807641 62912 net.cpp:156] Memory required for data: 864484212
I1028 01:25:51.807643 62912 layer_factory.hpp:77] Creating layer loss_p2
I1028 01:25:51.807651 62912 net.cpp:91] Creating Layer loss_p2
I1028 01:25:51.807654 62912 net.cpp:425] loss_p2 <- fc8_p_fc8_p_0_split_2
I1028 01:25:51.807658 62912 net.cpp:425] loss_p2 <- label_data_1_split_2
I1028 01:25:51.807665 62912 net.cpp:399] loss_p2 -> loss_p2
I1028 01:25:51.807674 62912 layer_factory.hpp:77] Creating layer loss_p2
I1028 01:25:51.807973 62912 net.cpp:141] Setting up loss_p2
I1028 01:25:51.807996 62912 net.cpp:148] Top shape: (1)
I1028 01:25:51.807999 62912 net.cpp:151]     with loss weight 0.65
I1028 01:25:51.808006 62912 net.cpp:156] Memory required for data: 864484216
I1028 01:25:51.808008 62912 net.cpp:217] loss_p2 needs backward computation.
I1028 01:25:51.808012 62912 net.cpp:217] loss_p1 needs backward computation.
I1028 01:25:51.808017 62912 net.cpp:217] fc7_pn needs backward computation.
I1028 01:25:51.808019 62912 net.cpp:219] precision@5 does not need backward computation.
I1028 01:25:51.808022 62912 net.cpp:219] precision@1 does not need backward computation.
I1028 01:25:51.808027 62912 net.cpp:217] fc8_p_fc8_p_0_split needs backward computation.
I1028 01:25:51.808029 62912 net.cpp:217] fc8_p needs backward computation.
I1028 01:25:51.808032 62912 net.cpp:217] fc7_p_drop7_p_0_split needs backward computation.
I1028 01:25:51.808044 62912 net.cpp:217] drop7_p needs backward computation.
I1028 01:25:51.808048 62912 net.cpp:217] relu7_p needs backward computation.
I1028 01:25:51.808050 62912 net.cpp:217] fc7_p needs backward computation.
I1028 01:25:51.808053 62912 net.cpp:217] drop6_p needs backward computation.
I1028 01:25:51.808056 62912 net.cpp:217] relu6_p needs backward computation.
I1028 01:25:51.808059 62912 net.cpp:217] fc6_p needs backward computation.
I1028 01:25:51.808063 62912 net.cpp:217] pool5_p needs backward computation.
I1028 01:25:51.808065 62912 net.cpp:217] relu5_p needs backward computation.
I1028 01:25:51.808068 62912 net.cpp:217] conv5_p needs backward computation.
I1028 01:25:51.808071 62912 net.cpp:217] relu4_p needs backward computation.
I1028 01:25:51.808074 62912 net.cpp:217] conv4_p needs backward computation.
I1028 01:25:51.808078 62912 net.cpp:217] relu3_p needs backward computation.
I1028 01:25:51.808080 62912 net.cpp:217] conv3_p needs backward computation.
I1028 01:25:51.808084 62912 net.cpp:217] pool2_p needs backward computation.
I1028 01:25:51.808086 62912 net.cpp:217] norm2_p needs backward computation.
I1028 01:25:51.808089 62912 net.cpp:217] relu2_p needs backward computation.
I1028 01:25:51.808092 62912 net.cpp:217] conv2_p needs backward computation.
I1028 01:25:51.808096 62912 net.cpp:217] pool1_p needs backward computation.
I1028 01:25:51.808099 62912 net.cpp:217] norm1_p needs backward computation.
I1028 01:25:51.808102 62912 net.cpp:217] relu1_p needs backward computation.
I1028 01:25:51.808105 62912 net.cpp:217] conv1_p needs backward computation.
I1028 01:25:51.808109 62912 net.cpp:219] fc7_n does not need backward computation.
I1028 01:25:51.808111 62912 net.cpp:219] fc7 does not need backward computation.
I1028 01:25:51.808115 62912 net.cpp:219] drop6 does not need backward computation.
I1028 01:25:51.808118 62912 net.cpp:219] relu6 does not need backward computation.
I1028 01:25:51.808121 62912 net.cpp:219] fc6 does not need backward computation.
I1028 01:25:51.808125 62912 net.cpp:219] pool5 does not need backward computation.
I1028 01:25:51.808128 62912 net.cpp:219] relu5 does not need backward computation.
I1028 01:25:51.808131 62912 net.cpp:219] conv5 does not need backward computation.
I1028 01:25:51.808135 62912 net.cpp:219] relu4 does not need backward computation.
I1028 01:25:51.808138 62912 net.cpp:219] conv4 does not need backward computation.
I1028 01:25:51.808141 62912 net.cpp:219] relu3 does not need backward computation.
I1028 01:25:51.808145 62912 net.cpp:219] conv3 does not need backward computation.
I1028 01:25:51.808147 62912 net.cpp:219] pool2 does not need backward computation.
I1028 01:25:51.808151 62912 net.cpp:219] norm2 does not need backward computation.
I1028 01:25:51.808154 62912 net.cpp:219] relu2 does not need backward computation.
I1028 01:25:51.808157 62912 net.cpp:219] conv2 does not need backward computation.
I1028 01:25:51.808161 62912 net.cpp:219] pool1 does not need backward computation.
I1028 01:25:51.808164 62912 net.cpp:219] norm1 does not need backward computation.
I1028 01:25:51.808167 62912 net.cpp:219] relu1 does not need backward computation.
I1028 01:25:51.808179 62912 net.cpp:219] conv1 does not need backward computation.
I1028 01:25:51.808185 62912 net.cpp:219] label_data_1_split does not need backward computation.
I1028 01:25:51.808189 62912 net.cpp:219] data_data_0_split does not need backward computation.
I1028 01:25:51.808192 62912 net.cpp:219] data does not need backward computation.
I1028 01:25:51.808195 62912 net.cpp:261] This network produces output loss_p1
I1028 01:25:51.808198 62912 net.cpp:261] This network produces output loss_p2
I1028 01:25:51.808202 62912 net.cpp:261] This network produces output precision@1
I1028 01:25:51.808205 62912 net.cpp:261] This network produces output precision@5
I1028 01:25:51.808233 62912 net.cpp:274] Network initialization done.
I1028 01:25:51.808392 62912 solver.cpp:60] Solver scaffolding done.
I1028 01:25:51.809562 62912 caffe.cpp:129] Finetuning from models/dissimilarity_siamese_net/pretrained_model/bvlc_dissimilarity.caffemodel
I1028 01:25:52.849794 62912 net.cpp:752] Ignoring source layer prob
I1028 01:25:53.890735 62912 net.cpp:752] Ignoring source layer prob
I1028 01:25:53.892740 62912 caffe.cpp:219] Starting Optimization
I1028 01:25:53.892818 62912 solver.cpp:279] Solving AlexNet
I1028 01:25:53.892825 62912 solver.cpp:280] Learning Rate Policy: step
I1028 01:25:53.895634 62912 solver.cpp:337] Iteration 0, Testing net (#0)
I1028 01:27:48.588835 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.720184 (* -0.35 = -0.252064 loss)
I1028 01:27:48.588937 62912 solver.cpp:404]     Test net output #1: loss_p2 = 6.22262 (* 0.65 = 4.04471 loss)
I1028 01:27:48.588943 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.002
I1028 01:27:48.588948 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.00998002
I1028 01:27:49.154749 62912 solver.cpp:228] Iteration 0, loss = 3.74955
I1028 01:27:49.154810 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.843198 (* -0.35 = -0.295119 loss)
I1028 01:27:49.154819 62912 solver.cpp:244]     Train net output #1: loss_p2 = 6.22258 (* 0.65 = 4.04467 loss)
I1028 01:27:49.154863 62912 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1028 01:28:38.557754 62912 solver.cpp:228] Iteration 40, loss = 3.02998
I1028 01:28:38.557974 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.881082 (* -0.35 = -0.308379 loss)
I1028 01:28:38.557986 62912 solver.cpp:244]     Train net output #1: loss_p2 = 5.13594 (* 0.65 = 3.33836 loss)
I1028 01:28:38.557994 62912 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I1028 01:29:27.954082 62912 solver.cpp:228] Iteration 80, loss = 2.76655
I1028 01:29:27.954309 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.904662 (* -0.35 = -0.316632 loss)
I1028 01:29:27.954319 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.74335 (* 0.65 = 3.08318 loss)
I1028 01:29:27.954329 62912 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I1028 01:30:17.338457 62912 solver.cpp:228] Iteration 120, loss = 2.833
I1028 01:30:17.338692 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.904314 (* -0.35 = -0.31651 loss)
I1028 01:30:17.338703 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.8454 (* 0.65 = 3.14951 loss)
I1028 01:30:17.338712 62912 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I1028 01:31:06.765658 62912 solver.cpp:228] Iteration 160, loss = 2.51973
I1028 01:31:06.765887 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.903842 (* -0.35 = -0.316345 loss)
I1028 01:31:06.765899 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.36319 (* 0.65 = 2.83607 loss)
I1028 01:31:06.765908 62912 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I1028 01:31:55.858837 62912 solver.cpp:228] Iteration 200, loss = 2.53962
I1028 01:31:55.859057 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.910971 (* -0.35 = -0.31884 loss)
I1028 01:31:55.859068 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.39763 (* 0.65 = 2.85846 loss)
I1028 01:31:55.859074 62912 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1028 01:32:44.838331 62912 solver.cpp:228] Iteration 240, loss = 2.51639
I1028 01:32:44.838518 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.91505 (* -0.35 = -0.320268 loss)
I1028 01:32:44.838536 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.36409 (* 0.65 = 2.83666 loss)
I1028 01:32:44.838541 62912 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I1028 01:33:33.866339 62912 solver.cpp:228] Iteration 280, loss = 2.45623
I1028 01:33:33.866550 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.918435 (* -0.35 = -0.321452 loss)
I1028 01:33:33.866560 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.27336 (* 0.65 = 2.77768 loss)
I1028 01:33:33.866567 62912 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I1028 01:34:22.848866 62912 solver.cpp:228] Iteration 320, loss = 2.20673
I1028 01:34:22.849093 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.907928 (* -0.35 = -0.317775 loss)
I1028 01:34:22.849103 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.88386 (* 0.65 = 2.52451 loss)
I1028 01:34:22.849109 62912 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I1028 01:35:11.799232 62912 solver.cpp:228] Iteration 360, loss = 2.34929
I1028 01:35:11.799386 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.920186 (* -0.35 = -0.322065 loss)
I1028 01:35:11.799396 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.10978 (* 0.65 = 2.67136 loss)
I1028 01:35:11.799401 62912 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I1028 01:36:00.746070 62912 solver.cpp:228] Iteration 400, loss = 2.29934
I1028 01:36:00.746199 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.923563 (* -0.35 = -0.323247 loss)
I1028 01:36:00.746211 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.03476 (* 0.65 = 2.62259 loss)
I1028 01:36:00.746217 62912 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1028 01:36:49.705425 62912 solver.cpp:228] Iteration 440, loss = 2.35199
I1028 01:36:49.705616 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.924178 (* -0.35 = -0.323462 loss)
I1028 01:36:49.705626 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.11608 (* 0.65 = 2.67545 loss)
I1028 01:36:49.705632 62912 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I1028 01:37:38.668130 62912 solver.cpp:228] Iteration 480, loss = 2.1476
I1028 01:37:38.668310 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.926564 (* -0.35 = -0.324297 loss)
I1028 01:37:38.668320 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.80292 (* 0.65 = 2.4719 loss)
I1028 01:37:38.668328 62912 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I1028 01:38:01.947826 62912 solver.cpp:337] Iteration 500, Testing net (#0)
I1028 01:39:56.804008 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.852836 (* -0.35 = -0.298493 loss)
I1028 01:39:56.804155 62912 solver.cpp:404]     Test net output #1: loss_p2 = 4.10864 (* 0.65 = 2.67062 loss)
I1028 01:39:56.804172 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.17972
I1028 01:39:56.804177 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.38534
I1028 01:40:21.810308 62912 solver.cpp:228] Iteration 520, loss = 2.19331
I1028 01:40:21.810339 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.926697 (* -0.35 = -0.324344 loss)
I1028 01:40:21.810346 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.87332 (* 0.65 = 2.51766 loss)
I1028 01:40:21.810353 62912 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I1028 01:41:10.825489 62912 solver.cpp:228] Iteration 560, loss = 2.40195
I1028 01:41:10.825696 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.926958 (* -0.35 = -0.324435 loss)
I1028 01:41:10.825706 62912 solver.cpp:244]     Train net output #1: loss_p2 = 4.19444 (* 0.65 = 2.72639 loss)
I1028 01:41:10.825712 62912 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I1028 01:41:59.827210 62912 solver.cpp:228] Iteration 600, loss = 2.14848
I1028 01:41:59.827409 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.924888 (* -0.35 = -0.323711 loss)
I1028 01:41:59.827419 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.80338 (* 0.65 = 2.47219 loss)
I1028 01:41:59.827425 62912 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1028 01:42:48.797586 62912 solver.cpp:228] Iteration 640, loss = 2.05446
I1028 01:42:48.797801 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.932313 (* -0.35 = -0.32631 loss)
I1028 01:42:48.797811 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.66272 (* 0.65 = 2.38077 loss)
I1028 01:42:48.797817 62912 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I1028 01:43:37.741654 62912 solver.cpp:228] Iteration 680, loss = 2.22507
I1028 01:43:37.741789 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.93017 (* -0.35 = -0.32556 loss)
I1028 01:43:37.741798 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.92404 (* 0.65 = 2.55063 loss)
I1028 01:43:37.741804 62912 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I1028 01:44:26.699708 62912 solver.cpp:228] Iteration 720, loss = 1.97609
I1028 01:44:26.699853 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.928906 (* -0.35 = -0.325117 loss)
I1028 01:44:26.699867 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.54031 (* 0.65 = 2.3012 loss)
I1028 01:44:26.699872 62912 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I1028 01:45:15.681030 62912 solver.cpp:228] Iteration 760, loss = 1.96561
I1028 01:45:15.681216 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.927536 (* -0.35 = -0.324638 loss)
I1028 01:45:15.681226 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.52346 (* 0.65 = 2.29025 loss)
I1028 01:45:15.681231 62912 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I1028 01:46:04.644487 62912 solver.cpp:228] Iteration 800, loss = 2.07911
I1028 01:46:04.644644 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.931907 (* -0.35 = -0.326167 loss)
I1028 01:46:04.644654 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.70043 (* 0.65 = 2.40528 loss)
I1028 01:46:04.644660 62912 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1028 01:46:53.590813 62912 solver.cpp:228] Iteration 840, loss = 2.20538
I1028 01:46:53.591004 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.934598 (* -0.35 = -0.327109 loss)
I1028 01:46:53.591015 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.89614 (* 0.65 = 2.53249 loss)
I1028 01:46:53.591022 62912 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I1028 01:47:42.547286 62912 solver.cpp:228] Iteration 880, loss = 2.08951
I1028 01:47:42.547438 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.939057 (* -0.35 = -0.32867 loss)
I1028 01:47:42.547448 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.72028 (* 0.65 = 2.41818 loss)
I1028 01:47:42.547454 62912 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I1028 01:48:31.498196 62912 solver.cpp:228] Iteration 920, loss = 1.90866
I1028 01:48:31.498340 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.937639 (* -0.35 = -0.328173 loss)
I1028 01:48:31.498350 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.44128 (* 0.65 = 2.23683 loss)
I1028 01:48:31.498356 62912 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I1028 01:49:20.429321 62912 solver.cpp:228] Iteration 960, loss = 1.92104
I1028 01:49:20.429477 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.933281 (* -0.35 = -0.326648 loss)
I1028 01:49:20.429487 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.45798 (* 0.65 = 2.24768 loss)
I1028 01:49:20.429493 62912 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I1028 01:50:08.146380 62912 solver.cpp:454] Snapshotting to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_1000.caffemodel
I1028 01:50:13.587561 62912 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_1000.solverstate
I1028 01:50:16.464462 62912 solver.cpp:337] Iteration 1000, Testing net (#0)
I1028 01:52:10.636045 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.864221 (* -0.35 = -0.302477 loss)
I1028 01:52:10.636261 62912 solver.cpp:404]     Test net output #1: loss_p2 = 3.91323 (* 0.65 = 2.5436 loss)
I1028 01:52:10.636268 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.20472
I1028 01:52:10.636273 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.42592
I1028 01:52:11.169678 62912 solver.cpp:228] Iteration 1000, loss = 1.84391
I1028 01:52:11.169708 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.929166 (* -0.35 = -0.325208 loss)
I1028 01:52:11.169715 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.33711 (* 0.65 = 2.16912 loss)
I1028 01:52:11.169724 62912 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1028 01:53:00.130305 62912 solver.cpp:228] Iteration 1040, loss = 2.11549
I1028 01:53:00.130465 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.937794 (* -0.35 = -0.328228 loss)
I1028 01:53:00.130476 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.75956 (* 0.65 = 2.44372 loss)
I1028 01:53:00.130482 62912 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I1028 01:53:49.211752 62912 solver.cpp:228] Iteration 1080, loss = 2.09884
I1028 01:53:49.211982 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.944196 (* -0.35 = -0.330469 loss)
I1028 01:53:49.211992 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.7374 (* 0.65 = 2.42931 loss)
I1028 01:53:49.212002 62912 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I1028 01:54:38.631901 62912 solver.cpp:228] Iteration 1120, loss = 1.95121
I1028 01:54:38.632118 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.941632 (* -0.35 = -0.329571 loss)
I1028 01:54:38.632129 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.5089 (* 0.65 = 2.28078 loss)
I1028 01:54:38.632138 62912 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I1028 01:55:28.013381 62912 solver.cpp:228] Iteration 1160, loss = 1.96154
I1028 01:55:28.013609 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.944247 (* -0.35 = -0.330486 loss)
I1028 01:55:28.013620 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.52619 (* 0.65 = 2.29202 loss)
I1028 01:55:28.013629 62912 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I1028 01:56:17.420096 62912 solver.cpp:228] Iteration 1200, loss = 1.85669
I1028 01:56:17.420318 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.944024 (* -0.35 = -0.330408 loss)
I1028 01:56:17.420328 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.36476 (* 0.65 = 2.1871 loss)
I1028 01:56:17.420338 62912 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1028 01:57:06.816162 62912 solver.cpp:228] Iteration 1240, loss = 1.88584
I1028 01:57:06.816381 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.938753 (* -0.35 = -0.328564 loss)
I1028 01:57:06.816391 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.40678 (* 0.65 = 2.21441 loss)
I1028 01:57:06.816401 62912 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I1028 01:57:56.201858 62912 solver.cpp:228] Iteration 1280, loss = 1.9969
I1028 01:57:56.202102 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.942915 (* -0.35 = -0.33002 loss)
I1028 01:57:56.202113 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.57988 (* 0.65 = 2.32692 loss)
I1028 01:57:56.202122 62912 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I1028 01:58:45.607560 62912 solver.cpp:228] Iteration 1320, loss = 1.7666
I1028 01:58:45.607776 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.945496 (* -0.35 = -0.330924 loss)
I1028 01:58:45.607786 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.22695 (* 0.65 = 2.09752 loss)
I1028 01:58:45.607796 62912 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I1028 01:59:34.999948 62912 solver.cpp:228] Iteration 1360, loss = 1.74964
I1028 01:59:35.000174 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.954221 (* -0.35 = -0.333978 loss)
I1028 01:59:35.000185 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.20556 (* 0.65 = 2.08362 loss)
I1028 01:59:35.000193 62912 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I1028 02:00:24.403559 62912 solver.cpp:228] Iteration 1400, loss = 1.96894
I1028 02:00:24.403800 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.943938 (* -0.35 = -0.330378 loss)
I1028 02:00:24.403811 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.53742 (* 0.65 = 2.29932 loss)
I1028 02:00:24.403820 62912 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I1028 02:01:13.765328 62912 solver.cpp:228] Iteration 1440, loss = 1.76565
I1028 02:01:13.765563 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.956358 (* -0.35 = -0.334725 loss)
I1028 02:01:13.765573 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.23135 (* 0.65 = 2.10038 loss)
I1028 02:01:13.765583 62912 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I1028 02:02:03.142539 62912 solver.cpp:228] Iteration 1480, loss = 1.78405
I1028 02:02:03.142760 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.953213 (* -0.35 = -0.333625 loss)
I1028 02:02:03.142771 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.25796 (* 0.65 = 2.11767 loss)
I1028 02:02:03.142779 62912 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I1028 02:02:26.616293 62912 solver.cpp:337] Iteration 1500, Testing net (#0)
I1028 02:04:21.460580 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.896272 (* -0.35 = -0.313695 loss)
I1028 02:04:21.460760 62912 solver.cpp:404]     Test net output #1: loss_p2 = 4.02773 (* 0.65 = 2.61802 loss)
I1028 02:04:21.460770 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.18988
I1028 02:04:21.460777 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.40654
I1028 02:04:46.708803 62912 solver.cpp:228] Iteration 1520, loss = 1.78337
I1028 02:04:46.708863 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.950459 (* -0.35 = -0.33266 loss)
I1028 02:04:46.708870 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.25544 (* 0.65 = 2.11603 loss)
I1028 02:04:46.708879 62912 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I1028 02:05:36.088275 62912 solver.cpp:228] Iteration 1560, loss = 1.72642
I1028 02:05:36.088501 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.955878 (* -0.35 = -0.334557 loss)
I1028 02:05:36.088511 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.17073 (* 0.65 = 2.06098 loss)
I1028 02:05:36.088521 62912 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I1028 02:06:25.478984 62912 solver.cpp:228] Iteration 1600, loss = 1.70831
I1028 02:06:25.479210 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.953473 (* -0.35 = -0.333716 loss)
I1028 02:06:25.479220 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.14158 (* 0.65 = 2.04203 loss)
I1028 02:06:25.479230 62912 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1028 02:07:14.880635 62912 solver.cpp:228] Iteration 1640, loss = 1.76801
I1028 02:07:14.880856 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.952163 (* -0.35 = -0.333257 loss)
I1028 02:07:14.880866 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.23272 (* 0.65 = 2.10127 loss)
I1028 02:07:14.880875 62912 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I1028 02:08:04.272425 62912 solver.cpp:228] Iteration 1680, loss = 1.82705
I1028 02:08:04.272650 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.952549 (* -0.35 = -0.333392 loss)
I1028 02:08:04.272660 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.32375 (* 0.65 = 2.16044 loss)
I1028 02:08:04.272670 62912 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I1028 02:08:53.661236 62912 solver.cpp:228] Iteration 1720, loss = 1.7881
I1028 02:08:53.661461 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.955792 (* -0.35 = -0.334527 loss)
I1028 02:08:53.661473 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.26558 (* 0.65 = 2.12263 loss)
I1028 02:08:53.661480 62912 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I1028 02:09:43.047121 62912 solver.cpp:228] Iteration 1760, loss = 1.67294
I1028 02:09:43.047341 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.961802 (* -0.35 = -0.336631 loss)
I1028 02:09:43.047353 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.09165 (* 0.65 = 2.00958 loss)
I1028 02:09:43.047361 62912 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I1028 02:10:32.434242 62912 solver.cpp:228] Iteration 1800, loss = 1.67441
I1028 02:10:32.434489 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.956283 (* -0.35 = -0.334699 loss)
I1028 02:10:32.434499 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.09093 (* 0.65 = 2.00911 loss)
I1028 02:10:32.434507 62912 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I1028 02:11:21.846606 62912 solver.cpp:228] Iteration 1840, loss = 1.70779
I1028 02:11:21.846830 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.956255 (* -0.35 = -0.334689 loss)
I1028 02:11:21.846840 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.14228 (* 0.65 = 2.04248 loss)
I1028 02:11:21.846849 62912 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I1028 02:12:11.235625 62912 solver.cpp:228] Iteration 1880, loss = 1.58647
I1028 02:12:11.235846 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.956081 (* -0.35 = -0.334628 loss)
I1028 02:12:11.235857 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.95553 (* 0.65 = 1.92109 loss)
I1028 02:12:11.235864 62912 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I1028 02:13:00.643177 62912 solver.cpp:228] Iteration 1920, loss = 1.57544
I1028 02:13:00.643388 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.962408 (* -0.35 = -0.336843 loss)
I1028 02:13:00.643399 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.94198 (* 0.65 = 1.91228 loss)
I1028 02:13:00.643407 62912 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I1028 02:13:50.042764 62912 solver.cpp:228] Iteration 1960, loss = 1.70007
I1028 02:13:50.042994 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.960944 (* -0.35 = -0.33633 loss)
I1028 02:13:50.043004 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.13292 (* 0.65 = 2.0364 loss)
I1028 02:13:50.043014 62912 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I1028 02:14:38.184605 62912 solver.cpp:454] Snapshotting to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_2000.caffemodel
I1028 02:14:43.769383 62912 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_2000.solverstate
I1028 02:14:46.906563 62912 solver.cpp:337] Iteration 2000, Testing net (#0)
I1028 02:16:41.122570 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.912438 (* -0.35 = -0.319353 loss)
I1028 02:16:41.122763 62912 solver.cpp:404]     Test net output #1: loss_p2 = 3.94129 (* 0.65 = 2.56184 loss)
I1028 02:16:41.122771 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.20524
I1028 02:16:41.122776 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.42242
I1028 02:16:41.665210 62912 solver.cpp:228] Iteration 2000, loss = 1.71016
I1028 02:16:41.665269 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.962323 (* -0.35 = -0.336813 loss)
I1028 02:16:41.665278 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.14919 (* 0.65 = 2.04697 loss)
I1028 02:16:41.665287 62912 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1028 02:17:31.081595 62912 solver.cpp:228] Iteration 2040, loss = 1.64359
I1028 02:17:31.081820 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.960781 (* -0.35 = -0.336273 loss)
I1028 02:17:31.081830 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.04594 (* 0.65 = 1.97986 loss)
I1028 02:17:31.081840 62912 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I1028 02:18:20.385367 62912 solver.cpp:228] Iteration 2080, loss = 1.5573
I1028 02:18:20.385581 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.972607 (* -0.35 = -0.340412 loss)
I1028 02:18:20.385591 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.91956 (* 0.65 = 1.89771 loss)
I1028 02:18:20.385597 62912 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I1028 02:19:09.317668 62912 solver.cpp:228] Iteration 2120, loss = 1.6419
I1028 02:19:09.317816 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.961378 (* -0.35 = -0.336482 loss)
I1028 02:19:09.317824 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.04367 (* 0.65 = 1.97838 loss)
I1028 02:19:09.317831 62912 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I1028 02:19:58.255813 62912 solver.cpp:228] Iteration 2160, loss = 1.62893
I1028 02:19:58.256000 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.964133 (* -0.35 = -0.337447 loss)
I1028 02:19:58.256011 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.02519 (* 0.65 = 1.96637 loss)
I1028 02:19:58.256017 62912 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I1028 02:20:47.202682 62912 solver.cpp:228] Iteration 2200, loss = 1.53452
I1028 02:20:47.202847 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.961372 (* -0.35 = -0.33648 loss)
I1028 02:20:47.202857 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.87846 (* 0.65 = 1.871 loss)
I1028 02:20:47.202863 62912 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1028 02:21:36.152884 62912 solver.cpp:228] Iteration 2240, loss = 1.68494
I1028 02:21:36.153045 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.963807 (* -0.35 = -0.337333 loss)
I1028 02:21:36.153055 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.11118 (* 0.65 = 2.02227 loss)
I1028 02:21:36.153061 62912 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I1028 02:22:25.083308 62912 solver.cpp:228] Iteration 2280, loss = 1.48435
I1028 02:22:25.083468 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.967628 (* -0.35 = -0.33867 loss)
I1028 02:22:25.083479 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.80465 (* 0.65 = 1.82302 loss)
I1028 02:22:25.083485 62912 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I1028 02:23:14.039793 62912 solver.cpp:228] Iteration 2320, loss = 1.65384
I1028 02:23:14.039952 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.964288 (* -0.35 = -0.337501 loss)
I1028 02:23:14.039960 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.06359 (* 0.65 = 1.99134 loss)
I1028 02:23:14.039966 62912 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I1028 02:24:02.985486 62912 solver.cpp:228] Iteration 2360, loss = 1.51015
I1028 02:24:02.985640 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.97172 (* -0.35 = -0.340102 loss)
I1028 02:24:02.985649 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.84654 (* 0.65 = 1.85025 loss)
I1028 02:24:02.985654 62912 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I1028 02:24:52.323400 62912 solver.cpp:228] Iteration 2400, loss = 1.51094
I1028 02:24:52.323668 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.968989 (* -0.35 = -0.339146 loss)
I1028 02:24:52.323678 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.84629 (* 0.65 = 1.85009 loss)
I1028 02:24:52.323686 62912 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I1028 02:25:41.669695 62912 solver.cpp:228] Iteration 2440, loss = 1.60236
I1028 02:25:41.669963 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.966947 (* -0.35 = -0.338432 loss)
I1028 02:25:41.669977 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.98583 (* 0.65 = 1.94079 loss)
I1028 02:25:41.669987 62912 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I1028 02:26:31.023586 62912 solver.cpp:228] Iteration 2480, loss = 1.39465
I1028 02:26:31.023841 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.96382 (* -0.35 = -0.337337 loss)
I1028 02:26:31.023851 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.6646 (* 0.65 = 1.73199 loss)
I1028 02:26:31.023860 62912 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I1028 02:26:54.466116 62912 solver.cpp:337] Iteration 2500, Testing net (#0)
I1028 02:28:49.279747 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.924732 (* -0.35 = -0.323656 loss)
I1028 02:28:49.279950 62912 solver.cpp:404]     Test net output #1: loss_p2 = 4.04883 (* 0.65 = 2.63174 loss)
I1028 02:28:49.279958 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.19642
I1028 02:28:49.279963 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.40512
I1028 02:29:14.306053 62912 solver.cpp:228] Iteration 2520, loss = 1.3714
I1028 02:29:14.306107 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.966367 (* -0.35 = -0.338228 loss)
I1028 02:29:14.306114 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.6302 (* 0.65 = 1.70963 loss)
I1028 02:29:14.306120 62912 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I1028 02:30:03.332877 62912 solver.cpp:228] Iteration 2560, loss = 1.51937
I1028 02:30:03.333093 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.974594 (* -0.35 = -0.341108 loss)
I1028 02:30:03.333103 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.86227 (* 0.65 = 1.86047 loss)
I1028 02:30:03.333111 62912 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I1028 02:30:52.302197 62912 solver.cpp:228] Iteration 2600, loss = 1.55234
I1028 02:30:52.319617 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.974853 (* -0.35 = -0.341199 loss)
I1028 02:30:52.319628 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.91313 (* 0.65 = 1.89354 loss)
I1028 02:30:52.319634 62912 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1028 02:31:41.233947 62912 solver.cpp:228] Iteration 2640, loss = 1.5959
I1028 02:31:41.234093 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.978405 (* -0.35 = -0.342442 loss)
I1028 02:31:41.234110 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.98207 (* 0.65 = 1.93834 loss)
I1028 02:31:41.234117 62912 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I1028 02:32:30.167604 62912 solver.cpp:228] Iteration 2680, loss = 1.45401
I1028 02:32:30.167760 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.973936 (* -0.35 = -0.340878 loss)
I1028 02:32:30.167770 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.76136 (* 0.65 = 1.79488 loss)
I1028 02:32:30.167776 62912 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I1028 02:33:19.152098 62912 solver.cpp:228] Iteration 2720, loss = 1.30841
I1028 02:33:19.152317 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.97201 (* -0.35 = -0.340203 loss)
I1028 02:33:19.152328 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.53633 (* 0.65 = 1.64861 loss)
I1028 02:33:19.152334 62912 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I1028 02:34:08.316324 62912 solver.cpp:228] Iteration 2760, loss = 1.41348
I1028 02:34:08.316550 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.971833 (* -0.35 = -0.340141 loss)
I1028 02:34:08.316560 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.69787 (* 0.65 = 1.75362 loss)
I1028 02:34:08.316570 62912 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I1028 02:34:57.432457 62912 solver.cpp:228] Iteration 2800, loss = 1.48878
I1028 02:34:57.432674 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.973374 (* -0.35 = -0.340681 loss)
I1028 02:34:57.432685 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.81456 (* 0.65 = 1.82946 loss)
I1028 02:34:57.432692 62912 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I1028 02:35:46.363926 62912 solver.cpp:228] Iteration 2840, loss = 1.54173
I1028 02:35:46.364084 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.978494 (* -0.35 = -0.342473 loss)
I1028 02:35:46.364094 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.89877 (* 0.65 = 1.8842 loss)
I1028 02:35:46.364100 62912 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I1028 02:36:35.342694 62912 solver.cpp:228] Iteration 2880, loss = 1.37998
I1028 02:36:35.342880 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.975371 (* -0.35 = -0.34138 loss)
I1028 02:36:35.342898 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.64824 (* 0.65 = 1.72136 loss)
I1028 02:36:35.342905 62912 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I1028 02:37:24.316720 62912 solver.cpp:228] Iteration 2920, loss = 1.41696
I1028 02:37:24.316931 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.973693 (* -0.35 = -0.340793 loss)
I1028 02:37:24.316946 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.70423 (* 0.65 = 1.75775 loss)
I1028 02:37:24.316951 62912 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I1028 02:38:13.231762 62912 solver.cpp:228] Iteration 2960, loss = 1.51267
I1028 02:38:13.231940 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.980631 (* -0.35 = -0.343221 loss)
I1028 02:38:13.231950 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.85521 (* 0.65 = 1.85589 loss)
I1028 02:38:13.231956 62912 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I1028 02:39:00.930276 62912 solver.cpp:454] Snapshotting to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_3000.caffemodel
I1028 02:39:06.548740 62912 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_3000.solverstate
I1028 02:39:10.836851 62912 solver.cpp:337] Iteration 3000, Testing net (#0)
I1028 02:41:05.033550 62912 solver.cpp:404]     Test net output #0: loss_p1 = 0.931039 (* -0.35 = -0.325863 loss)
I1028 02:41:05.033757 62912 solver.cpp:404]     Test net output #1: loss_p2 = 3.98498 (* 0.65 = 2.59024 loss)
I1028 02:41:05.033771 62912 solver.cpp:404]     Test net output #2: precision@1 = 0.20784
I1028 02:41:05.033776 62912 solver.cpp:404]     Test net output #3: precision@5 = 0.42414
I1028 02:41:05.566977 62912 solver.cpp:228] Iteration 3000, loss = 1.25226
I1028 02:41:05.567008 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.970941 (* -0.35 = -0.33983 loss)
I1028 02:41:05.567016 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.44936 (* 0.65 = 1.59209 loss)
I1028 02:41:05.567026 62912 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1028 02:41:54.508340 62912 solver.cpp:228] Iteration 3040, loss = 1.43382
I1028 02:41:54.508497 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.979361 (* -0.35 = -0.342776 loss)
I1028 02:41:54.508507 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.73322 (* 0.65 = 1.77659 loss)
I1028 02:41:54.508513 62912 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I1028 02:42:43.467388 62912 solver.cpp:228] Iteration 3080, loss = 1.50256
I1028 02:42:43.467547 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.982825 (* -0.35 = -0.343989 loss)
I1028 02:42:43.467557 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.84084 (* 0.65 = 1.84655 loss)
I1028 02:42:43.467563 62912 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I1028 02:43:32.405980 62912 solver.cpp:228] Iteration 3120, loss = 1.40079
I1028 02:43:32.406136 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.982519 (* -0.35 = -0.343882 loss)
I1028 02:43:32.406147 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.68411 (* 0.65 = 1.74467 loss)
I1028 02:43:32.406152 62912 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I1028 02:44:21.351675 62912 solver.cpp:228] Iteration 3160, loss = 1.33646
I1028 02:44:21.351833 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.982259 (* -0.35 = -0.343791 loss)
I1028 02:44:21.351843 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.585 (* 0.65 = 1.68025 loss)
I1028 02:44:21.351850 62912 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I1028 02:45:10.301502 62912 solver.cpp:228] Iteration 3200, loss = 1.61998
I1028 02:45:10.301702 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.990767 (* -0.35 = -0.346768 loss)
I1028 02:45:10.301712 62912 solver.cpp:244]     Train net output #1: loss_p2 = 3.02576 (* 0.65 = 1.96674 loss)
I1028 02:45:10.301717 62912 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I1028 02:45:59.231456 62912 solver.cpp:228] Iteration 3240, loss = 1.41107
I1028 02:45:59.231607 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.986977 (* -0.35 = -0.345442 loss)
I1028 02:45:59.231622 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.70232 (* 0.65 = 1.75651 loss)
I1028 02:45:59.231628 62912 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I1028 02:46:48.155987 62912 solver.cpp:228] Iteration 3280, loss = 1.30868
I1028 02:46:48.156184 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.985584 (* -0.35 = -0.344954 loss)
I1028 02:46:48.156194 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.54405 (* 0.65 = 1.65363 loss)
I1028 02:46:48.156200 62912 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I1028 02:47:37.094607 62912 solver.cpp:228] Iteration 3320, loss = 1.25604
I1028 02:47:37.094776 62912 solver.cpp:244]     Train net output #0: loss_p1 = 0.977171 (* -0.35 = -0.34201 loss)
I1028 02:47:37.094786 62912 solver.cpp:244]     Train net output #1: loss_p2 = 2.45853 (* 0.65 = 1.59805 loss)
I1028 02:47:37.094792 62912 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
