I1028 02:50:53.267763 63928 caffe.cpp:185] Using GPUs 0
I1028 02:50:53.273658 63928 caffe.cpp:190] GPU 0: Tesla K40m
I1028 02:50:53.527757 63928 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 500
base_lr: 0.01
display: 40
max_iter: 20000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 1000
snapshot_prefix: "models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train"
solver_mode: GPU
device_id: 0
net: "models/dissimilarity_siamese_net/train_val.prototxt"
I1028 02:50:53.530730 63928 solver.cpp:91] Creating training net from net file: models/dissimilarity_siamese_net/train_val.prototxt
I1028 02:50:53.534358 63928 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1028 02:50:53.534425 63928 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer precision@1
I1028 02:50:53.534430 63928 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer precision@5
I1028 02:50:53.535537 63928 net.cpp:49] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "examples/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/work/03713/harshal1/maverick/vision_proj/data/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "fc7_n"
  type: "L2Norm"
  bottom: "fc7"
  top: "fc7_n"
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data"
  top: "conv1_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "conv1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "norm1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "conv2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "norm2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "pool2_p"
  top: "conv3_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "pool5_p"
  type: "Pooling"
  bottom: "conv5_p"
  top: "pool5_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "pool5_p"
  top: "fc6_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7_p"
  type: "ReLU"
  bottom: "fc7_p"
  top: "fc7_p"
}
layer {
  name: "drop7_p"
  type: "Dropout"
  bottom: "fc7_p"
  top: "fc7_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 504
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc7_pn"
  type: "L2Norm"
  bottom: "fc7_p"
  top: "fc7_pn"
}
layer {
  name: "loss_p1"
  type: "EuclideanLoss"
  bottom: "fc7_n"
  bottom: "fc7_pn"
  top: "loss_p1"
  loss_weight: -0.35
}
layer {
  name: "loss_p2"
  type: "SoftmaxWithLoss"
  bottom: "fc8_p"
  bottom: "label"
  top: "loss_p2"
  loss_weight: 0.65
}
I1028 02:50:53.535931 63928 layer_factory.hpp:77] Creating layer data
I1028 02:50:53.536801 63928 net.cpp:91] Creating Layer data
I1028 02:50:53.536854 63928 net.cpp:399] data -> data
I1028 02:50:53.536967 63928 net.cpp:399] data -> label
I1028 02:50:53.537024 63928 data_transformer.cpp:25] Loading mean file from: examples/imagenet/imagenet_mean.binaryproto
I1028 02:50:53.547641 64039 db_lmdb.cpp:35] Opened lmdb /work/03713/harshal1/maverick/vision_proj/data/train_lmdb
I1028 02:50:53.559206 63928 data_layer.cpp:41] output data size: 256,3,227,227
I1028 02:50:53.860445 63928 net.cpp:141] Setting up data
I1028 02:50:53.860577 63928 net.cpp:148] Top shape: 256 3 227 227 (39574272)
I1028 02:50:53.860585 63928 net.cpp:148] Top shape: 256 (256)
I1028 02:50:53.860589 63928 net.cpp:156] Memory required for data: 158298112
I1028 02:50:53.860607 63928 layer_factory.hpp:77] Creating layer data_data_0_split
I1028 02:50:53.860653 63928 net.cpp:91] Creating Layer data_data_0_split
I1028 02:50:53.860687 63928 net.cpp:425] data_data_0_split <- data
I1028 02:50:53.860715 63928 net.cpp:399] data_data_0_split -> data_data_0_split_0
I1028 02:50:53.860733 63928 net.cpp:399] data_data_0_split -> data_data_0_split_1
I1028 02:50:53.860812 63928 net.cpp:141] Setting up data_data_0_split
I1028 02:50:53.860824 63928 net.cpp:148] Top shape: 256 3 227 227 (39574272)
I1028 02:50:53.860828 63928 net.cpp:148] Top shape: 256 3 227 227 (39574272)
I1028 02:50:53.860831 63928 net.cpp:156] Memory required for data: 474892288
I1028 02:50:53.860836 63928 layer_factory.hpp:77] Creating layer conv1
I1028 02:50:53.860893 63928 net.cpp:91] Creating Layer conv1
I1028 02:50:53.860916 63928 net.cpp:425] conv1 <- data_data_0_split_0
I1028 02:50:53.860924 63928 net.cpp:399] conv1 -> conv1
I1028 02:50:54.256157 63928 net.cpp:141] Setting up conv1
I1028 02:50:54.256207 63928 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 02:50:54.256249 63928 net.cpp:156] Memory required for data: 772261888
I1028 02:50:54.256342 63928 layer_factory.hpp:77] Creating layer relu1
I1028 02:50:54.256386 63928 net.cpp:91] Creating Layer relu1
I1028 02:50:54.256397 63928 net.cpp:425] relu1 <- conv1
I1028 02:50:54.256403 63928 net.cpp:386] relu1 -> conv1 (in-place)
I1028 02:50:54.256724 63928 net.cpp:141] Setting up relu1
I1028 02:50:54.256736 63928 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 02:50:54.256738 63928 net.cpp:156] Memory required for data: 1069631488
I1028 02:50:54.256742 63928 layer_factory.hpp:77] Creating layer norm1
I1028 02:50:54.256791 63928 net.cpp:91] Creating Layer norm1
I1028 02:50:54.256799 63928 net.cpp:425] norm1 <- conv1
I1028 02:50:54.256805 63928 net.cpp:399] norm1 -> norm1
I1028 02:50:54.257061 63928 net.cpp:141] Setting up norm1
I1028 02:50:54.257071 63928 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 02:50:54.257074 63928 net.cpp:156] Memory required for data: 1367001088
I1028 02:50:54.257077 63928 layer_factory.hpp:77] Creating layer pool1
I1028 02:50:54.257086 63928 net.cpp:91] Creating Layer pool1
I1028 02:50:54.257089 63928 net.cpp:425] pool1 <- norm1
I1028 02:50:54.257094 63928 net.cpp:399] pool1 -> pool1
I1028 02:50:54.257206 63928 net.cpp:141] Setting up pool1
I1028 02:50:54.257216 63928 net.cpp:148] Top shape: 256 96 27 27 (17915904)
I1028 02:50:54.257220 63928 net.cpp:156] Memory required for data: 1438664704
I1028 02:50:54.257222 63928 layer_factory.hpp:77] Creating layer conv2
I1028 02:50:54.257243 63928 net.cpp:91] Creating Layer conv2
I1028 02:50:54.257247 63928 net.cpp:425] conv2 <- pool1
I1028 02:50:54.257254 63928 net.cpp:399] conv2 -> conv2
I1028 02:50:54.263963 63928 net.cpp:141] Setting up conv2
I1028 02:50:54.263978 63928 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 02:50:54.263994 63928 net.cpp:156] Memory required for data: 1629767680
I1028 02:50:54.264003 63928 layer_factory.hpp:77] Creating layer relu2
I1028 02:50:54.264011 63928 net.cpp:91] Creating Layer relu2
I1028 02:50:54.264014 63928 net.cpp:425] relu2 <- conv2
I1028 02:50:54.264019 63928 net.cpp:386] relu2 -> conv2 (in-place)
I1028 02:50:54.264171 63928 net.cpp:141] Setting up relu2
I1028 02:50:54.264181 63928 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 02:50:54.264184 63928 net.cpp:156] Memory required for data: 1820870656
I1028 02:50:54.264188 63928 layer_factory.hpp:77] Creating layer norm2
I1028 02:50:54.264197 63928 net.cpp:91] Creating Layer norm2
I1028 02:50:54.264201 63928 net.cpp:425] norm2 <- conv2
I1028 02:50:54.264209 63928 net.cpp:399] norm2 -> norm2
I1028 02:50:54.264493 63928 net.cpp:141] Setting up norm2
I1028 02:50:54.264504 63928 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 02:50:54.264519 63928 net.cpp:156] Memory required for data: 2011973632
I1028 02:50:54.264523 63928 layer_factory.hpp:77] Creating layer pool2
I1028 02:50:54.264530 63928 net.cpp:91] Creating Layer pool2
I1028 02:50:54.264539 63928 net.cpp:425] pool2 <- norm2
I1028 02:50:54.264544 63928 net.cpp:399] pool2 -> pool2
I1028 02:50:54.264590 63928 net.cpp:141] Setting up pool2
I1028 02:50:54.264596 63928 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 02:50:54.264600 63928 net.cpp:156] Memory required for data: 2056275968
I1028 02:50:54.264602 63928 layer_factory.hpp:77] Creating layer conv3
I1028 02:50:54.264616 63928 net.cpp:91] Creating Layer conv3
I1028 02:50:54.264619 63928 net.cpp:425] conv3 <- pool2
I1028 02:50:54.264626 63928 net.cpp:399] conv3 -> conv3
I1028 02:50:54.278403 63928 net.cpp:141] Setting up conv3
I1028 02:50:54.278415 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:54.278419 63928 net.cpp:156] Memory required for data: 2122729472
I1028 02:50:54.278427 63928 layer_factory.hpp:77] Creating layer relu3
I1028 02:50:54.278435 63928 net.cpp:91] Creating Layer relu3
I1028 02:50:54.278439 63928 net.cpp:425] relu3 <- conv3
I1028 02:50:54.278446 63928 net.cpp:386] relu3 -> conv3 (in-place)
I1028 02:50:54.278601 63928 net.cpp:141] Setting up relu3
I1028 02:50:54.278610 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:54.278640 63928 net.cpp:156] Memory required for data: 2189182976
I1028 02:50:54.278643 63928 layer_factory.hpp:77] Creating layer conv4
I1028 02:50:54.278658 63928 net.cpp:91] Creating Layer conv4
I1028 02:50:54.278662 63928 net.cpp:425] conv4 <- conv3
I1028 02:50:54.278668 63928 net.cpp:399] conv4 -> conv4
I1028 02:50:54.300581 63928 net.cpp:141] Setting up conv4
I1028 02:50:54.300609 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:54.300612 63928 net.cpp:156] Memory required for data: 2255636480
I1028 02:50:54.300619 63928 layer_factory.hpp:77] Creating layer relu4
I1028 02:50:54.300627 63928 net.cpp:91] Creating Layer relu4
I1028 02:50:54.300631 63928 net.cpp:425] relu4 <- conv4
I1028 02:50:54.300639 63928 net.cpp:386] relu4 -> conv4 (in-place)
I1028 02:50:54.300789 63928 net.cpp:141] Setting up relu4
I1028 02:50:54.300801 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:54.300804 63928 net.cpp:156] Memory required for data: 2322089984
I1028 02:50:54.300807 63928 layer_factory.hpp:77] Creating layer conv5
I1028 02:50:54.300819 63928 net.cpp:91] Creating Layer conv5
I1028 02:50:54.300823 63928 net.cpp:425] conv5 <- conv4
I1028 02:50:54.300829 63928 net.cpp:399] conv5 -> conv5
I1028 02:50:54.309007 63928 net.cpp:141] Setting up conv5
I1028 02:50:54.309034 63928 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 02:50:54.309038 63928 net.cpp:156] Memory required for data: 2366392320
I1028 02:50:54.309049 63928 layer_factory.hpp:77] Creating layer relu5
I1028 02:50:54.309057 63928 net.cpp:91] Creating Layer relu5
I1028 02:50:54.309061 63928 net.cpp:425] relu5 <- conv5
I1028 02:50:54.309067 63928 net.cpp:386] relu5 -> conv5 (in-place)
I1028 02:50:54.309217 63928 net.cpp:141] Setting up relu5
I1028 02:50:54.309226 63928 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 02:50:54.309229 63928 net.cpp:156] Memory required for data: 2410694656
I1028 02:50:54.309233 63928 layer_factory.hpp:77] Creating layer pool5
I1028 02:50:54.309247 63928 net.cpp:91] Creating Layer pool5
I1028 02:50:54.309252 63928 net.cpp:425] pool5 <- conv5
I1028 02:50:54.309258 63928 net.cpp:399] pool5 -> pool5
I1028 02:50:54.309299 63928 net.cpp:141] Setting up pool5
I1028 02:50:54.309308 63928 net.cpp:148] Top shape: 256 256 6 6 (2359296)
I1028 02:50:54.309310 63928 net.cpp:156] Memory required for data: 2420131840
I1028 02:50:54.309314 63928 layer_factory.hpp:77] Creating layer fc6
I1028 02:50:54.309325 63928 net.cpp:91] Creating Layer fc6
I1028 02:50:54.309329 63928 net.cpp:425] fc6 <- pool5
I1028 02:50:54.309334 63928 net.cpp:399] fc6 -> fc6
I1028 02:50:54.841773 63928 net.cpp:141] Setting up fc6
I1028 02:50:54.841821 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:54.841826 63928 net.cpp:156] Memory required for data: 2424326144
I1028 02:50:54.841838 63928 layer_factory.hpp:77] Creating layer relu6
I1028 02:50:54.841856 63928 net.cpp:91] Creating Layer relu6
I1028 02:50:54.841861 63928 net.cpp:425] relu6 <- fc6
I1028 02:50:54.841871 63928 net.cpp:386] relu6 -> fc6 (in-place)
I1028 02:50:54.842355 63928 net.cpp:141] Setting up relu6
I1028 02:50:54.842365 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:54.842368 63928 net.cpp:156] Memory required for data: 2428520448
I1028 02:50:54.842372 63928 layer_factory.hpp:77] Creating layer drop6
I1028 02:50:54.842447 63928 net.cpp:91] Creating Layer drop6
I1028 02:50:54.842453 63928 net.cpp:425] drop6 <- fc6
I1028 02:50:54.842463 63928 net.cpp:386] drop6 -> fc6 (in-place)
I1028 02:50:54.842531 63928 net.cpp:141] Setting up drop6
I1028 02:50:54.842543 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:54.842546 63928 net.cpp:156] Memory required for data: 2432714752
I1028 02:50:54.842550 63928 layer_factory.hpp:77] Creating layer fc7
I1028 02:50:54.842561 63928 net.cpp:91] Creating Layer fc7
I1028 02:50:54.842564 63928 net.cpp:425] fc7 <- fc6
I1028 02:50:54.842572 63928 net.cpp:399] fc7 -> fc7
I1028 02:50:55.075062 63928 net.cpp:141] Setting up fc7
I1028 02:50:55.075119 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.075161 63928 net.cpp:156] Memory required for data: 2436909056
I1028 02:50:55.075176 63928 layer_factory.hpp:77] Creating layer fc7_n
I1028 02:50:55.075255 63928 net.cpp:91] Creating Layer fc7_n
I1028 02:50:55.075263 63928 net.cpp:425] fc7_n <- fc7
I1028 02:50:55.075275 63928 net.cpp:399] fc7_n -> fc7_n
I1028 02:50:55.075314 63928 net.cpp:141] Setting up fc7_n
I1028 02:50:55.075325 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.075328 63928 net.cpp:156] Memory required for data: 2441103360
I1028 02:50:55.075332 63928 layer_factory.hpp:77] Creating layer conv1_p
I1028 02:50:55.075351 63928 net.cpp:91] Creating Layer conv1_p
I1028 02:50:55.075356 63928 net.cpp:425] conv1_p <- data_data_0_split_1
I1028 02:50:55.075362 63928 net.cpp:399] conv1_p -> conv1_p
I1028 02:50:55.076974 63928 net.cpp:141] Setting up conv1_p
I1028 02:50:55.076985 63928 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 02:50:55.077000 63928 net.cpp:156] Memory required for data: 2738472960
I1028 02:50:55.077006 63928 layer_factory.hpp:77] Creating layer relu1_p
I1028 02:50:55.077018 63928 net.cpp:91] Creating Layer relu1_p
I1028 02:50:55.077021 63928 net.cpp:425] relu1_p <- conv1_p
I1028 02:50:55.077025 63928 net.cpp:386] relu1_p -> conv1_p (in-place)
I1028 02:50:55.077175 63928 net.cpp:141] Setting up relu1_p
I1028 02:50:55.077186 63928 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 02:50:55.077189 63928 net.cpp:156] Memory required for data: 3035842560
I1028 02:50:55.077193 63928 layer_factory.hpp:77] Creating layer norm1_p
I1028 02:50:55.077203 63928 net.cpp:91] Creating Layer norm1_p
I1028 02:50:55.077206 63928 net.cpp:425] norm1_p <- conv1_p
I1028 02:50:55.077214 63928 net.cpp:399] norm1_p -> norm1_p
I1028 02:50:55.077488 63928 net.cpp:141] Setting up norm1_p
I1028 02:50:55.077510 63928 net.cpp:148] Top shape: 256 96 55 55 (74342400)
I1028 02:50:55.077513 63928 net.cpp:156] Memory required for data: 3333212160
I1028 02:50:55.077517 63928 layer_factory.hpp:77] Creating layer pool1_p
I1028 02:50:55.077527 63928 net.cpp:91] Creating Layer pool1_p
I1028 02:50:55.077530 63928 net.cpp:425] pool1_p <- norm1_p
I1028 02:50:55.077535 63928 net.cpp:399] pool1_p -> pool1_p
I1028 02:50:55.077586 63928 net.cpp:141] Setting up pool1_p
I1028 02:50:55.077592 63928 net.cpp:148] Top shape: 256 96 27 27 (17915904)
I1028 02:50:55.077594 63928 net.cpp:156] Memory required for data: 3404875776
I1028 02:50:55.077599 63928 layer_factory.hpp:77] Creating layer conv2_p
I1028 02:50:55.077608 63928 net.cpp:91] Creating Layer conv2_p
I1028 02:50:55.077612 63928 net.cpp:425] conv2_p <- pool1_p
I1028 02:50:55.077620 63928 net.cpp:399] conv2_p -> conv2_p
I1028 02:50:55.083353 63928 net.cpp:141] Setting up conv2_p
I1028 02:50:55.083364 63928 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 02:50:55.083379 63928 net.cpp:156] Memory required for data: 3595978752
I1028 02:50:55.083392 63928 layer_factory.hpp:77] Creating layer relu2_p
I1028 02:50:55.083400 63928 net.cpp:91] Creating Layer relu2_p
I1028 02:50:55.083403 63928 net.cpp:425] relu2_p <- conv2_p
I1028 02:50:55.083408 63928 net.cpp:386] relu2_p -> conv2_p (in-place)
I1028 02:50:55.083662 63928 net.cpp:141] Setting up relu2_p
I1028 02:50:55.083673 63928 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 02:50:55.083675 63928 net.cpp:156] Memory required for data: 3787081728
I1028 02:50:55.083679 63928 layer_factory.hpp:77] Creating layer norm2_p
I1028 02:50:55.083688 63928 net.cpp:91] Creating Layer norm2_p
I1028 02:50:55.083691 63928 net.cpp:425] norm2_p <- conv2_p
I1028 02:50:55.083696 63928 net.cpp:399] norm2_p -> norm2_p
I1028 02:50:55.083865 63928 net.cpp:141] Setting up norm2_p
I1028 02:50:55.083874 63928 net.cpp:148] Top shape: 256 256 27 27 (47775744)
I1028 02:50:55.083878 63928 net.cpp:156] Memory required for data: 3978184704
I1028 02:50:55.083880 63928 layer_factory.hpp:77] Creating layer pool2_p
I1028 02:50:55.083886 63928 net.cpp:91] Creating Layer pool2_p
I1028 02:50:55.083894 63928 net.cpp:425] pool2_p <- norm2_p
I1028 02:50:55.083914 63928 net.cpp:399] pool2_p -> pool2_p
I1028 02:50:55.083952 63928 net.cpp:141] Setting up pool2_p
I1028 02:50:55.083958 63928 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 02:50:55.083961 63928 net.cpp:156] Memory required for data: 4022487040
I1028 02:50:55.083963 63928 layer_factory.hpp:77] Creating layer conv3_p
I1028 02:50:55.083986 63928 net.cpp:91] Creating Layer conv3_p
I1028 02:50:55.083988 63928 net.cpp:425] conv3_p <- pool2_p
I1028 02:50:55.083994 63928 net.cpp:399] conv3_p -> conv3_p
I1028 02:50:55.097362 63928 net.cpp:141] Setting up conv3_p
I1028 02:50:55.097386 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:55.097389 63928 net.cpp:156] Memory required for data: 4088940544
I1028 02:50:55.097395 63928 layer_factory.hpp:77] Creating layer relu3_p
I1028 02:50:55.097403 63928 net.cpp:91] Creating Layer relu3_p
I1028 02:50:55.097405 63928 net.cpp:425] relu3_p <- conv3_p
I1028 02:50:55.097410 63928 net.cpp:386] relu3_p -> conv3_p (in-place)
I1028 02:50:55.097659 63928 net.cpp:141] Setting up relu3_p
I1028 02:50:55.097671 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:55.097673 63928 net.cpp:156] Memory required for data: 4155394048
I1028 02:50:55.097676 63928 layer_factory.hpp:77] Creating layer conv4_p
I1028 02:50:55.097694 63928 net.cpp:91] Creating Layer conv4_p
I1028 02:50:55.097698 63928 net.cpp:425] conv4_p <- conv3_p
I1028 02:50:55.097707 63928 net.cpp:399] conv4_p -> conv4_p
I1028 02:50:55.108799 63928 net.cpp:141] Setting up conv4_p
I1028 02:50:55.108810 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:55.108825 63928 net.cpp:156] Memory required for data: 4221847552
I1028 02:50:55.108831 63928 layer_factory.hpp:77] Creating layer relu4_p
I1028 02:50:55.108839 63928 net.cpp:91] Creating Layer relu4_p
I1028 02:50:55.108842 63928 net.cpp:425] relu4_p <- conv4_p
I1028 02:50:55.108851 63928 net.cpp:386] relu4_p -> conv4_p (in-place)
I1028 02:50:55.109122 63928 net.cpp:141] Setting up relu4_p
I1028 02:50:55.109133 63928 net.cpp:148] Top shape: 256 384 13 13 (16613376)
I1028 02:50:55.109136 63928 net.cpp:156] Memory required for data: 4288301056
I1028 02:50:55.109140 63928 layer_factory.hpp:77] Creating layer conv5_p
I1028 02:50:55.109153 63928 net.cpp:91] Creating Layer conv5_p
I1028 02:50:55.109156 63928 net.cpp:425] conv5_p <- conv4_p
I1028 02:50:55.109165 63928 net.cpp:399] conv5_p -> conv5_p
I1028 02:50:55.116884 63928 net.cpp:141] Setting up conv5_p
I1028 02:50:55.116899 63928 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 02:50:55.116915 63928 net.cpp:156] Memory required for data: 4332603392
I1028 02:50:55.116921 63928 layer_factory.hpp:77] Creating layer relu5_p
I1028 02:50:55.116927 63928 net.cpp:91] Creating Layer relu5_p
I1028 02:50:55.116930 63928 net.cpp:425] relu5_p <- conv5_p
I1028 02:50:55.116937 63928 net.cpp:386] relu5_p -> conv5_p (in-place)
I1028 02:50:55.117179 63928 net.cpp:141] Setting up relu5_p
I1028 02:50:55.117189 63928 net.cpp:148] Top shape: 256 256 13 13 (11075584)
I1028 02:50:55.117192 63928 net.cpp:156] Memory required for data: 4376905728
I1028 02:50:55.117197 63928 layer_factory.hpp:77] Creating layer pool5_p
I1028 02:50:55.117202 63928 net.cpp:91] Creating Layer pool5_p
I1028 02:50:55.117207 63928 net.cpp:425] pool5_p <- conv5_p
I1028 02:50:55.117213 63928 net.cpp:399] pool5_p -> pool5_p
I1028 02:50:55.117254 63928 net.cpp:141] Setting up pool5_p
I1028 02:50:55.117262 63928 net.cpp:148] Top shape: 256 256 6 6 (2359296)
I1028 02:50:55.117265 63928 net.cpp:156] Memory required for data: 4386342912
I1028 02:50:55.117269 63928 layer_factory.hpp:77] Creating layer fc6_p
I1028 02:50:55.117277 63928 net.cpp:91] Creating Layer fc6_p
I1028 02:50:55.117280 63928 net.cpp:425] fc6_p <- pool5_p
I1028 02:50:55.117288 63928 net.cpp:399] fc6_p -> fc6_p
I1028 02:50:55.648957 63928 net.cpp:141] Setting up fc6_p
I1028 02:50:55.649011 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.649015 63928 net.cpp:156] Memory required for data: 4390537216
I1028 02:50:55.649029 63928 layer_factory.hpp:77] Creating layer relu6_p
I1028 02:50:55.649088 63928 net.cpp:91] Creating Layer relu6_p
I1028 02:50:55.649094 63928 net.cpp:425] relu6_p <- fc6_p
I1028 02:50:55.649106 63928 net.cpp:386] relu6_p -> fc6_p (in-place)
I1028 02:50:55.649572 63928 net.cpp:141] Setting up relu6_p
I1028 02:50:55.649595 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.649598 63928 net.cpp:156] Memory required for data: 4394731520
I1028 02:50:55.649602 63928 layer_factory.hpp:77] Creating layer drop6_p
I1028 02:50:55.649613 63928 net.cpp:91] Creating Layer drop6_p
I1028 02:50:55.649616 63928 net.cpp:425] drop6_p <- fc6_p
I1028 02:50:55.649621 63928 net.cpp:386] drop6_p -> fc6_p (in-place)
I1028 02:50:55.649647 63928 net.cpp:141] Setting up drop6_p
I1028 02:50:55.649652 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.649655 63928 net.cpp:156] Memory required for data: 4398925824
I1028 02:50:55.649658 63928 layer_factory.hpp:77] Creating layer fc7_p
I1028 02:50:55.649670 63928 net.cpp:91] Creating Layer fc7_p
I1028 02:50:55.649673 63928 net.cpp:425] fc7_p <- fc6_p
I1028 02:50:55.649682 63928 net.cpp:399] fc7_p -> fc7_p
I1028 02:50:55.884856 63928 net.cpp:141] Setting up fc7_p
I1028 02:50:55.884914 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.884919 63928 net.cpp:156] Memory required for data: 4403120128
I1028 02:50:55.884932 63928 layer_factory.hpp:77] Creating layer relu7_p
I1028 02:50:55.884953 63928 net.cpp:91] Creating Layer relu7_p
I1028 02:50:55.884960 63928 net.cpp:425] relu7_p <- fc7_p
I1028 02:50:55.884970 63928 net.cpp:386] relu7_p -> fc7_p (in-place)
I1028 02:50:55.885212 63928 net.cpp:141] Setting up relu7_p
I1028 02:50:55.885223 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.885226 63928 net.cpp:156] Memory required for data: 4407314432
I1028 02:50:55.885229 63928 layer_factory.hpp:77] Creating layer drop7_p
I1028 02:50:55.885241 63928 net.cpp:91] Creating Layer drop7_p
I1028 02:50:55.885246 63928 net.cpp:425] drop7_p <- fc7_p
I1028 02:50:55.885249 63928 net.cpp:386] drop7_p -> fc7_p (in-place)
I1028 02:50:55.885274 63928 net.cpp:141] Setting up drop7_p
I1028 02:50:55.885282 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.885284 63928 net.cpp:156] Memory required for data: 4411508736
I1028 02:50:55.885288 63928 layer_factory.hpp:77] Creating layer fc7_p_drop7_p_0_split
I1028 02:50:55.885295 63928 net.cpp:91] Creating Layer fc7_p_drop7_p_0_split
I1028 02:50:55.885298 63928 net.cpp:425] fc7_p_drop7_p_0_split <- fc7_p
I1028 02:50:55.885305 63928 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_0
I1028 02:50:55.885313 63928 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_1
I1028 02:50:55.885349 63928 net.cpp:141] Setting up fc7_p_drop7_p_0_split
I1028 02:50:55.885359 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.885362 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.885365 63928 net.cpp:156] Memory required for data: 4419897344
I1028 02:50:55.885368 63928 layer_factory.hpp:77] Creating layer fc8_p
I1028 02:50:55.885380 63928 net.cpp:91] Creating Layer fc8_p
I1028 02:50:55.885382 63928 net.cpp:425] fc8_p <- fc7_p_drop7_p_0_split_0
I1028 02:50:55.885390 63928 net.cpp:399] fc8_p -> fc8_p
I1028 02:50:55.914347 63928 net.cpp:141] Setting up fc8_p
I1028 02:50:55.914358 63928 net.cpp:148] Top shape: 256 504 (129024)
I1028 02:50:55.914373 63928 net.cpp:156] Memory required for data: 4420413440
I1028 02:50:55.914379 63928 layer_factory.hpp:77] Creating layer fc7_pn
I1028 02:50:55.914386 63928 net.cpp:91] Creating Layer fc7_pn
I1028 02:50:55.914391 63928 net.cpp:425] fc7_pn <- fc7_p_drop7_p_0_split_1
I1028 02:50:55.914396 63928 net.cpp:399] fc7_pn -> fc7_pn
I1028 02:50:55.914423 63928 net.cpp:141] Setting up fc7_pn
I1028 02:50:55.914430 63928 net.cpp:148] Top shape: 256 4096 (1048576)
I1028 02:50:55.914433 63928 net.cpp:156] Memory required for data: 4424607744
I1028 02:50:55.914436 63928 layer_factory.hpp:77] Creating layer loss_p1
I1028 02:50:55.914497 63928 net.cpp:91] Creating Layer loss_p1
I1028 02:50:55.914505 63928 net.cpp:425] loss_p1 <- fc7_n
I1028 02:50:55.914541 63928 net.cpp:425] loss_p1 <- fc7_pn
I1028 02:50:55.914551 63928 net.cpp:399] loss_p1 -> loss_p1
I1028 02:50:55.914633 63928 net.cpp:141] Setting up loss_p1
I1028 02:50:55.914645 63928 net.cpp:148] Top shape: (1)
I1028 02:50:55.914649 63928 net.cpp:151]     with loss weight -0.35
I1028 02:50:55.914737 63928 net.cpp:156] Memory required for data: 4424607748
I1028 02:50:55.914742 63928 layer_factory.hpp:77] Creating layer loss_p2
I1028 02:50:55.914783 63928 net.cpp:91] Creating Layer loss_p2
I1028 02:50:55.914789 63928 net.cpp:425] loss_p2 <- fc8_p
I1028 02:50:55.914793 63928 net.cpp:425] loss_p2 <- label
I1028 02:50:55.914798 63928 net.cpp:399] loss_p2 -> loss_p2
I1028 02:50:55.914841 63928 layer_factory.hpp:77] Creating layer loss_p2
I1028 02:50:55.915879 63928 net.cpp:141] Setting up loss_p2
I1028 02:50:55.915904 63928 net.cpp:148] Top shape: (1)
I1028 02:50:55.915909 63928 net.cpp:151]     with loss weight 0.65
I1028 02:50:55.915915 63928 net.cpp:156] Memory required for data: 4424607752
I1028 02:50:55.915918 63928 net.cpp:217] loss_p2 needs backward computation.
I1028 02:50:55.915921 63928 net.cpp:217] loss_p1 needs backward computation.
I1028 02:50:55.915925 63928 net.cpp:217] fc7_pn needs backward computation.
I1028 02:50:55.915927 63928 net.cpp:217] fc8_p needs backward computation.
I1028 02:50:55.915930 63928 net.cpp:217] fc7_p_drop7_p_0_split needs backward computation.
I1028 02:50:55.915933 63928 net.cpp:217] drop7_p needs backward computation.
I1028 02:50:55.915936 63928 net.cpp:217] relu7_p needs backward computation.
I1028 02:50:55.915940 63928 net.cpp:217] fc7_p needs backward computation.
I1028 02:50:55.915941 63928 net.cpp:217] drop6_p needs backward computation.
I1028 02:50:55.915944 63928 net.cpp:217] relu6_p needs backward computation.
I1028 02:50:55.915947 63928 net.cpp:217] fc6_p needs backward computation.
I1028 02:50:55.915951 63928 net.cpp:217] pool5_p needs backward computation.
I1028 02:50:55.915953 63928 net.cpp:217] relu5_p needs backward computation.
I1028 02:50:55.915956 63928 net.cpp:217] conv5_p needs backward computation.
I1028 02:50:55.915958 63928 net.cpp:217] relu4_p needs backward computation.
I1028 02:50:55.915961 63928 net.cpp:217] conv4_p needs backward computation.
I1028 02:50:55.915964 63928 net.cpp:217] relu3_p needs backward computation.
I1028 02:50:55.915966 63928 net.cpp:217] conv3_p needs backward computation.
I1028 02:50:55.915969 63928 net.cpp:217] pool2_p needs backward computation.
I1028 02:50:55.915972 63928 net.cpp:217] norm2_p needs backward computation.
I1028 02:50:55.915976 63928 net.cpp:217] relu2_p needs backward computation.
I1028 02:50:55.915978 63928 net.cpp:217] conv2_p needs backward computation.
I1028 02:50:55.915982 63928 net.cpp:217] pool1_p needs backward computation.
I1028 02:50:55.915984 63928 net.cpp:217] norm1_p needs backward computation.
I1028 02:50:55.915987 63928 net.cpp:217] relu1_p needs backward computation.
I1028 02:50:55.915990 63928 net.cpp:217] conv1_p needs backward computation.
I1028 02:50:55.915993 63928 net.cpp:219] fc7_n does not need backward computation.
I1028 02:50:55.915998 63928 net.cpp:219] fc7 does not need backward computation.
I1028 02:50:55.916002 63928 net.cpp:219] drop6 does not need backward computation.
I1028 02:50:55.916004 63928 net.cpp:219] relu6 does not need backward computation.
I1028 02:50:55.916007 63928 net.cpp:219] fc6 does not need backward computation.
I1028 02:50:55.916010 63928 net.cpp:219] pool5 does not need backward computation.
I1028 02:50:55.916013 63928 net.cpp:219] relu5 does not need backward computation.
I1028 02:50:55.916016 63928 net.cpp:219] conv5 does not need backward computation.
I1028 02:50:55.916019 63928 net.cpp:219] relu4 does not need backward computation.
I1028 02:50:55.916021 63928 net.cpp:219] conv4 does not need backward computation.
I1028 02:50:55.916024 63928 net.cpp:219] relu3 does not need backward computation.
I1028 02:50:55.916028 63928 net.cpp:219] conv3 does not need backward computation.
I1028 02:50:55.916030 63928 net.cpp:219] pool2 does not need backward computation.
I1028 02:50:55.916046 63928 net.cpp:219] norm2 does not need backward computation.
I1028 02:50:55.916050 63928 net.cpp:219] relu2 does not need backward computation.
I1028 02:50:55.916054 63928 net.cpp:219] conv2 does not need backward computation.
I1028 02:50:55.916057 63928 net.cpp:219] pool1 does not need backward computation.
I1028 02:50:55.916060 63928 net.cpp:219] norm1 does not need backward computation.
I1028 02:50:55.916064 63928 net.cpp:219] relu1 does not need backward computation.
I1028 02:50:55.916066 63928 net.cpp:219] conv1 does not need backward computation.
I1028 02:50:55.916069 63928 net.cpp:219] data_data_0_split does not need backward computation.
I1028 02:50:55.916074 63928 net.cpp:219] data does not need backward computation.
I1028 02:50:55.916075 63928 net.cpp:261] This network produces output loss_p1
I1028 02:50:55.916079 63928 net.cpp:261] This network produces output loss_p2
I1028 02:50:55.916107 63928 net.cpp:274] Network initialization done.
I1028 02:50:55.919164 63928 solver.cpp:181] Creating test net (#0) specified by net file: models/dissimilarity_siamese_net/train_val.prototxt
I1028 02:50:55.919248 63928 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1028 02:50:55.920337 63928 net.cpp:49] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "examples/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/work/03713/harshal1/maverick/vision_proj/data/val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "fc7_n"
  type: "L2Norm"
  bottom: "fc7"
  top: "fc7_n"
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data"
  top: "conv1_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "conv1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "norm1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "conv2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "norm2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "pool2_p"
  top: "conv3_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "pool5_p"
  type: "Pooling"
  bottom: "conv5_p"
  top: "pool5_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "pool5_p"
  top: "fc6_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7_p"
  type: "ReLU"
  bottom: "fc7_p"
  top: "fc7_p"
}
layer {
  name: "drop7_p"
  type: "Dropout"
  bottom: "fc7_p"
  top: "fc7_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 504
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "precision@1"
  type: "Accuracy"
  bottom: "fc8_p"
  bottom: "label"
  top: "precision@1"
  include {
    phase: TEST
  }
}
layer {
  name: "precision@5"
  type: "Accuracy"
  bottom: "fc8_p"
  bottom: "label"
  top: "precision@5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "fc7_pn"
  type: "L2Norm"
  bottom: "fc7_p"
  top: "fc7_pn"
}
layer {
  name: "loss_p1"
  type: "EuclideanLoss"
  bottom: "fc7_n"
  bottom: "fc7_pn"
  top: "loss_p1"
  loss_weight: -0.35
}
layer {
  name: "loss_p2"
  type: "SoftmaxWithLoss"
  bottom: "fc8_p"
  bottom: "label"
  top: "loss_p2"
  loss_weight: 0.65
}
I1028 02:50:55.920547 63928 layer_factory.hpp:77] Creating layer data
I1028 02:50:55.920713 63928 net.cpp:91] Creating Layer data
I1028 02:50:55.920722 63928 net.cpp:399] data -> data
I1028 02:50:55.920732 63928 net.cpp:399] data -> label
I1028 02:50:55.920752 63928 data_transformer.cpp:25] Loading mean file from: examples/imagenet/imagenet_mean.binaryproto
I1028 02:50:55.931598 64041 db_lmdb.cpp:35] Opened lmdb /work/03713/harshal1/maverick/vision_proj/data/val_lmdb
I1028 02:50:55.932837 63928 data_layer.cpp:41] output data size: 50,3,227,227
I1028 02:50:55.988389 63928 net.cpp:141] Setting up data
I1028 02:50:55.988426 63928 net.cpp:148] Top shape: 50 3 227 227 (7729350)
I1028 02:50:55.988432 63928 net.cpp:148] Top shape: 50 (50)
I1028 02:50:55.988446 63928 net.cpp:156] Memory required for data: 30917600
I1028 02:50:55.988456 63928 layer_factory.hpp:77] Creating layer data_data_0_split
I1028 02:50:55.988477 63928 net.cpp:91] Creating Layer data_data_0_split
I1028 02:50:55.988482 63928 net.cpp:425] data_data_0_split <- data
I1028 02:50:55.988492 63928 net.cpp:399] data_data_0_split -> data_data_0_split_0
I1028 02:50:55.988507 63928 net.cpp:399] data_data_0_split -> data_data_0_split_1
I1028 02:50:55.988565 63928 net.cpp:141] Setting up data_data_0_split
I1028 02:50:55.988572 63928 net.cpp:148] Top shape: 50 3 227 227 (7729350)
I1028 02:50:55.988615 63928 net.cpp:148] Top shape: 50 3 227 227 (7729350)
I1028 02:50:55.988618 63928 net.cpp:156] Memory required for data: 92752400
I1028 02:50:55.988622 63928 layer_factory.hpp:77] Creating layer label_data_1_split
I1028 02:50:55.988629 63928 net.cpp:91] Creating Layer label_data_1_split
I1028 02:50:55.988632 63928 net.cpp:425] label_data_1_split <- label
I1028 02:50:55.988638 63928 net.cpp:399] label_data_1_split -> label_data_1_split_0
I1028 02:50:55.988646 63928 net.cpp:399] label_data_1_split -> label_data_1_split_1
I1028 02:50:55.988651 63928 net.cpp:399] label_data_1_split -> label_data_1_split_2
I1028 02:50:55.988708 63928 net.cpp:141] Setting up label_data_1_split
I1028 02:50:55.988718 63928 net.cpp:148] Top shape: 50 (50)
I1028 02:50:55.988720 63928 net.cpp:148] Top shape: 50 (50)
I1028 02:50:55.988724 63928 net.cpp:148] Top shape: 50 (50)
I1028 02:50:55.988726 63928 net.cpp:156] Memory required for data: 92753000
I1028 02:50:55.988730 63928 layer_factory.hpp:77] Creating layer conv1
I1028 02:50:55.988749 63928 net.cpp:91] Creating Layer conv1
I1028 02:50:55.988752 63928 net.cpp:425] conv1 <- data_data_0_split_0
I1028 02:50:55.988759 63928 net.cpp:399] conv1 -> conv1
I1028 02:50:55.993218 63928 net.cpp:141] Setting up conv1
I1028 02:50:55.993244 63928 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 02:50:55.993248 63928 net.cpp:156] Memory required for data: 150833000
I1028 02:50:55.993261 63928 layer_factory.hpp:77] Creating layer relu1
I1028 02:50:55.993271 63928 net.cpp:91] Creating Layer relu1
I1028 02:50:55.993275 63928 net.cpp:425] relu1 <- conv1
I1028 02:50:55.993280 63928 net.cpp:386] relu1 -> conv1 (in-place)
I1028 02:50:55.993542 63928 net.cpp:141] Setting up relu1
I1028 02:50:55.993553 63928 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 02:50:55.993557 63928 net.cpp:156] Memory required for data: 208913000
I1028 02:50:55.993561 63928 layer_factory.hpp:77] Creating layer norm1
I1028 02:50:55.993571 63928 net.cpp:91] Creating Layer norm1
I1028 02:50:55.993574 63928 net.cpp:425] norm1 <- conv1
I1028 02:50:55.993579 63928 net.cpp:399] norm1 -> norm1
I1028 02:50:55.993868 63928 net.cpp:141] Setting up norm1
I1028 02:50:55.993896 63928 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 02:50:55.993899 63928 net.cpp:156] Memory required for data: 266993000
I1028 02:50:55.993903 63928 layer_factory.hpp:77] Creating layer pool1
I1028 02:50:55.993911 63928 net.cpp:91] Creating Layer pool1
I1028 02:50:55.993916 63928 net.cpp:425] pool1 <- norm1
I1028 02:50:55.993921 63928 net.cpp:399] pool1 -> pool1
I1028 02:50:55.993974 63928 net.cpp:141] Setting up pool1
I1028 02:50:55.993981 63928 net.cpp:148] Top shape: 50 96 27 27 (3499200)
I1028 02:50:55.993984 63928 net.cpp:156] Memory required for data: 280989800
I1028 02:50:55.993988 63928 layer_factory.hpp:77] Creating layer conv2
I1028 02:50:55.993999 63928 net.cpp:91] Creating Layer conv2
I1028 02:50:55.994014 63928 net.cpp:425] conv2 <- pool1
I1028 02:50:55.994021 63928 net.cpp:399] conv2 -> conv2
I1028 02:50:56.000082 63928 net.cpp:141] Setting up conv2
I1028 02:50:56.000108 63928 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 02:50:56.000111 63928 net.cpp:156] Memory required for data: 318314600
I1028 02:50:56.000120 63928 layer_factory.hpp:77] Creating layer relu2
I1028 02:50:56.000131 63928 net.cpp:91] Creating Layer relu2
I1028 02:50:56.000135 63928 net.cpp:425] relu2 <- conv2
I1028 02:50:56.000141 63928 net.cpp:386] relu2 -> conv2 (in-place)
I1028 02:50:56.000407 63928 net.cpp:141] Setting up relu2
I1028 02:50:56.000418 63928 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 02:50:56.000422 63928 net.cpp:156] Memory required for data: 355639400
I1028 02:50:56.000425 63928 layer_factory.hpp:77] Creating layer norm2
I1028 02:50:56.000432 63928 net.cpp:91] Creating Layer norm2
I1028 02:50:56.000437 63928 net.cpp:425] norm2 <- conv2
I1028 02:50:56.000442 63928 net.cpp:399] norm2 -> norm2
I1028 02:50:56.000622 63928 net.cpp:141] Setting up norm2
I1028 02:50:56.000713 63928 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 02:50:56.000732 63928 net.cpp:156] Memory required for data: 392964200
I1028 02:50:56.000737 63928 layer_factory.hpp:77] Creating layer pool2
I1028 02:50:56.000744 63928 net.cpp:91] Creating Layer pool2
I1028 02:50:56.000748 63928 net.cpp:425] pool2 <- norm2
I1028 02:50:56.000754 63928 net.cpp:399] pool2 -> pool2
I1028 02:50:56.000798 63928 net.cpp:141] Setting up pool2
I1028 02:50:56.000807 63928 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 02:50:56.000809 63928 net.cpp:156] Memory required for data: 401617000
I1028 02:50:56.000813 63928 layer_factory.hpp:77] Creating layer conv3
I1028 02:50:56.000823 63928 net.cpp:91] Creating Layer conv3
I1028 02:50:56.000828 63928 net.cpp:425] conv3 <- pool2
I1028 02:50:56.000834 63928 net.cpp:399] conv3 -> conv3
I1028 02:50:56.015583 63928 net.cpp:141] Setting up conv3
I1028 02:50:56.015622 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.015627 63928 net.cpp:156] Memory required for data: 414596200
I1028 02:50:56.015643 63928 layer_factory.hpp:77] Creating layer relu3
I1028 02:50:56.015658 63928 net.cpp:91] Creating Layer relu3
I1028 02:50:56.015663 63928 net.cpp:425] relu3 <- conv3
I1028 02:50:56.015672 63928 net.cpp:386] relu3 -> conv3 (in-place)
I1028 02:50:56.015967 63928 net.cpp:141] Setting up relu3
I1028 02:50:56.015980 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.015983 63928 net.cpp:156] Memory required for data: 427575400
I1028 02:50:56.015995 63928 layer_factory.hpp:77] Creating layer conv4
I1028 02:50:56.016011 63928 net.cpp:91] Creating Layer conv4
I1028 02:50:56.016016 63928 net.cpp:425] conv4 <- conv3
I1028 02:50:56.016023 63928 net.cpp:399] conv4 -> conv4
I1028 02:50:56.027354 63928 net.cpp:141] Setting up conv4
I1028 02:50:56.027369 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.027384 63928 net.cpp:156] Memory required for data: 440554600
I1028 02:50:56.027391 63928 layer_factory.hpp:77] Creating layer relu4
I1028 02:50:56.027400 63928 net.cpp:91] Creating Layer relu4
I1028 02:50:56.027405 63928 net.cpp:425] relu4 <- conv4
I1028 02:50:56.027411 63928 net.cpp:386] relu4 -> conv4 (in-place)
I1028 02:50:56.027671 63928 net.cpp:141] Setting up relu4
I1028 02:50:56.027683 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.027686 63928 net.cpp:156] Memory required for data: 453533800
I1028 02:50:56.027689 63928 layer_factory.hpp:77] Creating layer conv5
I1028 02:50:56.027701 63928 net.cpp:91] Creating Layer conv5
I1028 02:50:56.027705 63928 net.cpp:425] conv5 <- conv4
I1028 02:50:56.027712 63928 net.cpp:399] conv5 -> conv5
I1028 02:50:56.035776 63928 net.cpp:141] Setting up conv5
I1028 02:50:56.035804 63928 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 02:50:56.035807 63928 net.cpp:156] Memory required for data: 462186600
I1028 02:50:56.035818 63928 layer_factory.hpp:77] Creating layer relu5
I1028 02:50:56.035830 63928 net.cpp:91] Creating Layer relu5
I1028 02:50:56.035835 63928 net.cpp:425] relu5 <- conv5
I1028 02:50:56.035840 63928 net.cpp:386] relu5 -> conv5 (in-place)
I1028 02:50:56.036134 63928 net.cpp:141] Setting up relu5
I1028 02:50:56.036146 63928 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 02:50:56.036149 63928 net.cpp:156] Memory required for data: 470839400
I1028 02:50:56.036154 63928 layer_factory.hpp:77] Creating layer pool5
I1028 02:50:56.036164 63928 net.cpp:91] Creating Layer pool5
I1028 02:50:56.036166 63928 net.cpp:425] pool5 <- conv5
I1028 02:50:56.036173 63928 net.cpp:399] pool5 -> pool5
I1028 02:50:56.036234 63928 net.cpp:141] Setting up pool5
I1028 02:50:56.036243 63928 net.cpp:148] Top shape: 50 256 6 6 (460800)
I1028 02:50:56.036247 63928 net.cpp:156] Memory required for data: 472682600
I1028 02:50:56.036250 63928 layer_factory.hpp:77] Creating layer fc6
I1028 02:50:56.036262 63928 net.cpp:91] Creating Layer fc6
I1028 02:50:56.036265 63928 net.cpp:425] fc6 <- pool5
I1028 02:50:56.036272 63928 net.cpp:399] fc6 -> fc6
I1028 02:50:56.579406 63928 net.cpp:141] Setting up fc6
I1028 02:50:56.579463 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:56.579507 63928 net.cpp:156] Memory required for data: 473501800
I1028 02:50:56.579521 63928 layer_factory.hpp:77] Creating layer relu6
I1028 02:50:56.579541 63928 net.cpp:91] Creating Layer relu6
I1028 02:50:56.579548 63928 net.cpp:425] relu6 <- fc6
I1028 02:50:56.579558 63928 net.cpp:386] relu6 -> fc6 (in-place)
I1028 02:50:56.579802 63928 net.cpp:141] Setting up relu6
I1028 02:50:56.579813 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:56.579815 63928 net.cpp:156] Memory required for data: 474321000
I1028 02:50:56.579818 63928 layer_factory.hpp:77] Creating layer drop6
I1028 02:50:56.579829 63928 net.cpp:91] Creating Layer drop6
I1028 02:50:56.579833 63928 net.cpp:425] drop6 <- fc6
I1028 02:50:56.579838 63928 net.cpp:386] drop6 -> fc6 (in-place)
I1028 02:50:56.579870 63928 net.cpp:141] Setting up drop6
I1028 02:50:56.579877 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:56.579880 63928 net.cpp:156] Memory required for data: 475140200
I1028 02:50:56.579884 63928 layer_factory.hpp:77] Creating layer fc7
I1028 02:50:56.579900 63928 net.cpp:91] Creating Layer fc7
I1028 02:50:56.579903 63928 net.cpp:425] fc7 <- fc6
I1028 02:50:56.579910 63928 net.cpp:399] fc7 -> fc7
I1028 02:50:56.816154 63928 net.cpp:141] Setting up fc7
I1028 02:50:56.816208 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:56.816212 63928 net.cpp:156] Memory required for data: 475959400
I1028 02:50:56.816226 63928 layer_factory.hpp:77] Creating layer fc7_n
I1028 02:50:56.816246 63928 net.cpp:91] Creating Layer fc7_n
I1028 02:50:56.816251 63928 net.cpp:425] fc7_n <- fc7
I1028 02:50:56.816263 63928 net.cpp:399] fc7_n -> fc7_n
I1028 02:50:56.816304 63928 net.cpp:141] Setting up fc7_n
I1028 02:50:56.816310 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:56.816313 63928 net.cpp:156] Memory required for data: 476778600
I1028 02:50:56.816316 63928 layer_factory.hpp:77] Creating layer conv1_p
I1028 02:50:56.816336 63928 net.cpp:91] Creating Layer conv1_p
I1028 02:50:56.816339 63928 net.cpp:425] conv1_p <- data_data_0_split_1
I1028 02:50:56.816347 63928 net.cpp:399] conv1_p -> conv1_p
I1028 02:50:56.818027 63928 net.cpp:141] Setting up conv1_p
I1028 02:50:56.818038 63928 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 02:50:56.818053 63928 net.cpp:156] Memory required for data: 534858600
I1028 02:50:56.818060 63928 layer_factory.hpp:77] Creating layer relu1_p
I1028 02:50:56.818070 63928 net.cpp:91] Creating Layer relu1_p
I1028 02:50:56.818073 63928 net.cpp:425] relu1_p <- conv1_p
I1028 02:50:56.818079 63928 net.cpp:386] relu1_p -> conv1_p (in-place)
I1028 02:50:56.818343 63928 net.cpp:141] Setting up relu1_p
I1028 02:50:56.818354 63928 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 02:50:56.818357 63928 net.cpp:156] Memory required for data: 592938600
I1028 02:50:56.818361 63928 layer_factory.hpp:77] Creating layer norm1_p
I1028 02:50:56.818372 63928 net.cpp:91] Creating Layer norm1_p
I1028 02:50:56.818375 63928 net.cpp:425] norm1_p <- conv1_p
I1028 02:50:56.818382 63928 net.cpp:399] norm1_p -> norm1_p
I1028 02:50:56.818559 63928 net.cpp:141] Setting up norm1_p
I1028 02:50:56.818569 63928 net.cpp:148] Top shape: 50 96 55 55 (14520000)
I1028 02:50:56.818572 63928 net.cpp:156] Memory required for data: 651018600
I1028 02:50:56.818577 63928 layer_factory.hpp:77] Creating layer pool1_p
I1028 02:50:56.818584 63928 net.cpp:91] Creating Layer pool1_p
I1028 02:50:56.818588 63928 net.cpp:425] pool1_p <- norm1_p
I1028 02:50:56.818594 63928 net.cpp:399] pool1_p -> pool1_p
I1028 02:50:56.818637 63928 net.cpp:141] Setting up pool1_p
I1028 02:50:56.818645 63928 net.cpp:148] Top shape: 50 96 27 27 (3499200)
I1028 02:50:56.818647 63928 net.cpp:156] Memory required for data: 665015400
I1028 02:50:56.818650 63928 layer_factory.hpp:77] Creating layer conv2_p
I1028 02:50:56.818661 63928 net.cpp:91] Creating Layer conv2_p
I1028 02:50:56.818665 63928 net.cpp:425] conv2_p <- pool1_p
I1028 02:50:56.818671 63928 net.cpp:399] conv2_p -> conv2_p
I1028 02:50:56.824584 63928 net.cpp:141] Setting up conv2_p
I1028 02:50:56.824626 63928 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 02:50:56.824641 63928 net.cpp:156] Memory required for data: 702340200
I1028 02:50:56.824656 63928 layer_factory.hpp:77] Creating layer relu2_p
I1028 02:50:56.824663 63928 net.cpp:91] Creating Layer relu2_p
I1028 02:50:56.824667 63928 net.cpp:425] relu2_p <- conv2_p
I1028 02:50:56.824673 63928 net.cpp:386] relu2_p -> conv2_p (in-place)
I1028 02:50:56.824828 63928 net.cpp:141] Setting up relu2_p
I1028 02:50:56.824838 63928 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 02:50:56.824841 63928 net.cpp:156] Memory required for data: 739665000
I1028 02:50:56.824844 63928 layer_factory.hpp:77] Creating layer norm2_p
I1028 02:50:56.824851 63928 net.cpp:91] Creating Layer norm2_p
I1028 02:50:56.824856 63928 net.cpp:425] norm2_p <- conv2_p
I1028 02:50:56.824861 63928 net.cpp:399] norm2_p -> norm2_p
I1028 02:50:56.825192 63928 net.cpp:141] Setting up norm2_p
I1028 02:50:56.825203 63928 net.cpp:148] Top shape: 50 256 27 27 (9331200)
I1028 02:50:56.825206 63928 net.cpp:156] Memory required for data: 776989800
I1028 02:50:56.825209 63928 layer_factory.hpp:77] Creating layer pool2_p
I1028 02:50:56.825217 63928 net.cpp:91] Creating Layer pool2_p
I1028 02:50:56.825219 63928 net.cpp:425] pool2_p <- norm2_p
I1028 02:50:56.825227 63928 net.cpp:399] pool2_p -> pool2_p
I1028 02:50:56.825280 63928 net.cpp:141] Setting up pool2_p
I1028 02:50:56.825286 63928 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 02:50:56.825289 63928 net.cpp:156] Memory required for data: 785642600
I1028 02:50:56.825292 63928 layer_factory.hpp:77] Creating layer conv3_p
I1028 02:50:56.825304 63928 net.cpp:91] Creating Layer conv3_p
I1028 02:50:56.825309 63928 net.cpp:425] conv3_p <- pool2_p
I1028 02:50:56.825314 63928 net.cpp:399] conv3_p -> conv3_p
I1028 02:50:56.838892 63928 net.cpp:141] Setting up conv3_p
I1028 02:50:56.838903 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.838918 63928 net.cpp:156] Memory required for data: 798621800
I1028 02:50:56.838924 63928 layer_factory.hpp:77] Creating layer relu3_p
I1028 02:50:56.838937 63928 net.cpp:91] Creating Layer relu3_p
I1028 02:50:56.838942 63928 net.cpp:425] relu3_p <- conv3_p
I1028 02:50:56.838946 63928 net.cpp:386] relu3_p -> conv3_p (in-place)
I1028 02:50:56.839098 63928 net.cpp:141] Setting up relu3_p
I1028 02:50:56.839107 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.839110 63928 net.cpp:156] Memory required for data: 811601000
I1028 02:50:56.839114 63928 layer_factory.hpp:77] Creating layer conv4_p
I1028 02:50:56.839125 63928 net.cpp:91] Creating Layer conv4_p
I1028 02:50:56.839129 63928 net.cpp:425] conv4_p <- conv3_p
I1028 02:50:56.839138 63928 net.cpp:399] conv4_p -> conv4_p
I1028 02:50:56.850005 63928 net.cpp:141] Setting up conv4_p
I1028 02:50:56.850018 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.850033 63928 net.cpp:156] Memory required for data: 824580200
I1028 02:50:56.850039 63928 layer_factory.hpp:77] Creating layer relu4_p
I1028 02:50:56.850045 63928 net.cpp:91] Creating Layer relu4_p
I1028 02:50:56.850049 63928 net.cpp:425] relu4_p <- conv4_p
I1028 02:50:56.850054 63928 net.cpp:386] relu4_p -> conv4_p (in-place)
I1028 02:50:56.850208 63928 net.cpp:141] Setting up relu4_p
I1028 02:50:56.850217 63928 net.cpp:148] Top shape: 50 384 13 13 (3244800)
I1028 02:50:56.850220 63928 net.cpp:156] Memory required for data: 837559400
I1028 02:50:56.850224 63928 layer_factory.hpp:77] Creating layer conv5_p
I1028 02:50:56.850235 63928 net.cpp:91] Creating Layer conv5_p
I1028 02:50:56.850239 63928 net.cpp:425] conv5_p <- conv4_p
I1028 02:50:56.850246 63928 net.cpp:399] conv5_p -> conv5_p
I1028 02:50:56.858055 63928 net.cpp:141] Setting up conv5_p
I1028 02:50:56.858067 63928 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 02:50:56.858081 63928 net.cpp:156] Memory required for data: 846212200
I1028 02:50:56.858088 63928 layer_factory.hpp:77] Creating layer relu5_p
I1028 02:50:56.858093 63928 net.cpp:91] Creating Layer relu5_p
I1028 02:50:56.858110 63928 net.cpp:425] relu5_p <- conv5_p
I1028 02:50:56.858116 63928 net.cpp:386] relu5_p -> conv5_p (in-place)
I1028 02:50:56.858270 63928 net.cpp:141] Setting up relu5_p
I1028 02:50:56.858279 63928 net.cpp:148] Top shape: 50 256 13 13 (2163200)
I1028 02:50:56.858283 63928 net.cpp:156] Memory required for data: 854865000
I1028 02:50:56.858285 63928 layer_factory.hpp:77] Creating layer pool5_p
I1028 02:50:56.858294 63928 net.cpp:91] Creating Layer pool5_p
I1028 02:50:56.858297 63928 net.cpp:425] pool5_p <- conv5_p
I1028 02:50:56.858302 63928 net.cpp:399] pool5_p -> pool5_p
I1028 02:50:56.858350 63928 net.cpp:141] Setting up pool5_p
I1028 02:50:56.858357 63928 net.cpp:148] Top shape: 50 256 6 6 (460800)
I1028 02:50:56.858361 63928 net.cpp:156] Memory required for data: 856708200
I1028 02:50:56.858363 63928 layer_factory.hpp:77] Creating layer fc6_p
I1028 02:50:56.858373 63928 net.cpp:91] Creating Layer fc6_p
I1028 02:50:56.858376 63928 net.cpp:425] fc6_p <- pool5_p
I1028 02:50:56.858383 63928 net.cpp:399] fc6_p -> fc6_p
I1028 02:50:57.390120 63928 net.cpp:141] Setting up fc6_p
I1028 02:50:57.390174 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.390179 63928 net.cpp:156] Memory required for data: 857527400
I1028 02:50:57.390192 63928 layer_factory.hpp:77] Creating layer relu6_p
I1028 02:50:57.390213 63928 net.cpp:91] Creating Layer relu6_p
I1028 02:50:57.390218 63928 net.cpp:425] relu6_p <- fc6_p
I1028 02:50:57.390229 63928 net.cpp:386] relu6_p -> fc6_p (in-place)
I1028 02:50:57.390729 63928 net.cpp:141] Setting up relu6_p
I1028 02:50:57.390739 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.390753 63928 net.cpp:156] Memory required for data: 858346600
I1028 02:50:57.390758 63928 layer_factory.hpp:77] Creating layer drop6_p
I1028 02:50:57.390769 63928 net.cpp:91] Creating Layer drop6_p
I1028 02:50:57.390771 63928 net.cpp:425] drop6_p <- fc6_p
I1028 02:50:57.390779 63928 net.cpp:386] drop6_p -> fc6_p (in-place)
I1028 02:50:57.390812 63928 net.cpp:141] Setting up drop6_p
I1028 02:50:57.390818 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.390823 63928 net.cpp:156] Memory required for data: 859165800
I1028 02:50:57.390826 63928 layer_factory.hpp:77] Creating layer fc7_p
I1028 02:50:57.390838 63928 net.cpp:91] Creating Layer fc7_p
I1028 02:50:57.390842 63928 net.cpp:425] fc7_p <- fc6_p
I1028 02:50:57.390849 63928 net.cpp:399] fc7_p -> fc7_p
I1028 02:50:57.627374 63928 net.cpp:141] Setting up fc7_p
I1028 02:50:57.627430 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.627434 63928 net.cpp:156] Memory required for data: 859985000
I1028 02:50:57.627447 63928 layer_factory.hpp:77] Creating layer relu7_p
I1028 02:50:57.627470 63928 net.cpp:91] Creating Layer relu7_p
I1028 02:50:57.627475 63928 net.cpp:425] relu7_p <- fc7_p
I1028 02:50:57.627486 63928 net.cpp:386] relu7_p -> fc7_p (in-place)
I1028 02:50:57.628015 63928 net.cpp:141] Setting up relu7_p
I1028 02:50:57.628028 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.628031 63928 net.cpp:156] Memory required for data: 860804200
I1028 02:50:57.628036 63928 layer_factory.hpp:77] Creating layer drop7_p
I1028 02:50:57.628047 63928 net.cpp:91] Creating Layer drop7_p
I1028 02:50:57.628051 63928 net.cpp:425] drop7_p <- fc7_p
I1028 02:50:57.628060 63928 net.cpp:386] drop7_p -> fc7_p (in-place)
I1028 02:50:57.628095 63928 net.cpp:141] Setting up drop7_p
I1028 02:50:57.628103 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.628106 63928 net.cpp:156] Memory required for data: 861623400
I1028 02:50:57.628109 63928 layer_factory.hpp:77] Creating layer fc7_p_drop7_p_0_split
I1028 02:50:57.628116 63928 net.cpp:91] Creating Layer fc7_p_drop7_p_0_split
I1028 02:50:57.628119 63928 net.cpp:425] fc7_p_drop7_p_0_split <- fc7_p
I1028 02:50:57.628129 63928 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_0
I1028 02:50:57.628144 63928 net.cpp:399] fc7_p_drop7_p_0_split -> fc7_p_drop7_p_0_split_1
I1028 02:50:57.628187 63928 net.cpp:141] Setting up fc7_p_drop7_p_0_split
I1028 02:50:57.628196 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.628235 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.628238 63928 net.cpp:156] Memory required for data: 863261800
I1028 02:50:57.628242 63928 layer_factory.hpp:77] Creating layer fc8_p
I1028 02:50:57.628257 63928 net.cpp:91] Creating Layer fc8_p
I1028 02:50:57.628260 63928 net.cpp:425] fc8_p <- fc7_p_drop7_p_0_split_0
I1028 02:50:57.628265 63928 net.cpp:399] fc8_p -> fc8_p
I1028 02:50:57.657160 63928 net.cpp:141] Setting up fc8_p
I1028 02:50:57.657169 63928 net.cpp:148] Top shape: 50 504 (25200)
I1028 02:50:57.657186 63928 net.cpp:156] Memory required for data: 863362600
I1028 02:50:57.657191 63928 layer_factory.hpp:77] Creating layer fc8_p_fc8_p_0_split
I1028 02:50:57.657199 63928 net.cpp:91] Creating Layer fc8_p_fc8_p_0_split
I1028 02:50:57.657203 63928 net.cpp:425] fc8_p_fc8_p_0_split <- fc8_p
I1028 02:50:57.657208 63928 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_0
I1028 02:50:57.657215 63928 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_1
I1028 02:50:57.657220 63928 net.cpp:399] fc8_p_fc8_p_0_split -> fc8_p_fc8_p_0_split_2
I1028 02:50:57.657271 63928 net.cpp:141] Setting up fc8_p_fc8_p_0_split
I1028 02:50:57.657279 63928 net.cpp:148] Top shape: 50 504 (25200)
I1028 02:50:57.657282 63928 net.cpp:148] Top shape: 50 504 (25200)
I1028 02:50:57.657285 63928 net.cpp:148] Top shape: 50 504 (25200)
I1028 02:50:57.657289 63928 net.cpp:156] Memory required for data: 863665000
I1028 02:50:57.657291 63928 layer_factory.hpp:77] Creating layer precision@1
I1028 02:50:57.657356 63928 net.cpp:91] Creating Layer precision@1
I1028 02:50:57.657362 63928 net.cpp:425] precision@1 <- fc8_p_fc8_p_0_split_0
I1028 02:50:57.657368 63928 net.cpp:425] precision@1 <- label_data_1_split_0
I1028 02:50:57.657373 63928 net.cpp:399] precision@1 -> precision@1
I1028 02:50:57.657428 63928 net.cpp:141] Setting up precision@1
I1028 02:50:57.657435 63928 net.cpp:148] Top shape: (1)
I1028 02:50:57.657438 63928 net.cpp:156] Memory required for data: 863665004
I1028 02:50:57.657441 63928 layer_factory.hpp:77] Creating layer precision@5
I1028 02:50:57.657449 63928 net.cpp:91] Creating Layer precision@5
I1028 02:50:57.657452 63928 net.cpp:425] precision@5 <- fc8_p_fc8_p_0_split_1
I1028 02:50:57.657456 63928 net.cpp:425] precision@5 <- label_data_1_split_1
I1028 02:50:57.657461 63928 net.cpp:399] precision@5 -> precision@5
I1028 02:50:57.657469 63928 net.cpp:141] Setting up precision@5
I1028 02:50:57.657472 63928 net.cpp:148] Top shape: (1)
I1028 02:50:57.657475 63928 net.cpp:156] Memory required for data: 863665008
I1028 02:50:57.657479 63928 layer_factory.hpp:77] Creating layer fc7_pn
I1028 02:50:57.657486 63928 net.cpp:91] Creating Layer fc7_pn
I1028 02:50:57.657490 63928 net.cpp:425] fc7_pn <- fc7_p_drop7_p_0_split_1
I1028 02:50:57.657495 63928 net.cpp:399] fc7_pn -> fc7_pn
I1028 02:50:57.657521 63928 net.cpp:141] Setting up fc7_pn
I1028 02:50:57.657528 63928 net.cpp:148] Top shape: 50 4096 (204800)
I1028 02:50:57.657531 63928 net.cpp:156] Memory required for data: 864484208
I1028 02:50:57.657534 63928 layer_factory.hpp:77] Creating layer loss_p1
I1028 02:50:57.657541 63928 net.cpp:91] Creating Layer loss_p1
I1028 02:50:57.657544 63928 net.cpp:425] loss_p1 <- fc7_n
I1028 02:50:57.657548 63928 net.cpp:425] loss_p1 <- fc7_pn
I1028 02:50:57.657552 63928 net.cpp:399] loss_p1 -> loss_p1
I1028 02:50:57.657598 63928 net.cpp:141] Setting up loss_p1
I1028 02:50:57.657605 63928 net.cpp:148] Top shape: (1)
I1028 02:50:57.657608 63928 net.cpp:151]     with loss weight -0.35
I1028 02:50:57.657621 63928 net.cpp:156] Memory required for data: 864484212
I1028 02:50:57.657624 63928 layer_factory.hpp:77] Creating layer loss_p2
I1028 02:50:57.657634 63928 net.cpp:91] Creating Layer loss_p2
I1028 02:50:57.657637 63928 net.cpp:425] loss_p2 <- fc8_p_fc8_p_0_split_2
I1028 02:50:57.657641 63928 net.cpp:425] loss_p2 <- label_data_1_split_2
I1028 02:50:57.657645 63928 net.cpp:399] loss_p2 -> loss_p2
I1028 02:50:57.657654 63928 layer_factory.hpp:77] Creating layer loss_p2
I1028 02:50:57.657959 63928 net.cpp:141] Setting up loss_p2
I1028 02:50:57.657985 63928 net.cpp:148] Top shape: (1)
I1028 02:50:57.657989 63928 net.cpp:151]     with loss weight 0.65
I1028 02:50:57.657994 63928 net.cpp:156] Memory required for data: 864484216
I1028 02:50:57.657997 63928 net.cpp:217] loss_p2 needs backward computation.
I1028 02:50:57.658001 63928 net.cpp:217] loss_p1 needs backward computation.
I1028 02:50:57.658004 63928 net.cpp:217] fc7_pn needs backward computation.
I1028 02:50:57.658007 63928 net.cpp:219] precision@5 does not need backward computation.
I1028 02:50:57.658011 63928 net.cpp:219] precision@1 does not need backward computation.
I1028 02:50:57.658015 63928 net.cpp:217] fc8_p_fc8_p_0_split needs backward computation.
I1028 02:50:57.658017 63928 net.cpp:217] fc8_p needs backward computation.
I1028 02:50:57.658020 63928 net.cpp:217] fc7_p_drop7_p_0_split needs backward computation.
I1028 02:50:57.658022 63928 net.cpp:217] drop7_p needs backward computation.
I1028 02:50:57.658025 63928 net.cpp:217] relu7_p needs backward computation.
I1028 02:50:57.658028 63928 net.cpp:217] fc7_p needs backward computation.
I1028 02:50:57.658030 63928 net.cpp:217] drop6_p needs backward computation.
I1028 02:50:57.658033 63928 net.cpp:217] relu6_p needs backward computation.
I1028 02:50:57.658036 63928 net.cpp:217] fc6_p needs backward computation.
I1028 02:50:57.658040 63928 net.cpp:217] pool5_p needs backward computation.
I1028 02:50:57.658041 63928 net.cpp:217] relu5_p needs backward computation.
I1028 02:50:57.658044 63928 net.cpp:217] conv5_p needs backward computation.
I1028 02:50:57.658047 63928 net.cpp:217] relu4_p needs backward computation.
I1028 02:50:57.658051 63928 net.cpp:217] conv4_p needs backward computation.
I1028 02:50:57.658052 63928 net.cpp:217] relu3_p needs backward computation.
I1028 02:50:57.658056 63928 net.cpp:217] conv3_p needs backward computation.
I1028 02:50:57.658058 63928 net.cpp:217] pool2_p needs backward computation.
I1028 02:50:57.658061 63928 net.cpp:217] norm2_p needs backward computation.
I1028 02:50:57.658063 63928 net.cpp:217] relu2_p needs backward computation.
I1028 02:50:57.658066 63928 net.cpp:217] conv2_p needs backward computation.
I1028 02:50:57.658069 63928 net.cpp:217] pool1_p needs backward computation.
I1028 02:50:57.658072 63928 net.cpp:217] norm1_p needs backward computation.
I1028 02:50:57.658076 63928 net.cpp:217] relu1_p needs backward computation.
I1028 02:50:57.658077 63928 net.cpp:217] conv1_p needs backward computation.
I1028 02:50:57.658080 63928 net.cpp:219] fc7_n does not need backward computation.
I1028 02:50:57.658084 63928 net.cpp:219] fc7 does not need backward computation.
I1028 02:50:57.658087 63928 net.cpp:219] drop6 does not need backward computation.
I1028 02:50:57.658090 63928 net.cpp:219] relu6 does not need backward computation.
I1028 02:50:57.658092 63928 net.cpp:219] fc6 does not need backward computation.
I1028 02:50:57.658097 63928 net.cpp:219] pool5 does not need backward computation.
I1028 02:50:57.658099 63928 net.cpp:219] relu5 does not need backward computation.
I1028 02:50:57.658102 63928 net.cpp:219] conv5 does not need backward computation.
I1028 02:50:57.658105 63928 net.cpp:219] relu4 does not need backward computation.
I1028 02:50:57.658107 63928 net.cpp:219] conv4 does not need backward computation.
I1028 02:50:57.658113 63928 net.cpp:219] relu3 does not need backward computation.
I1028 02:50:57.658116 63928 net.cpp:219] conv3 does not need backward computation.
I1028 02:50:57.658119 63928 net.cpp:219] pool2 does not need backward computation.
I1028 02:50:57.658123 63928 net.cpp:219] norm2 does not need backward computation.
I1028 02:50:57.658125 63928 net.cpp:219] relu2 does not need backward computation.
I1028 02:50:57.658128 63928 net.cpp:219] conv2 does not need backward computation.
I1028 02:50:57.658131 63928 net.cpp:219] pool1 does not need backward computation.
I1028 02:50:57.658134 63928 net.cpp:219] norm1 does not need backward computation.
I1028 02:50:57.658138 63928 net.cpp:219] relu1 does not need backward computation.
I1028 02:50:57.658150 63928 net.cpp:219] conv1 does not need backward computation.
I1028 02:50:57.658155 63928 net.cpp:219] label_data_1_split does not need backward computation.
I1028 02:50:57.658159 63928 net.cpp:219] data_data_0_split does not need backward computation.
I1028 02:50:57.658162 63928 net.cpp:219] data does not need backward computation.
I1028 02:50:57.658165 63928 net.cpp:261] This network produces output loss_p1
I1028 02:50:57.658169 63928 net.cpp:261] This network produces output loss_p2
I1028 02:50:57.658171 63928 net.cpp:261] This network produces output precision@1
I1028 02:50:57.658174 63928 net.cpp:261] This network produces output precision@5
I1028 02:50:57.658205 63928 net.cpp:274] Network initialization done.
I1028 02:50:57.658367 63928 solver.cpp:60] Solver scaffolding done.
I1028 02:50:57.659544 63928 caffe.cpp:129] Finetuning from models/dissimilarity_siamese_net/pretrained_model/bvlc_dissimilarity.caffemodel
I1028 02:50:58.693594 63928 net.cpp:752] Ignoring source layer prob
I1028 02:50:59.725925 63928 net.cpp:752] Ignoring source layer prob
I1028 02:50:59.727938 63928 caffe.cpp:219] Starting Optimization
I1028 02:50:59.728014 63928 solver.cpp:279] Solving AlexNet
I1028 02:50:59.728021 63928 solver.cpp:280] Learning Rate Policy: step
I1028 02:50:59.730790 63928 solver.cpp:337] Iteration 0, Testing net (#0)
I1028 02:52:54.652458 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.720184 (* -0.35 = -0.252064 loss)
I1028 02:52:54.652640 63928 solver.cpp:404]     Test net output #1: loss_p2 = 6.22262 (* 0.65 = 4.04471 loss)
I1028 02:52:54.652650 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.002
I1028 02:52:54.652655 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.00998002
I1028 02:52:55.218825 63928 solver.cpp:228] Iteration 0, loss = 3.74882
I1028 02:52:55.218888 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.845291 (* -0.35 = -0.295852 loss)
I1028 02:52:55.218901 63928 solver.cpp:244]     Train net output #1: loss_p2 = 6.22258 (* 0.65 = 4.04467 loss)
I1028 02:52:55.218950 63928 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1028 02:53:44.603555 63928 solver.cpp:228] Iteration 40, loss = 3.15802
I1028 02:53:44.603796 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.930403 (* -0.35 = -0.325641 loss)
I1028 02:53:44.603806 63928 solver.cpp:244]     Train net output #1: loss_p2 = 5.35947 (* 0.65 = 3.48366 loss)
I1028 02:53:44.603814 63928 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I1028 02:54:33.978895 63928 solver.cpp:228] Iteration 80, loss = 2.92075
I1028 02:54:33.979132 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.92978 (* -0.35 = -0.325423 loss)
I1028 02:54:33.979141 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.99411 (* 0.65 = 3.24617 loss)
I1028 02:54:33.979151 63928 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I1028 02:55:23.336853 63928 solver.cpp:228] Iteration 120, loss = 2.87804
I1028 02:55:23.337131 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.92822 (* -0.35 = -0.324877 loss)
I1028 02:55:23.337141 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.92757 (* 0.65 = 3.20292 loss)
I1028 02:55:23.337148 63928 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I1028 02:56:12.721487 63928 solver.cpp:228] Iteration 160, loss = 2.5847
I1028 02:56:12.721726 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.930309 (* -0.35 = -0.325608 loss)
I1028 02:56:12.721736 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.4774 (* 0.65 = 2.91031 loss)
I1028 02:56:12.721745 63928 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I1028 02:57:02.007388 63928 solver.cpp:228] Iteration 200, loss = 2.69744
I1028 02:57:02.007647 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.936993 (* -0.35 = -0.327947 loss)
I1028 02:57:02.007658 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.65444 (* 0.65 = 3.02539 loss)
I1028 02:57:02.007664 63928 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1028 02:57:51.308030 63928 solver.cpp:228] Iteration 240, loss = 2.60361
I1028 02:57:51.308336 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.942239 (* -0.35 = -0.329784 loss)
I1028 02:57:51.308346 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.51291 (* 0.65 = 2.93339 loss)
I1028 02:57:51.308354 63928 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I1028 02:58:40.594420 63928 solver.cpp:228] Iteration 280, loss = 2.48654
I1028 02:58:40.594655 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.936198 (* -0.35 = -0.327669 loss)
I1028 02:58:40.594665 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.32955 (* 0.65 = 2.81421 loss)
I1028 02:58:40.594671 63928 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I1028 02:59:29.663810 63928 solver.cpp:228] Iteration 320, loss = 2.34867
I1028 02:59:29.664027 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.929437 (* -0.35 = -0.325303 loss)
I1028 02:59:29.664037 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.1138 (* 0.65 = 2.67397 loss)
I1028 02:59:29.664043 63928 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I1028 03:00:18.661069 63928 solver.cpp:228] Iteration 360, loss = 2.51948
I1028 03:00:18.661239 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.94022 (* -0.35 = -0.329077 loss)
I1028 03:00:18.661249 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.38239 (* 0.65 = 2.84856 loss)
I1028 03:00:18.661254 63928 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I1028 03:01:07.642372 63928 solver.cpp:228] Iteration 400, loss = 2.36492
I1028 03:01:07.642534 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.934215 (* -0.35 = -0.326975 loss)
I1028 03:01:07.642544 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.14137 (* 0.65 = 2.69189 loss)
I1028 03:01:07.642549 63928 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1028 03:01:56.658689 63928 solver.cpp:228] Iteration 440, loss = 2.44385
I1028 03:01:56.658850 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.941715 (* -0.35 = -0.3296 loss)
I1028 03:01:56.658859 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.26684 (* 0.65 = 2.77345 loss)
I1028 03:01:56.658865 63928 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I1028 03:02:45.660655 63928 solver.cpp:228] Iteration 480, loss = 2.28936
I1028 03:02:45.660818 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.946467 (* -0.35 = -0.331263 loss)
I1028 03:02:45.660827 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.03173 (* 0.65 = 2.62063 loss)
I1028 03:02:45.660833 63928 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I1028 03:03:08.930326 63928 solver.cpp:337] Iteration 500, Testing net (#0)
I1028 03:05:03.823925 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.880209 (* -0.35 = -0.308073 loss)
I1028 03:05:03.824126 63928 solver.cpp:404]     Test net output #1: loss_p2 = 4.16929 (* 0.65 = 2.71004 loss)
I1028 03:05:03.824131 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.16972
I1028 03:05:03.824137 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.37556
I1028 03:05:28.837087 63928 solver.cpp:228] Iteration 520, loss = 2.2367
I1028 03:05:28.837117 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.944055 (* -0.35 = -0.330419 loss)
I1028 03:05:28.837124 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.94941 (* 0.65 = 2.56712 loss)
I1028 03:05:28.837129 63928 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I1028 03:06:17.852777 63928 solver.cpp:228] Iteration 560, loss = 2.49882
I1028 03:06:17.852941 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.95153 (* -0.35 = -0.333035 loss)
I1028 03:06:17.852949 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.3567 (* 0.65 = 2.83185 loss)
I1028 03:06:17.852955 63928 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I1028 03:07:06.845732 63928 solver.cpp:228] Iteration 600, loss = 2.26579
I1028 03:07:06.845899 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.941674 (* -0.35 = -0.329586 loss)
I1028 03:07:06.845909 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.99289 (* 0.65 = 2.59538 loss)
I1028 03:07:06.845914 63928 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1028 03:07:55.829053 63928 solver.cpp:228] Iteration 640, loss = 2.22225
I1028 03:07:55.829232 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.947277 (* -0.35 = -0.331547 loss)
I1028 03:07:55.829242 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.92892 (* 0.65 = 2.5538 loss)
I1028 03:07:55.829248 63928 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I1028 03:08:44.830540 63928 solver.cpp:228] Iteration 680, loss = 2.31581
I1028 03:08:44.830706 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.943998 (* -0.35 = -0.330399 loss)
I1028 03:08:44.830716 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.0711 (* 0.65 = 2.64621 loss)
I1028 03:08:44.830721 63928 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I1028 03:09:33.811188 63928 solver.cpp:228] Iteration 720, loss = 2.02571
I1028 03:09:33.811352 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.939913 (* -0.35 = -0.328969 loss)
I1028 03:09:33.811360 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.62258 (* 0.65 = 2.35468 loss)
I1028 03:09:33.811367 63928 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I1028 03:10:22.809196 63928 solver.cpp:228] Iteration 760, loss = 2.24106
I1028 03:10:22.809356 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.954497 (* -0.35 = -0.334074 loss)
I1028 03:10:22.809366 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.96174 (* 0.65 = 2.57513 loss)
I1028 03:10:22.809372 63928 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I1028 03:11:11.803031 63928 solver.cpp:228] Iteration 800, loss = 2.25585
I1028 03:11:11.803200 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.95278 (* -0.35 = -0.333473 loss)
I1028 03:11:11.803210 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.98358 (* 0.65 = 2.58933 loss)
I1028 03:11:11.803215 63928 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1028 03:12:00.801082 63928 solver.cpp:228] Iteration 840, loss = 2.30062
I1028 03:12:00.801240 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.943483 (* -0.35 = -0.330219 loss)
I1028 03:12:00.801250 63928 solver.cpp:244]     Train net output #1: loss_p2 = 4.04744 (* 0.65 = 2.63084 loss)
I1028 03:12:00.801255 63928 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I1028 03:12:49.779320 63928 solver.cpp:228] Iteration 880, loss = 2.19325
I1028 03:12:49.779443 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.955923 (* -0.35 = -0.334573 loss)
I1028 03:12:49.779451 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.88896 (* 0.65 = 2.52783 loss)
I1028 03:12:49.779458 63928 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I1028 03:13:38.807960 63928 solver.cpp:228] Iteration 920, loss = 1.99369
I1028 03:13:38.808138 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.953874 (* -0.35 = -0.333856 loss)
I1028 03:13:38.808147 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.58083 (* 0.65 = 2.32754 loss)
I1028 03:13:38.808153 63928 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I1028 03:14:27.789397 63928 solver.cpp:228] Iteration 960, loss = 2.06021
I1028 03:14:27.789568 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.957568 (* -0.35 = -0.335149 loss)
I1028 03:14:27.789577 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.68517 (* 0.65 = 2.39536 loss)
I1028 03:14:27.789583 63928 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I1028 03:15:15.549592 63928 solver.cpp:454] Snapshotting to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_1000.caffemodel
I1028 03:15:20.893110 63928 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_1000.solverstate
I1028 03:15:23.645913 63928 solver.cpp:337] Iteration 1000, Testing net (#0)
I1028 03:17:17.848937 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.88616 (* -0.35 = -0.310156 loss)
I1028 03:17:17.849159 63928 solver.cpp:404]     Test net output #1: loss_p2 = 4.03004 (* 0.65 = 2.61953 loss)
I1028 03:17:17.849166 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.18818
I1028 03:17:17.849171 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.40288
I1028 03:17:18.394026 63928 solver.cpp:228] Iteration 1000, loss = 1.953
I1028 03:17:18.394042 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.947346 (* -0.35 = -0.331571 loss)
I1028 03:17:18.394060 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.51473 (* 0.65 = 2.28457 loss)
I1028 03:17:18.394068 63928 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1028 03:18:07.415962 63928 solver.cpp:228] Iteration 1040, loss = 2.17337
I1028 03:18:07.416116 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.949499 (* -0.35 = -0.332325 loss)
I1028 03:18:07.416126 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.85492 (* 0.65 = 2.5057 loss)
I1028 03:18:07.416131 63928 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I1028 03:18:56.408488 63928 solver.cpp:228] Iteration 1080, loss = 2.10676
I1028 03:18:56.408653 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.960143 (* -0.35 = -0.33605 loss)
I1028 03:18:56.408663 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.75817 (* 0.65 = 2.44281 loss)
I1028 03:18:56.408668 63928 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I1028 03:19:45.406352 63928 solver.cpp:228] Iteration 1120, loss = 2.06068
I1028 03:19:45.406527 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.955442 (* -0.35 = -0.334405 loss)
I1028 03:19:45.406535 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.68475 (* 0.65 = 2.39509 loss)
I1028 03:19:45.406541 63928 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I1028 03:20:34.391201 63928 solver.cpp:228] Iteration 1160, loss = 1.9553
I1028 03:20:34.391329 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.960061 (* -0.35 = -0.336021 loss)
I1028 03:20:34.391343 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.52511 (* 0.65 = 2.29132 loss)
I1028 03:20:34.391350 63928 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I1028 03:21:23.433991 63928 solver.cpp:228] Iteration 1200, loss = 1.9607
I1028 03:21:23.434187 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.95128 (* -0.35 = -0.332948 loss)
I1028 03:21:23.434197 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.52869 (* 0.65 = 2.29365 loss)
I1028 03:21:23.434203 63928 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1028 03:22:12.445492 63928 solver.cpp:228] Iteration 1240, loss = 1.96675
I1028 03:22:12.445657 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.951949 (* -0.35 = -0.333182 loss)
I1028 03:22:12.445667 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.53835 (* 0.65 = 2.29993 loss)
I1028 03:22:12.445672 63928 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I1028 03:23:01.426882 63928 solver.cpp:228] Iteration 1280, loss = 1.92757
I1028 03:23:01.427048 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.956262 (* -0.35 = -0.334692 loss)
I1028 03:23:01.427058 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.48041 (* 0.65 = 2.26227 loss)
I1028 03:23:01.427063 63928 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I1028 03:23:50.409760 63928 solver.cpp:228] Iteration 1320, loss = 1.86867
I1028 03:23:50.409910 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.953559 (* -0.35 = -0.333746 loss)
I1028 03:23:50.409920 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.38833 (* 0.65 = 2.20241 loss)
I1028 03:23:50.409925 63928 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I1028 03:24:39.386654 63928 solver.cpp:228] Iteration 1360, loss = 1.84212
I1028 03:24:39.386804 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.96302 (* -0.35 = -0.337057 loss)
I1028 03:24:39.386814 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.35257 (* 0.65 = 2.17917 loss)
I1028 03:24:39.386819 63928 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I1028 03:25:28.375900 63928 solver.cpp:228] Iteration 1400, loss = 2.13593
I1028 03:25:28.376082 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.958054 (* -0.35 = -0.335319 loss)
I1028 03:25:28.376092 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.80192 (* 0.65 = 2.47125 loss)
I1028 03:25:28.376097 63928 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I1028 03:26:17.392928 63928 solver.cpp:228] Iteration 1440, loss = 1.95967
I1028 03:26:17.393105 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.965062 (* -0.35 = -0.337772 loss)
I1028 03:26:17.393115 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.53452 (* 0.65 = 2.29744 loss)
I1028 03:26:17.393120 63928 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I1028 03:27:06.392184 63928 solver.cpp:228] Iteration 1480, loss = 1.92104
I1028 03:27:06.392354 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.96787 (* -0.35 = -0.338754 loss)
I1028 03:27:06.392364 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.4766 (* 0.65 = 2.25979 loss)
I1028 03:27:06.392370 63928 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I1028 03:27:29.653818 63928 solver.cpp:337] Iteration 1500, Testing net (#0)
I1028 03:29:24.631722 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.916187 (* -0.35 = -0.320666 loss)
I1028 03:29:24.631870 63928 solver.cpp:404]     Test net output #1: loss_p2 = 4.07459 (* 0.65 = 2.64848 loss)
I1028 03:29:24.631875 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.18484
I1028 03:29:24.631880 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.39378
I1028 03:29:49.669293 63928 solver.cpp:228] Iteration 1520, loss = 1.99321
I1028 03:29:49.669311 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.958906 (* -0.35 = -0.335617 loss)
I1028 03:29:49.669329 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.5828 (* 0.65 = 2.32882 loss)
I1028 03:29:49.669333 63928 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I1028 03:30:38.668777 63928 solver.cpp:228] Iteration 1560, loss = 1.78555
I1028 03:30:38.668952 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.962787 (* -0.35 = -0.336976 loss)
I1028 03:30:38.668962 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.26542 (* 0.65 = 2.12252 loss)
I1028 03:30:38.668967 63928 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I1028 03:31:27.684341 63928 solver.cpp:228] Iteration 1600, loss = 1.82978
I1028 03:31:27.684504 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.964877 (* -0.35 = -0.337707 loss)
I1028 03:31:27.684514 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.3346 (* 0.65 = 2.16749 loss)
I1028 03:31:27.684520 63928 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1028 03:32:16.682180 63928 solver.cpp:228] Iteration 1640, loss = 1.84848
I1028 03:32:16.682346 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.969533 (* -0.35 = -0.339337 loss)
I1028 03:32:16.682355 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.36587 (* 0.65 = 2.18781 loss)
I1028 03:32:16.682361 63928 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I1028 03:33:05.678930 63928 solver.cpp:228] Iteration 1680, loss = 1.91089
I1028 03:33:05.679095 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.958258 (* -0.35 = -0.33539 loss)
I1028 03:33:05.679105 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.45582 (* 0.65 = 2.24628 loss)
I1028 03:33:05.679111 63928 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I1028 03:33:54.699084 63928 solver.cpp:228] Iteration 1720, loss = 1.97661
I1028 03:33:54.699198 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.967265 (* -0.35 = -0.338543 loss)
I1028 03:33:54.699208 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.56177 (* 0.65 = 2.31515 loss)
I1028 03:33:54.699213 63928 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I1028 03:34:43.695363 63928 solver.cpp:228] Iteration 1760, loss = 1.77181
I1028 03:34:43.695530 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.964429 (* -0.35 = -0.33755 loss)
I1028 03:34:43.695539 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.24517 (* 0.65 = 2.10936 loss)
I1028 03:34:43.695544 63928 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I1028 03:35:32.871945 63928 solver.cpp:228] Iteration 1800, loss = 1.87216
I1028 03:35:32.872145 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.967438 (* -0.35 = -0.338603 loss)
I1028 03:35:32.872155 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.40117 (* 0.65 = 2.21076 loss)
I1028 03:35:32.872161 63928 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I1028 03:36:22.094524 63928 solver.cpp:228] Iteration 1840, loss = 1.90533
I1028 03:36:22.094764 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.972425 (* -0.35 = -0.340349 loss)
I1028 03:36:22.094775 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.45489 (* 0.65 = 2.24568 loss)
I1028 03:36:22.094785 63928 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I1028 03:37:11.459130 63928 solver.cpp:228] Iteration 1880, loss = 1.8563
I1028 03:37:11.459357 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.970052 (* -0.35 = -0.339518 loss)
I1028 03:37:11.459367 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.37818 (* 0.65 = 2.19581 loss)
I1028 03:37:11.459375 63928 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I1028 03:38:00.804059 63928 solver.cpp:228] Iteration 1920, loss = 1.6958
I1028 03:38:00.804303 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.968968 (* -0.35 = -0.339139 loss)
I1028 03:38:00.804313 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.13068 (* 0.65 = 2.03494 loss)
I1028 03:38:00.804322 63928 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I1028 03:38:50.170367 63928 solver.cpp:228] Iteration 1960, loss = 1.81164
I1028 03:38:50.170598 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.967528 (* -0.35 = -0.338635 loss)
I1028 03:38:50.170609 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.30811 (* 0.65 = 2.15027 loss)
I1028 03:38:50.170619 63928 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I1028 03:39:38.317028 63928 solver.cpp:454] Snapshotting to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_2000.caffemodel
I1028 03:39:43.508023 63928 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_2000.solverstate
I1028 03:39:46.277010 63928 solver.cpp:337] Iteration 2000, Testing net (#0)
I1028 03:41:40.639209 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.916139 (* -0.35 = -0.320649 loss)
I1028 03:41:40.639405 63928 solver.cpp:404]     Test net output #1: loss_p2 = 4.00494 (* 0.65 = 2.60321 loss)
I1028 03:41:40.639411 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.19486
I1028 03:41:40.639416 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.40998
I1028 03:41:41.182545 63928 solver.cpp:228] Iteration 2000, loss = 1.89479
I1028 03:41:41.182611 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.965787 (* -0.35 = -0.338026 loss)
I1028 03:41:41.182621 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.43511 (* 0.65 = 2.23282 loss)
I1028 03:41:41.182629 63928 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1028 03:42:30.555644 63928 solver.cpp:228] Iteration 2040, loss = 1.79751
I1028 03:42:30.555912 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.979838 (* -0.35 = -0.342943 loss)
I1028 03:42:30.555922 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.29301 (* 0.65 = 2.14046 loss)
I1028 03:42:30.555930 63928 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I1028 03:43:19.953817 63928 solver.cpp:228] Iteration 2080, loss = 1.39869
I1028 03:43:19.954062 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.967992 (* -0.35 = -0.338797 loss)
I1028 03:43:19.954072 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.67305 (* 0.65 = 1.73748 loss)
I1028 03:43:19.954082 63928 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I1028 03:44:09.337864 63928 solver.cpp:228] Iteration 2120, loss = 1.77845
I1028 03:44:09.338137 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.974591 (* -0.35 = -0.341107 loss)
I1028 03:44:09.338147 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.26085 (* 0.65 = 2.11955 loss)
I1028 03:44:09.338155 63928 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I1028 03:44:58.659844 63928 solver.cpp:228] Iteration 2160, loss = 1.71893
I1028 03:44:58.660104 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.970881 (* -0.35 = -0.339808 loss)
I1028 03:44:58.660115 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.16729 (* 0.65 = 2.05874 loss)
I1028 03:44:58.660120 63928 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I1028 03:45:48.005620 63928 solver.cpp:228] Iteration 2200, loss = 1.73649
I1028 03:45:48.005869 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.973422 (* -0.35 = -0.340698 loss)
I1028 03:45:48.005879 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.19567 (* 0.65 = 2.07719 loss)
I1028 03:45:48.005893 63928 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1028 03:46:37.070225 63928 solver.cpp:228] Iteration 2240, loss = 1.85217
I1028 03:46:37.070396 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.975785 (* -0.35 = -0.341525 loss)
I1028 03:46:37.070415 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.37492 (* 0.65 = 2.1937 loss)
I1028 03:46:37.070420 63928 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I1028 03:47:26.103703 63928 solver.cpp:228] Iteration 2280, loss = 1.64973
I1028 03:47:26.103834 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.986921 (* -0.35 = -0.345422 loss)
I1028 03:47:26.103844 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.06947 (* 0.65 = 1.99516 loss)
I1028 03:47:26.103849 63928 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I1028 03:48:15.127497 63928 solver.cpp:228] Iteration 2320, loss = 1.88954
I1028 03:48:15.127660 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.977458 (* -0.35 = -0.34211 loss)
I1028 03:48:15.127668 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.4333 (* 0.65 = 2.23165 loss)
I1028 03:48:15.127674 63928 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I1028 03:49:04.105804 63928 solver.cpp:228] Iteration 2360, loss = 1.67594
I1028 03:49:04.105926 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.980327 (* -0.35 = -0.343114 loss)
I1028 03:49:04.105936 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.10623 (* 0.65 = 2.01905 loss)
I1028 03:49:04.105940 63928 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I1028 03:49:53.107424 63928 solver.cpp:228] Iteration 2400, loss = 1.69898
I1028 03:49:53.107547 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.97847 (* -0.35 = -0.342464 loss)
I1028 03:49:53.107556 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.14068 (* 0.65 = 2.04144 loss)
I1028 03:49:53.107561 63928 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I1028 03:50:42.110910 63928 solver.cpp:228] Iteration 2440, loss = 1.71701
I1028 03:50:42.111079 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.982397 (* -0.35 = -0.343839 loss)
I1028 03:50:42.111088 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.17054 (* 0.65 = 2.06085 loss)
I1028 03:50:42.111094 63928 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I1028 03:51:31.139632 63928 solver.cpp:228] Iteration 2480, loss = 1.62172
I1028 03:51:31.139829 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.978229 (* -0.35 = -0.34238 loss)
I1028 03:51:31.139839 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.02169 (* 0.65 = 1.9641 loss)
I1028 03:51:31.139845 63928 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I1028 03:51:54.387506 63928 solver.cpp:337] Iteration 2500, Testing net (#0)
I1028 03:53:49.180788 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.938636 (* -0.35 = -0.328523 loss)
I1028 03:53:49.181000 63928 solver.cpp:404]     Test net output #1: loss_p2 = 4.08579 (* 0.65 = 2.65576 loss)
I1028 03:53:49.181007 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.18558
I1028 03:53:49.181012 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.39768
I1028 03:54:14.197868 63928 solver.cpp:228] Iteration 2520, loss = 1.4849
I1028 03:54:14.197901 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.984539 (* -0.35 = -0.344589 loss)
I1028 03:54:14.197909 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.8146 (* 0.65 = 1.82949 loss)
I1028 03:54:14.197914 63928 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I1028 03:55:03.137979 63928 solver.cpp:228] Iteration 2560, loss = 1.67536
I1028 03:55:03.138157 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.98789 (* -0.35 = -0.345761 loss)
I1028 03:55:03.138166 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.10943 (* 0.65 = 2.02113 loss)
I1028 03:55:03.138172 63928 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I1028 03:55:52.070338 63928 solver.cpp:228] Iteration 2600, loss = 1.71676
I1028 03:55:52.070494 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.982596 (* -0.35 = -0.343909 loss)
I1028 03:55:52.070504 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.17026 (* 0.65 = 2.06067 loss)
I1028 03:55:52.070510 63928 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1028 03:56:41.007760 63928 solver.cpp:228] Iteration 2640, loss = 1.75028
I1028 03:56:41.007915 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.981918 (* -0.35 = -0.343671 loss)
I1028 03:56:41.007923 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.22147 (* 0.65 = 2.09395 loss)
I1028 03:56:41.007930 63928 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I1028 03:57:29.928730 63928 solver.cpp:228] Iteration 2680, loss = 1.50996
I1028 03:57:29.928916 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.986966 (* -0.35 = -0.345438 loss)
I1028 03:57:29.928926 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.85445 (* 0.65 = 1.85539 loss)
I1028 03:57:29.928932 63928 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I1028 03:58:18.876945 63928 solver.cpp:228] Iteration 2720, loss = 1.47405
I1028 03:58:18.877101 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.975116 (* -0.35 = -0.341291 loss)
I1028 03:58:18.877110 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.79283 (* 0.65 = 1.81534 loss)
I1028 03:58:18.877116 63928 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I1028 03:59:07.803071 63928 solver.cpp:228] Iteration 2760, loss = 1.5836
I1028 03:59:07.803215 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.986551 (* -0.35 = -0.345293 loss)
I1028 03:59:07.803225 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.96753 (* 0.65 = 1.9289 loss)
I1028 03:59:07.803231 63928 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I1028 03:59:57.003285 63928 solver.cpp:228] Iteration 2800, loss = 1.66621
I1028 03:59:57.003515 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.980335 (* -0.35 = -0.343117 loss)
I1028 03:59:57.003525 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.09127 (* 0.65 = 2.00932 loss)
I1028 03:59:57.003535 63928 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I1028 04:00:46.360852 63928 solver.cpp:228] Iteration 2840, loss = 1.54633
I1028 04:00:46.361102 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.988588 (* -0.35 = -0.346006 loss)
I1028 04:00:46.361111 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.91129 (* 0.65 = 1.89234 loss)
I1028 04:00:46.361121 63928 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I1028 04:01:35.715214 63928 solver.cpp:228] Iteration 2880, loss = 1.71649
I1028 04:01:35.715442 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.987318 (* -0.35 = -0.345561 loss)
I1028 04:01:35.715452 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.17239 (* 0.65 = 2.06205 loss)
I1028 04:01:35.715461 63928 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I1028 04:02:25.096849 63928 solver.cpp:228] Iteration 2920, loss = 1.53854
I1028 04:02:25.097092 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.987233 (* -0.35 = -0.345531 loss)
I1028 04:02:25.097102 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.89858 (* 0.65 = 1.88407 loss)
I1028 04:02:25.097112 63928 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I1028 04:03:14.447700 63928 solver.cpp:228] Iteration 2960, loss = 1.66934
I1028 04:03:14.447948 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.983697 (* -0.35 = -0.344294 loss)
I1028 04:03:14.447958 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.09791 (* 0.65 = 2.01364 loss)
I1028 04:03:14.447968 63928 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I1028 04:04:02.573446 63928 solver.cpp:454] Snapshotting to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_3000.caffemodel
I1028 04:04:07.766438 63928 sgd_solver.cpp:273] Snapshotting solver state to binary proto file models/dissimilarity_siamese_net/snapshots/caffe_alexnet_train_iter_3000.solverstate
I1028 04:04:10.582067 63928 solver.cpp:337] Iteration 3000, Testing net (#0)
I1028 04:06:04.720268 63928 solver.cpp:404]     Test net output #0: loss_p1 = 0.949585 (* -0.35 = -0.332355 loss)
I1028 04:06:04.720453 63928 solver.cpp:404]     Test net output #1: loss_p2 = 4.06382 (* 0.65 = 2.64148 loss)
I1028 04:06:04.720460 63928 solver.cpp:404]     Test net output #2: precision@1 = 0.19234
I1028 04:06:04.720465 63928 solver.cpp:404]     Test net output #3: precision@5 = 0.40538
I1028 04:06:05.266404 63928 solver.cpp:228] Iteration 3000, loss = 1.65555
I1028 04:06:05.266456 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.987015 (* -0.35 = -0.345455 loss)
I1028 04:06:05.266464 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.07847 (* 0.65 = 2.00101 loss)
I1028 04:06:05.266475 63928 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1028 04:06:54.636988 63928 solver.cpp:228] Iteration 3040, loss = 1.58148
I1028 04:06:54.637212 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.991665 (* -0.35 = -0.347083 loss)
I1028 04:06:54.637222 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.96702 (* 0.65 = 1.92856 loss)
I1028 04:06:54.637231 63928 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I1028 04:07:43.792518 63928 solver.cpp:228] Iteration 3080, loss = 1.51957
I1028 04:07:43.792706 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.991468 (* -0.35 = -0.347014 loss)
I1028 04:07:43.792726 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.87167 (* 0.65 = 1.86659 loss)
I1028 04:07:43.792732 63928 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I1028 04:08:32.735399 63928 solver.cpp:228] Iteration 3120, loss = 1.52407
I1028 04:08:32.735554 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.989905 (* -0.35 = -0.346467 loss)
I1028 04:08:32.735564 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.87776 (* 0.65 = 1.87054 loss)
I1028 04:08:32.735569 63928 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I1028 04:09:21.665609 63928 solver.cpp:228] Iteration 3160, loss = 1.52322
I1028 04:09:21.665763 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.991508 (* -0.35 = -0.347028 loss)
I1028 04:09:21.665772 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.87731 (* 0.65 = 1.87025 loss)
I1028 04:09:21.665778 63928 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I1028 04:10:10.590411 63928 solver.cpp:228] Iteration 3200, loss = 1.65743
I1028 04:10:10.590571 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.997381 (* -0.35 = -0.349083 loss)
I1028 04:10:10.590581 63928 solver.cpp:244]     Train net output #1: loss_p2 = 3.08695 (* 0.65 = 2.00651 loss)
I1028 04:10:10.590586 63928 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I1028 04:10:59.504402 63928 solver.cpp:228] Iteration 3240, loss = 1.57304
I1028 04:10:59.504559 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.990479 (* -0.35 = -0.346668 loss)
I1028 04:10:59.504568 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.9534 (* 0.65 = 1.91971 loss)
I1028 04:10:59.504573 63928 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I1028 04:11:48.426499 63928 solver.cpp:228] Iteration 3280, loss = 1.37514
I1028 04:11:48.426656 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.990797 (* -0.35 = -0.346779 loss)
I1028 04:11:48.426664 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.6491 (* 0.65 = 1.72192 loss)
I1028 04:11:48.426671 63928 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I1028 04:12:37.357795 63928 solver.cpp:228] Iteration 3320, loss = 1.44149
I1028 04:12:37.357973 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.993608 (* -0.35 = -0.347763 loss)
I1028 04:12:37.357982 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.7527 (* 0.65 = 1.78925 loss)
I1028 04:12:37.357988 63928 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
I1028 04:13:26.408396 63928 solver.cpp:228] Iteration 3360, loss = 1.48113
I1028 04:13:26.408596 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.98775 (* -0.35 = -0.345713 loss)
I1028 04:13:26.408607 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.81052 (* 0.65 = 1.82684 loss)
I1028 04:13:26.408612 63928 sgd_solver.cpp:106] Iteration 3360, lr = 0.01
I1028 04:14:15.687485 63928 solver.cpp:228] Iteration 3400, loss = 1.4036
I1028 04:14:15.687734 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.994011 (* -0.35 = -0.347904 loss)
I1028 04:14:15.687744 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.69462 (* 0.65 = 1.75151 loss)
I1028 04:14:15.687753 63928 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I1028 04:15:04.991086 63928 solver.cpp:228] Iteration 3440, loss = 1.59113
I1028 04:15:04.991341 63928 solver.cpp:244]     Train net output #0: loss_p1 = 0.992945 (* -0.35 = -0.347531 loss)
I1028 04:15:04.991350 63928 solver.cpp:244]     Train net output #1: loss_p2 = 2.98256 (* 0.65 = 1.93866 loss)
I1028 04:15:04.991359 63928 sgd_solver.cpp:106] Iteration 3440, lr = 0.01
